{
    "docs": [
        {
            "location": "/",
            "text": "Let there be light!\n\n\n\n\nA Data Science documentation website.\n\n\n\n\nugodoc\n is a corpus; a catalog of books, manuals, articles, presentations, videos, podcasts, summaries, notes, code snippets, excerpts, websites, etc.\n\n\nThe \nCollection\n page is a high-level catalogue. The rest of the webpages are more comprehensive.\n\n\nThe \u2018docs\u2019 is a searchable knowledge-based system.\n\n\nYou type a keyword, it leads to several sources, you identify the document, and you go retrieve the document; whether it is a digital or a material document. Fast and easy!\n\n\nThe corpus is unstructured. Knowledge is rather cumulated, layer after layer, resulting in a hotchpotch of information. Information may be repeted among many documents, with different explanations, some more comprehensive. Newer entries might also supplement or contradict older entries.",
            "title": "Home"
        },
        {
            "location": "/collection/",
            "text": "Foreword\n\n\nCorpus of documents: titles and keywords.\n\n\n\n\nArduino, Raspberry\n\u00b6\n\n\nPremiers pas sur Raspberry Pi, BLOG\u00c9CLAIR.COM, 2017\n\n\nProgrammer avec Arduino en s\u2019amusant, Pour les nuls, 2017\n\n\nRaspberry Pi, Le guide de l\u2019utilisateur, Dunod, 2017\n\n\nRaspberry Pi pour les Nuls grand format, 2e \u00e9dition, First Interactive, 2017\n\n\nBash, Powershell\n\u00b6\n\n\nLearning the bash Shell, Third Edition, O\u2019Reilly, 2005\n\n\nManaging Projects with GNU Make, Third Edition, O\u2019Reilly, 2005\n\n\nThe Linux Command Line (completed course, notes, snapshots, codes, manual, exercises), No Starch Press, living book!\n\n\nBig Data, Spark, Cloud\n\u00b6\n\n\nAdvanced Analytics with Spark, Patterns for learning from Data at Scale, O\u2019Reilly, 2017\n\n\nApache Spark 2 Cookbook, 2\nnd\n Edition\n\n\nAWS Amazon Web Services, CC, 2017\n\n\nIntroducing Data Science, Big Data, Machine Learning and more, using Python tools, Manning, 2016, \nbig data, project management, machine learning, large data, steps, nosql, graph databases, text mining, text analytics, data visualization, user\n\n\nLarge Scale Machine Learning with Spark, Packt, 2016\n\n\nLearning PySpark, Packt, 2017\n\n\nSpark 2.0 for Beginners, Packt, 2016\n\n\nBlockchain\n\u00b6\n\n\nDiverse\u2026\n\n\nDataCamp\n\u00b6\n\n\nIntroduction to R (Beta) (completed course, slides), \nvariable, workspace, comments, scripts, data type, logical, numeric, character, double, complex, raw, coercion, coerce, as., create name, vectors, calculus, element, sum, subset, matrix, rbind, cbind, bind, rownames, colnames, colsums, rowsums, scalar, factor, categorical, order, levels, nominal, ordinal, order, list, extend, data frame, data.frame, sort, add column, row, sorting, graphics, plot numerical, plot categorical, hist, bins, barplot, boxplot, pairs, par, parameters, line type, lty, plot symbol, pch, multiple plots, mfrow, mfcol, grid, layout, stack\n\n\nIntermediate R (completed course, slides, snapshots), \nrelational operators, greater, lower, equal, logical operators, and , or, not, conditional, if, else, else if, loops, while, infinite, break, for, list, function, print, function documentation, na.rm, built-in functions, arguments, args, create functions, optional argument, return, packages, install, load, search, apply, lapply, sapply, vapply, identical, abs, absolute, round, sum, mean, sequence, seq, repeat, rep, sort, string, str, is., as., append, reverse, rev, regular expressions, grepl, grep, sub, gsub, subset, time, date, sys.date, sys.time, posixct, lubridate, zoo, xts, time series packages\n\n\nImporting Data into R (completed course, snapshots), \nflat files, excel files, spss files, sas files, stata files, postgresql database, mysql database, sqlite database, web, utils, read.table, header, separator, stringasfactors,  read.csv, comma, dot, , semi-colon, tab, delimited, states_eu.csv, readr, data.table, read_delim, col_names, col_types, skip, n_max, n_min, fread, readxl, excel_sheets, gdata, xlsx, xlconnect, loadworkbook, getsheets, readworksheet, createsheet, writeworksheet, saveworkbook, haven, foreign, read_sas, read_stata, read_dta,  as_factor, read_spss, read.dta, convert.factors, read.spss, rmysql, dbconnect, dblisttables, dbreadtable, dbgetquery, dbfetch, dbdisconnect, rpostgresql, web, path, download.file, url, authentication, dest_path, httr, api, json, twitter, jsonlite, array, nesting, tojson, prettify, minify\n\n\nCleaning Data in R (completed course, snapshots), \nexplore, structure, class, dim, names, str, glimpse, summary, visualize, head, tail, print, hist, plot, tidy, dirty, wide datasets, long datasets, tidyr, key, value, gather column, spread column, separate, unite, convert, conversion, as., lubridate, date, tmd, mdy, hms, ymd_hms, string manipulation, stringr, str_trim, trim, str_pad, str_detect, str_replace, replace, tolower, toupper, lowercase, uppercase, missing, na, not available, empty, dot, infinite value, inf, not a number, nan, outlier, errors, extreme, unexpected\n\n\nWriting Functions in R (completed course), \ndefine, return value, object, scope, scoping, lookup, data structures, atomic, list, missing very, not available, na, subset, less code, clearer, snippet, temporary, name, naming, argument, order, good function, functional programming, domain, variable, common code, functions as arguments, map, map_dbl, map_lgl, map_int, map_chr, failure, safely, possibly, quietly, map2, iterate, pmap, invoke_map, mapping, map2_dbl, map2_lgl, pmap_dbl, walk, pipe, pipeline, robust, errors, hidden argument, getting, setting options\n\n\nData Manipulation in R with dplyr (completed course, snapshots), \npipe, pipeline, tbl, select, filter, arrange, mutate, summarize, summarise, tidyr, tidy, starts_with, ends_with, contains, matches, num_range, one_of, is.na, !is.na, min, sum, sd, max, mean, var, length, median, iqr, first, last, nth, n, n_distinct, %>%, magrittr, group_by, data frame, data table, database, tbl_dt, tbl\n\n\nData Analysis in R, the data.table Way (completed course, snapshots, cheat sheet), \ndata table, dt, select row, i, column, j, compute, by, group, subset, chain, chaining, :=, set, setnames, setcolorder, index, indexing, key, mult, nomatch, two-column key, join, setkey\n\n\nData Visualization in R with ggvis (completed course, snapshots), \npoints, bars, maps, histograms, scatter, density, data, coordinate system, mark, properties, html, javascript, js, THE%>%, magrittr, pipe, pipeline, :=, sets, maps, =, stroke, width, opacity, dash, fill, lines, paths, ribbons, smooths, prediction, model_predictions, later, layer_smooths,  dplyr, layer_paths, layer_lines, group_by, mapping, chart, charting, interactivity, multilayered, axes, axis, legends\n\n\nData Visualization with ggplot2 (1) (completed course, snapshots, videos), \ngraphical data analysis, design, communication, exploratory, explanatory, anscombe plot, fit, fitting, grammar of graphics, layers, data, aesthetics, statistics, geometries, geom_, facets, coordinates, themes, base plot, points, tidy, color, size, shape, attribute, alpha, line type, label, shape, axes, positions, identity, dodge, stack, fill, jitter, jitterdodge, scale, scale functions, limit, breaks, expand, labs, abline, area, bar, bin2d, blank, boxplot, contour, crossbar, density, density2d, dotplot, errorbar, errorbar, freqpoly, hex, histogram, hline, line, linerange, map, path, pointrange, polygon, quantile, raster, rect, vline, ribbon, rug, segment, smooth, step, text, tile, violin, eas, geom, pch, crosshairs, remarks, time series, linetype, size, qplot\n\n\nData Visualization with ggplot2 (2) (completed course, snapshots, videos), \nstatistics, coordinates, facets, themes, best practices, function, geom, geom_, stat_, fill,  bin, histogram, bar, freqpoly, smooth, boxplot, bindot, bin2d, binhex, contour, quantile, sum, boxplot, dotplot, bin2d, hex, contour, quantile, count, summary, confidence interval, qq, quantile-quantile, coord_, coord_cartesian, scale_x_continuous, xlim, coord_cartesian, aspect ratio, facets, edward tufte, tidy, theme layer, element_, text, line, rectangle inheritance, blank, recycling, save theme, reuse theme, discrete x-axis, derivative theme, built-in theme templates, ggthemes, theme update, theme_set, pitfalls, dynamic plot, error bar, pointrange, pie charts, stacked bar chart, haircol, horizontal, heat maps\n\n\nReporting with R Markdown (completed course, snapshots, videos), \nreproducible, research, knitr, pandoc, shiny,  interactive, web-based, rstudio, html, css, pdf, word, beamer, slidy, ioslides, markdown::render, rmd, latex\n\n\nIntroduction to Machine Learning (completed course, snapshots, notes), \nregression, shopping basket analysis, recommendation systems, decision-making, classification, clustering, cluster, k-means, supervised, unsupervised, model performance, error, accuracy, competition time, interpretability, limits, confusion matrix, precision, recall, true, false, real mean squared error, cluster similarity, between cluster sum of squares, inter-cluster distance, dunn\u2019s index, training, train, testing, test, predictive power, split the dataset, cross validation, n-fold, number of validation, bias, variance, quadratic data, overfitting, underfitting, decision tree, numerical, categorical, classify, choose, split, sticking criteria, information gain, pruning, k-nearest neighbours, k-nn, distance, euclidean, scaling, dummy, roc curve, receiver operator characteristic curve, probability, simple regression, linear regression, r-squared, multiple linear regression, adjusted, predictors, assumptions, non-parametric, kernel regression, regression trees, generalized regression, within cluster sums of squares, between cluster sums of squares, k, scree plot, choosing, intercluster, hierarchical clustering, dendrogram, pros and cons\n\n\nIntro to Statistics with R (completed course, snapshots, videos):\n\n\n\n\nCourse One Introduction, \ntypes of variables, nominal, ordinal, interval, ratio, histogram, distribution, bimodal, skewed, skewness, uniform, platykurtic, leptokurtic, scales, z-score, percentile rank, measure of central tendency, average, median, mode, variability, variance, standard deviation\n\n\nCourse Two Student\u2019s T-test, \nz-test, t-test,  single sample, dependent, independent, cohen, cohen\u2019s d, confidence interval, t-value, upper bound, lower bound\n\n\nCourse Three Analysis of Variance (ANOVA), \nanova, analysis of variance, continuous, independent t-test, dependent t-test, between groups, repeated measures, f-test, f-ratio, post-hoc, tukey\u2019s procedure, tukey, factorial anova, two independent variables, one dependent variable, main effect, interaction effect, simple effect, effect size, homogeneity of variance, normal distribution\n\n\nCourse Four Repeated Measures (ANOVA), \nstatistical power, order effects, counterbalancing, missing data, extra assumption, homogeneity of variance, homogeneity of covariance, sphericity, systematic, between, unsystematic, within, subjects, f-test, mean-squared, ms, post-hoc test, holm\n\n\nCourse Five Correlation and Regression, \ncorrelation, r, sum of cross products, sp, covariance, variance, magnitude, causation, causality, sampling, sample, measurement, regression, r-squared, estimation, coefficients, assumptions, normal, linear, homoscedasticity, anscombe\u2019s quartet, anscombe, scatterplot, residuals\n\n\nCourse Six Multiple Regression, \nregression, simple, multiple, predictors, r-squared, dummy, dummies, matrix algebra, data frame, variance, covariance, standard deviation, correlation, estimation, coefficients, unweighted coding, weighted coding\n\n\nCourse Seven Moderation and Mediation, \nexperimental manipulation, moderator, moderation, enhance, test for moderation, centering predictors, avoid multicollinearity, constant, mediation, partial, full, sobel test\n\n\n\n\nBig Data Analysis with Revolution R Enterprise (completed course, snapshots, videos), \nchallenge, move, merge, manage, munge, large datasets, inspect, transform, create available, summarize, xdf, parallel, visualize, histogram, correlation, subset, linear model, nonlinear relationship, categorical, revoscaler, revolution, distributed environments, cluster, information, dayofweek, summary, quantile, crosstabs, facets, weights, scatterplot, log, data frame, import, min, max, apply, means, drop, predict, machine learning, split, train, test, split, cross validation, standard errors, covariance, logit, logistic regression, generalized least square, cluster, clustering, decision trees, regression tree, revotree, roc curve, classification tree, confusion matrix\n\n\nDataCamp, Data Analysis and Statistical Inference (completed course, snapshots, videos, notes, manuals, codes, datasets), \nimport, plot, inspect, web, load, table, barplot, mosaicplot, mosaic, summarize, structure, nrow, head, tail, str, summary, boxplot, names, descriptive statistics, histogram, sample, simulation, distribution, population, sampling distribution, for loop, sample size, confidence intervals, standard error, dotplot of samples, inferential statistics, bootstrap, interference, confidence level, bootstrap method, parameter of interest, relationships between two variables, by, anova, continuous, categorical, margin of error, proportion, regression, correlation, plot, moneyball, abline, linearity, normal residuals, constant variability, qqnorm, qqline, boxplot, mosaicplot, kitter, adding, removing, residuals, openintro statistics manual\n\n\nData Exploration With Kaggle Scripts (completed course, snapshots, videos, codes, datasets), \nportfolio, github, cases, map, mapping, spanish production of silver, chopsticks effectiveness, pigeon data, kaggle account, phd earnings, american community survey\n\n\nDrivenData Water Pumps Challenge (case), \ndata mining, water table, train, test, ggplots, visualization, map, well, location, random forest\n\n\nExploring Polling Data in R (case), \nvisualization, pools, election\n\n\nHaving Fun with googleVis (case), \nvisualization, gapminder, interactive, graphs\n\n\nHow to work with Quandl in R (completed course, snapshots), \ndata connection, api, python, excel, web, r, database aggregator, library, stocks, finance, economics, census, public services, government, world organizations\n\n\nIntro to Computational Finance with R (completed course, snapshots, notes, codes, datasets, manuals), \ntseries, stock returns, zoo, performanceanalytics, econometrics, risk analysis, financial data, returns, plot, chart, matrix, histogram, boxplot, plot, qqnorm, return distribution, descriptive statistics, args, apply, annualized monthly estimates, graphical analysis, covariance matrix, mvtnorm, simulate, set.seed, sigma, joint probability, rho, probability, expected return model, global minimum variance portfolio, efficient portfolio, efficiency frontier, tangency portfolio, calculate returns, standard error of the variances, hypothesis test of mean, of the correlation, normality, asset returns, class, portfolio theory, t-bills, sharpe slope, the global minimum variance portfolio, efficiency portfolio, quantiles, densities, normal curve, value-at-risk, continuously compounded monthly returns, simple total returns, dividend yields, annual returns, portfolio shares, portfolio returns, price data, index, indices, subset, continuously compounded 1-month returns, monthly compounding, xts, time series, moving average, ma, autoregressive, ar\n\n\nKaggle R Tutorial on Machine Learning (completed course, snapshot, codes), \ntitanic, prediction, decision trees, interpret, predict, summit, overfitting, reengineering, survival rates, pruning, random forest\n\n\nPlotly Tutorial Plotly and R (case)\n\n\nR for the Intimidated (introductory course, videos)\n\n\nR, Yelp and the Search for Good Indian Food (case), \ndata wrangling, web scraping, filter, sieve, sift, data mining, dplyr\n\n\nAssessing Tank Production (case), \nmonte carlo, bootstrap\n\n\nCredit Risk Modeling in R (completed course, IPython notebook, slides, datasets), \nexpected loss, probability, default, exposure, loss, crosstable, loan, mortgage, gmodel, interest, grade, ownership, annual income, age, histogram, outlier, missing, confusion matrix, model prediction, classification accuracy, sensitivity, specificity, logistic regression, logit, generalized linear model, glm, table, logarithmic, predicting, prediction, discriminative, train, training, test, testing, probabilistic regression, probit, log-log, loglog, cutoff, decision tree, gini-measure, gini, rpart, sample, dataset, set, glass matrix, pruning, plotcp, printcp, complexity parameter, cp, prune, plotcp, prp, bad rate, fixed acceptance rate, strategy curve, table, bank, roc, comparison, proc, area under curve, auc, model reduction, discriminant analysis, random forest, neural networks, support vector machine, survival analysis\n\n\nExploring Pitch Data with R (completed course, IPython notebook, slides, notes, images, manuals, datasets), \nvelocity, graphical skills, distribution, fastball, hitting, outcomes, game, date, subset, histogram, abline, vertical line, ifelse, tapply, plot, time series, overlap, jitter, multimodal, bimodal, mix, change, table, prop.table, ball-strike cout, pitch usage, expectancy, paste, concatenate, pitch location, strike zone, locational variable, horizontal, vertical, binning, grid, loop, plot, visual interpretation, bat, batting, batted, contact rate, ggplot2, wide, long, locgrid, layer\n\n\nIntroduction to Python & Machine Learning (completed course, IPython notebook, slides, notes, images, manuals, datasets)\n\n\nKaggle Python Tutorial on Machine Learning (completed course, IPython notebook, slides, notes, images, manuals, datasets)\n\n\nPython 3 (including Importing Data, completed courses, IPython notebook, slides, notes, datesets), \ndata types, float, int, str, bool, list, subset, slice, change, add, remove, function, type, round, method, built-in, max, len, capitalize, replace, bit_length, index, count, index, append, package, library, pip, python3, numpy, array, pip3, 3d array, ndarray, mean, median, corrcoef, std, round, column_stack, print, concatenation, inline, offline, matplotlib, pyplot, plot, show, scatter, hist, xlabel, ylabel, title, yticks, xticks, dictionary, dictionaries, keys, pandas, tabular, table, data frame, dataframe, index, read_csv, select, bracket, row, column, label-based, loc, integer position-based, iloc, comparison, greater, lower, equal, true, false, boolean, and, or, not, conditional, if, else, elif, filter, compare, loop, while, for, enumerate, in, list, dictionary, array, 2d array, my_dict.items, nditer(my_array), iterrows, apply, random, rand, seed, randint, random walk, range, tails, append, recfromcsv, xscale, clean frame, clf, transpose, flat file, csv, txt, comma, tab, delimiters, semi-colon, numpy, np, vector, array, import, essential for scikit-learn, loadtxt, genfromtxt, datatypes, pandas, data frame, dataframe, scipy, excel, matlab, sas, stata, hdf5, pickled, serialize, bytestream, spreadsheet, pickle, pickle.load, pd.excelfile, parse, sas7bdat, file.to_data_frame, pd.read_stata, h5py.file, data.keys, key, keys, value, values, scipy.io, sci.io.loadmat, scipy.io.savemat, .mat, relational database, postgresql, mysql, sqlite, sql, sqlalchemy, create_engine, engine.connect, con.execute, engine_table_name, query, execute, pd.dataframe, fetch, fetchall, close, rs.keys, engine.connect, join, read_sql_query, web, urllib, urlopen, urlretrieve, request, read, read_csv, close,  pd.read_excel, requests, get, scrape, scraping, beautifulsoup, prettify, find_all, api, json, tweepy, authentification, oauthhandler, os, getcsd, listdir\n\n\nStatistical Thinking in Python (Part 1) (completed course, IPython notebook, slides, notes, datasets)\n\n\nText Mining, Bag of Words (completed course, IPython notebook, slides, notes, datasets), \nworkflow, problem definition, specific goals, identify text, text organization, feature extraction, analysis, insight, recommendation, bag of words, semantic parsing, Documents, corpus, corpora, vcorpus, read.csv, cleaning, preprocessing, tolower, removepunctuation, removenumbers, stripwhitespace, removewords, word stemming, stemdocument, stem_words, stem_completion, turn document matrix, tdm, document time matrix, dtm, word frequency matrix, wfm, tm, rowsums, sort, colsums, term_frequency, qdap, freq_terms, word cloud, stop word, tm_map, function, commonality clouds, commonality.cloud, union, paste, vectorsource, vcorpus, as.matrix, comparison cloud, pyramid plot, pyramid.plot, subset, abs, cbind, data.frame, rownames, word network, word clustering,  hclust, dendrogram, unigram, bifram, trigram, ngram, term weight, single word count, tfldf, penalize word, metadata, readtabular, hr analytics\n\n\nTidy Data in Python (completed courses, IPython notebook, datasets)\n\n\nTime Series in R The Power of xts and zoo (completed courses, IPython notebook, datasets), \nextensible time, xts, zoo, matrix, date, seq, core, coredata, convert, as.xts, as.matrix, import, export, iso8601, date, time, intraday, interval, extraction, first, last, coredata, as.numeric, merge, index, inner join, outer, left, right, rbind, cbind, na.locf, na.approx, lag, difference, diff, endpoints, apply, lapply, split, do.call, convert, to.period, to.quarterly, rolling value, time zone, timezones, indexclass, indextz, indexformat, time, periodicity, to.yearly, nmonths, nquarters, nyears, timestamps, indexwday, unique\n\n\nVisualization with Bokeh (completed courses, IPython notebook, datasets)\n\n\nDigital Humanities\n\u00b6\n\n\nDiverse\u2026\n\n\nDocker\n\u00b6\n\n\nDocker Cookbook, Packt, 2015\n\n\nDocker in Action, Manning, 2016\n\n\nEconometrics, Spatial, GIS\n\u00b6\n\n\nA Primer for Spatial Econometrics with Applications in R, Palgrave, 2014\n\n\nAn Introduction to R for Quantitative Economics, Springer, 2015, \nrstudio, crude oil price, supply, demand, fish, function, derivative, elasticity, linear, log-log, cobb,douglas, matrix, statistics, regression, simulation, normal, uniform, binomial, central limit theorem, t-test, logit, anscombe, graphs, scatter, growth, time series, time, random walks, cycles, stochastic, difference, air passengers, inflation, phillips curve, stock market\n\n\ncas (notes)\n\n\nMathematics for Marketing Models (notes)\n\n\nEconometrics by Example, Palgrave Macmillan, 2014\n\n\nEvent History Analysis with R, CRC Press, 2012 (PAPER)\n\n\nIntroduction to Python for Econometrics, Statistics and Data Analysis, CC, 2014\n\n\nMachine-learning Techniques in Economics, New Tools for Predicting Economics Growth, Springer, 2017\n\n\nIntroducing Survival and Event History Analysis, SAGE, 2011, \nsurvival, event, history, r, data, exploration, descriptive statistics, data structures, nonparametric, kaplan-meier estimator, \n\nIntroduction to Stochastic Process with R, Wiley, 2016, \nmarkov chain, stationary, periodicity, ergodic, time, absorbing, branching, probability, extinction, monte carlo, gibbs sampler, sampler, eigenvalue, card shuffling, poisson process, arrival, interarrival, thinning, superposition, uniform distribution, spatial poisson, nonhomogeneous, parting, continuous-time, brownian motion, gaussian process, transformation, properties, variations, applications, ito integral, discrete random variable, joint distribution, continuous random variable, common probability distributions, moment-generating functino, matrix algebra\n\n\nSpatial Analysis, Statistics, Visualization, and Computational Methods, CRC Press, 2016\n\n\nSpatial Statistics & Geostatistics, SAGE, 2013 (PAPER)\n\n\nStatistics Done Wrong, No Starch Press, 2015\n\n\nThe Cox Model and Its Applications, Springer, 2016\n\n\nThe Economics of Obesity, Poverty, Income Inequality and Health, Springer, 2017\n\n\nThink Stat 2, Exploratory Data Analysis in Python, Green Tea Press, 2014\n\n\nEmployment, Education\n\u00b6\n\n\n21 cl\u00e9s pour activer la transformation num\u00e9rique de votre entreprise, Eyrolles, 2017\n\n\nCracking the Coding Interview, 150 Programming Interview Questions and Solutions, 4\nth\n Edition, CareerCup, 2010, \ninterview, behind the scenes, while stories, before, resume, behavioural, preparation, questions, mistakes, frequently asked questions, data structure, algorithms, programming languages, problem solving, knowledge\n\n\nPr\u00e9venir plut\u00f4t que gu\u00e9rir la r\u00e9volution de la e-sant, Eyrolles, 2017\n\n\nIndustry\n\u00b6\n\n\nBig Data Analytics, A Practical Guide for Managers, Auerbach Publications, 2015\n\n\nLaTeX, Markdown\n\u00b6\n\n\nA Crash Course in LaTeX, 2018\n\n\nLaTeX pour l\u2019impatient, 3e \u00e9dition, MiniMax, 2009\n\n\nLaTeX pour l\u2019impatient, 4e \u00e9dition, MiniMax, 2016 (BANQ)\n\n\nLaTeX appliqu\u00e9 aux sciences humaines, Atramenta, 2012\n\n\nPetit guide pour les d\u00e9butants en LaTeX, 2008\n\n\nTikZ pour l\u2019impatient, CC, 2017\n\n\nTout ce que vous avez toujours voulu savoir sur LaTeX sans jamais oser le demander, CC, 2018\n\n\nLinux\n\u00b6\n\n\nPython for Unix and Linux System Administration, O\u2019Reilly, \n\n\nNLP, Text\n\u00b6\n\n\nAnalyzing Linguistic Data, A Practical Introduction to Statistics using R, cambridge, 2008\n\n\nNatural Language Processing: Python and NLTK, Packt, 2016\n\n\nNLTK Essentials, Packt, 2015\n\n\nStatistics for Linguistics with R, A Practical Introduction, 2\nnd\n edition, 2013\n\n\nText Analysis with R for Students of Literature, Springer, 2014\n\n\nNeural Networks\n\u00b6\n\n\ndiverse\u2026\n\n\nHands-On Machine Learning with Scikit-Learn & TensorFlow, O\u2019Reilly, 2017\n\n\nNo Starch Press Collection\n\u00b6\n\n\nAbsolute OpenBSD, Unix For The Practical Paranoid\n\n\nBook of Ruby\n\n\nBuilding a Server with FreeBSD 7\n\n\nCisco Routers for the Desperate, Router Management the Easy Way\n\n\nFreeBSD Device Drivers\n\n\nHow Linux Works What Every Superuser Should Know-viny\n\nLinux Appliance Design, A Hands-On Guide to Building Linux Appliances\n\n\nSystem and Network Monitoring\n\n\nAbsolute FreeBSD, The Complete Guide to FreeBSD, 2\nnd\n Edition\n\n\nLinux Firewalls\n\n\nThe Tangled Web\n\n\nPGP & GPG, Email for the Practical Paranoid\n\n\nPractical Packet Analysis\n\n\nRuby by Example\n\n\nThe Book of CSS3, A Developer\u2019s Guide to the Future of Web Design\n\n\nThe Book of PF, A No-Nonsense Guide to the OpenBSD Firewall\n\n\nThe Book of Postfix\n\n\nThe Linux Command Line\n\n\nWebbots Spiders and Screen Scrapers, 2\nnd\n Edition\n\n\nWicked Cool Shell Scripts, 101 Scripts for Linux Mac OS X and Unix Systems\n\n\nWrite Great Code, Volume II, Thinking Low-Level, Writing High-Level\n\n\nWrite Great Code, Volume I, Understanding The Machine\n\n\nOrange\n\u00b6\n\n\nsoftware\u2026\n\n\nPredictive Modeling, Data Science, Data Mining, Marketing Analytics\n\u00b6\n\n\nAnalytics for Insurance, The Real Business of Big Data, Wiley, 2016, \nrisk, management, underwriting, claim, marketing, property, liability, life, pensions, people\n\n\nArtificial Intelligence for Marketing, Practival Applications, Wiley, 2017\n\n\nBig Data opportunit\u00e9 ou menace pour l\u2019assurance, RB \u00c9dition, 2016\n\n\nMarketing Analytics, A Practical Guide to Real Marketing Science, Wiley, 2015, \nstatistics, consumer, behaviour, strategy, regression, business case, segmentation, elasticity, test, control, lift, collinearity, logistic, market basket analysis, survival, lifetime, value, ltv, descriptive, predictive, simultaneous equations, segment, k0means, lca, rfm, behavioural, missing value, research, conjoint analysis, structural equation, sem, sample, a/b, testing, factorial, engagement\n (BANQ)\n\n\nData Science for Business, O\u2019Reilly, 2013\n\n\nMarketing Data Science, Modeling Techniques in Predictive Analytics with R and Python, Wiley, 2015, \nmarket, consumer choice, customer, retention, acquisition, positioning, promiting, recommending, brands, prices, social networks, competitors, predicting, sales, database, regression, bayesian, data mining, machine learning, data visualization, text, sentiment analysis, time series, market response, sampling, www, social media, surveys, experiments, interviews, focus groups, field search cases\n\n\nMarketing analytics : a practical guide to real marketing science, Kogan Page, 2015 (BANQ)\n\n\nReal-World Machine Learning, Manning, 2017\n\n\nThink Like a Data Scientist, Manning, 2017\n\n\nProgrammation pour les kids\n\u00b6\n\n\nCahier d\u2019activit\u00e9s 3D pour les kids, Eyrolles, 2017\n\n\nArduino pour les Nuls poche, 2e \u00e9dition, 2017, First\n\n\nSylvia pr\u00e9sente Super Projets Arduino\n\n\nProgrammer avec Arduino en s\u2019amusant, 2017, First\n\n\nProject Management\n\u00b6\n\n\nAn Entire MBA in 1 Book, 2016\n\n\nEffective DevOps, O\u2019Reilly, 2015\n\n\nNegotiating 101, From Planning Your Strategy\n\n\nProject Management Metrics, KPIs, and Dashboards, A Guide to Measuring and Monitoring Project Performance, 3\nrd\n Edition, Wiley, 2017\n\n\nPMI-ACP\n\u00b6\n\n\n\u2026\n\n\nPython\n\u00b6\n\n\nAn Introduction to Statistics with Python, With Applications in the Life Sciences, Springer, 2016\n\n\nApprendre \u00e0 programmer en Python 3, CC, 2012\n\n\nAutomate the Boring Stuff with Python, No Starch Press, 2015\n\n\nA Whirlwind Tour of Python, O\u2019Reilly, 2016\n\n\nBayesian Methods for Hackers, Addison-Wesley\n\n, 2016, \nbayesian, inference, pymc, python, a/b testing, cases, distribution, algorithms, plot, graphs, statistics, probability, loss function, machine learning\n (https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)\n\n\nBuilding Machine Learning Systems with Python, Get more from your data through creating practical machine learning systems with Python, 2\nnd\n Edition, Packt, 2015, \niris, classifying, scikit-learn, clustering, bag of words, k-means, topic modelling, data mining, knn, logistic, regression, sentiment analysis, naive bayes, classifier, tweets, clean, word,  cross-validation, penalize, regulatize, lasso, elasticnet, text, predictions, recommendations, basket analysis, classification, music, computer vision, dimension reduction, big data, amazon web service\n\n\nBuilding Recommendation Engines, Packt, 2017\n\n\nData Science from Scratch First Principles with Python, O\u2019Reilly, 2015, \nvisualization, linear algebra, statistics, probability, hypothesis, inference, gradient descent, fetch, read, file, scraping, web, api, twitter, exploring, machine learning, k-nearest neighbours, knn, naive bayes, regression, logistic, decision tree, neural networks, clustering, natural language processing, network analysis, recommender system, databases, sql, mapreduce\n\n\nData Wrangling with Python Tips and Tools to Make Your Life Easier, O\u2019Reilly, 2016, \nexcel, spreadsheet, pdf, storing, clean, format, outilier, bad, duplicates, match, regex, standardizing, scripting, exploration, presenting, reporting, web, scraping, screen, spiders, api, automation, scaling\n \n\n\nFlask Building Python Web Services, Packt, 2017\n\n\nHands-On Machine Learning with Scikit-Learn & TensorFlow, O\u2019Reilly, 2017\n\n\nPython Geospatial Analysis Cookbook, Packt, 2015\n\n\nIntroducing Data Science, Big Data, Machine Learning and more, using Python tools, Manning, 2016\n\n\nGeoprocessing with Python, Manning, 2016\n\n\nIntroduction to Machine Learning with Python, O\u2019Reilly, 2017\n\n\nLearning Predictive Analytics with Python, Packt, 2016\n\n\nMastering Python Regular Expressions, Packt, 2014\n\n\nManaging your Biological Data with Python, CRC Press, 2014 (PAPER)\n\n\nMastering Python Regular Expressions, Leverage regular expressions in Python even for the most complex features, Packt, 2014\n\n\nPractical Data Analysis, Packt, 2013\n\n\nPractical Data Science Cookbook, 2\nnd\n Edition, Packt, 2017\n\n\nPython 3, Apprendre \u00e0 programmer en Python avec Pyzo et Jupyter Notebook, Dunod, 2017\n\n\nPython in 8 hours, 2017\n\n\nPython Data Analysis, 2\nnd\n Edition, Packt, 2017\n\n\nPractical Statistics for Data Scientists 50 Essential Concepts, O\u2019Reilly, 2017\n\n\nPython Data Analysis, 2\nnd\n edition, Packt, 2017\n\n\nPython Data Analysis Cookbook, Packt, 2016\n\n\nPython Data Analytics and Visualization, Packt, 2017\n\n\nPython Data Science Handbook, O\u2019Reilly, 2017\n\n\nPython Data Visualization, 2\nnd\n edition, Packt, 2015\n\n\nPython End-to-end Data Analysis, Packt, 2016\n\n\nPython for R Users, Wiley, 2018\n\n\nPython Geospatial Analysis Essentials, Packt 2015\n\n\nPython Geospatial Development, Third Edition, Packt, 2016\n\n\nPython Machine Learning Cookbook, Packt, 2016\n\n\nReal-World Machine Learning, Manning, 2017\n\n\nUne introduction \u00e0 Python 3, CC \n\n\nWeb Scraping with Python Collecting Data from the Modern Web, O\u2019reilly, 2015\n\n\nR, Statistics\n\u00b6\n\n\nAn Introduction to Statistical Learning with Applications in R, Springer, 2015\n\n\nAnalyse de donn\u00e9es avec R, PUR, 2016, \npca, principal components, fa, factorial, factor, analysis, multiple, correspondence, classifier, classification, visualization\n (BANQ)\n\n\nApplied Spatial Data Analysis with R, Springer, 2\nnd\n edition, 2013\n\n\nA Tiny Handbook of R Springer, 2011\n\n\nBasic Data Analysis for Time Series with R, Wiley, 2014\n\n\nBiostatistique: une approche intuitive, De Boeck, 2013, \ntests, hypothesis, p-value, proportion confidence interval, survival analysis, censored, poisson, error, bias, continuous, categorical, spread, gauss, log-normal, geometric, significance, tests, type i, type ii, equivalence, comparison, group, outlier, khi-squared, propective, experimental, relative risk, survey, two mean, groups, correlation, regression, model, nonlinear, logistic, anova, nonparametric, sensitivity, roc, bayes, sample, sampling, cases\n (BANQ)\n\n\nComprendre et r\u00e9aliser les tests statistiques \u00e0 l\u2019aide de R: manuel de biostatistique, De Boeck, 2014, \nr, descriptive statistics, random variable, data table, scientific methodology, survey, outliers, tests, probability distribution, binomial, multinomial, pascal, negative binomial, geometric, hypergeometric, poisson, laplace-gauss, normal, exponential, gamma, chi-squared, fisher-snedecor, student, mann-whitney, wilcoxon, hypothesis, type i, type ii, bias, error, proportion comparison, conformity, homogeneity, g, mantel-haenszel, mac nemar, mean comparison, welch, anova, median comparison, mann-whitney-wilcoxon, kruskal-wallis, variance comparison, ansari-bradley, bartlett, fligner-killeen, correlation comparison, pearson, spearman, kendall, distribution comparison, kolmogorov-smirnov, shapiro-wilk, regression, survival, covariance, central limit\n (BANQ)\n\n\nData Mining with R, Learning with Case Studies, CRC Press, 2011 (PAPER)\n\n\nData Scientist et langage R, ENI, (Table des mati\u00e8res)\n\n\nLarge Scale Machine Learning with Spark, Packt, 2016\n\n\nLearning Data Mining with R, Develop key skills and techniques with R to create and customize data mining algorithms, Packt, 2015\n\n\nLearning Predictive Analytics with R, Packt, 2015\n\n\nMastering RStudio, Packt, 2015\n\n\nModeling Techniques in Predictive Analytics, Business Problems and Solutions with R, Pearson, 2014 (BANQ)\n\n\nMod\u00e9lisation pr\u00e9dictive et apprentissage statistique avec R, Technip, 2015 (BANQ)\n\n\nPolitical Analysis Using R, Springer, 2015 (PAPER)\n\n\nR for Data Science, Import, Tidy, Transform, Visualize, and Model Data, O\u2019Reilly, 2017, \nggplots2, workflow, transformation, dplyr, filter, arrange, select, add, group, mutate, summary, script, exploratory, question, variation, missing value, covariation, patterns, models, wrangle, tibbles, data frames, older code, readr, parsing, writing, tidy, spreading, gathering, separating, pull, missing value, case, nontidy, mutating, joins, stringr, string, regular expression, regex, pattern, factor, farcats, social survey, factor order, levels, dates, times, spans, zones, pipe, magrittr, function, conditional, arguments, return, vector, atomic recursive vector, iteration, purrr, loops, map, failure, modelr, visualizing, formulas, families, missing values, model, broom, gapminder, list-columns, markdown, output, document, notebook, presentation, dashboard, interactivity, website, format\n\n\n=R for Marketing Research and Analytics, Springer, 2015\n\n\nR for Microsoft Excel Users, Making the Transition, Pearson Education, 2017 \n\n\nR Graphics Cookbook, O\u2019Reilly, 2013 (PAPER)\n\n\nR in Action, Data Analysis and Graphics with R, 2\nnd\n Edition, Manning, 2015\n\n\nRegression Modeling Strategies With Applications to Linear Models\n\n, Logistic and Ordinal Regression, and Survival Analysis, 2\nnd\n edition, 2015, \nlinear, cubic spline, knot, nonparametric, tree-based, interaction, ordinal, missing data, reduction, clustering, scaling, simultaneous, transformation, scoring, predictive modeling, influential, observations, bootstrap, resampling, sampling, validating, simplifying, generalized, principal components, smoothers, hypothesis test, maximum likelihood, ninary, logistic, ordinal, transform, survival analysis, nonparametric, parametric, cox, proportinal hazards, accelerated failure time, aft\n\n\nS\u00e9ries temporelles avec R, Springer, 2011 (PAPER)\n\n\nStatistiques en sciences humaines avec R, de boeck, 2014 (PAPER)\n\n\nText Mining and Visualization, Case Studies Using Open-Source ToolsCRC Press (RapidMiner, Knime, Python, R), 2016, \nrapidminer, text analytics, corpus, token, repository, mining, visualization, documents, rank-frequency, sequential window, zipf-mandelbrot, knime, preprocessing, frequencies, transformation, data table, social media, network mining, slashdot, python, mongodb, sparse matrix, character encoding, web scraping, cleansing, visualization, exploration, classification, clustering, pca, principal compenent analysis, sentiment, mining search, logs, r\n (BANQ)\n\n\nText Mining with R, O\u2019Reilly, 2017\n\n\nWeb Application Development with R Using Shiny, Packt, 2013\n\n\nR Compilation\n\u00b6\n\n\n\n\nA Beginner\u2019s Guide to R [Zuur, Ieno & Meesters 2009-07-02]\n\n\nA First Course in Statistical Programming with R [Braun & Murdoch 2008-01-28]\n\n\nA Handbook of Statistical Analyses using R (2\nnd\n ed.) [Everitt & Hothorn 2009-07-20]\n\n\nA Modern Approach to Regression with R [Sheather 2009-03-11]\n\n\nA Practical Guide to Ecological Modelling Using R as a Simulation Platform [Soetaert & Herman 2008-11-21]\n\n\nAdaptive Design Theory and Implementation using SAS and R [Chang 2007-06-27]\n\n\nAdvances in Social Science Research Using R - Vinod H. - 2011\n\n\nAn Introduction to Analysis of Financial Data with R [Tsay 2012-10-29]\n\n\nAn Introduction to Bootstrap Methods with Applications to R [Chernick & LaBudde 2011-11-01]\n\n\nAnalysis of Categorical Data with R [Bilder & Loughin 2014-08-11]\n\n\nAnalysis of Correlated Data with SAS and R (3\nrd\n ed.) [Shoukri & Chaudhary 2007-05-17]\n\n\nAnalysis of Financial Time Series\n\n\nAnalysis of Integrated and Cointegrated Time Series with R - Pfaff B. - 2008\n\n\nAnalysis of Phylogenetics and Evolution with R (2\nnd\n ed.) [Paradis 2011-11-09]\n\n\nAnalyzing Baseball Data with R [Marchi & Albert 2013-10-29]\n\n\nAnalyzing Linguistic Data - A Practical Introduction to Statistics using R - Baayen R. - 2008\n\n\nAnalyzing Sensory Data with R [L\u00ea & Worch 2014-10-09]\n\n\nAnalyzing Spatial Models of Choice and Judgment with R [Armstrong, Bakker, Carroll, Hare, Poole & Rosenthal 2014-02-07]\n\n\nApplied Bayesian Statistics With R and OpenBUGS Examples [Cowles 2013-01-03]\n\n\nApplied Statistical Genetics with R - Foulkes A. - 2009\n\n\nApplied Statistics Using SPSS, STATISTICA, MATLAB and R - Marques J. - 2007\n\n\nBasic R for Finance [W\u00fcrtz, Lam, Ellis & Chalabi 2010]\n\n\nBayesian Essentials with R (2\nnd\n ed.) [Marin & Robert 2013-10-29]\n\n\nBayesian Networks in R With Applications in Systems Biology [Nagarajan, Scutari & L\u00e8bre 2013-04-27]\n\n\nBayesian Networks With Examples in R [Scutari & Denis 2014-06-20]\n\n\nBeginner\u2019s Guide to R - Zuur A. et al. - 2009\n\n\nBeginning Data Science with R [Pathak 2014-12-09]\n\n\nBeginning R - An Introduction to Statistical Programming - Pace L.\n\n\nBeginning R - The Statistical Programming Language - Gardener M. - 2012\n\n\nBehavioral Research Data Analysis with R - Li Y. et al. - 2012\n\n\nBioinformatics and Computational Biology Solutions using R and Bioconductor [Gentleman, Irizarry, Carey, Dudolt & Huber 2005-08-31]\n\n\nBioinformatics with R Cookbook [Sinha 2014-07-23]\n\n\nBiostatistical Design and Analysis Using R - A Practical Guide - Logan M. - 2010\n\n\nBiostatistical Design and Analysis using R_ A Practical Guide [Logan 2010-05-10]\n\n\nBiostatistics with R An Introduction to Statistics through Biological Data [Shahbaba 2011-12-17]\n\n\nBusiness Analytics for Managers - Jank W. - 2011\n\n\nChemometrics with R Multivariate Data Analysis in the Natural Sciences and Life Sciences [Wehrens 2011-01-31]\n\n\nClinical Trial Data Analysis - Using R - Din Chen, Karl E. Peace - 2010\n\n\nCompeting Risks and Multistate Models with R - Beyersmann J. et al. - 2012\n\n\nComputational Actuarial Science with R [Charpentier 2014-08-26]\n\n\nComputational Finance An Introductory Course with R [Arratia 2014-05-09]\n\n\nComputational Statistics An Introduction to R [Sawitzki 2009-01-26]\n\n\nContingency Table Analysis Methods and Implementation using R [Kateri 2014-06-15]\n\n\nData Analysis and Graphics Using R - An Example-Based Approach, 3e - Maindonald J. et al. - 2010\n\n\nData Analysis using Regression and Multilevel_Hierarchical Models [Gelman & Hill 2006-12-18]\n\n\nData Manipulation with R (2\nnd\n ed.) [Abedin & Das 2015-03-31]\n\n\nData Manipulation with R - Spector P. - 2008\n\n\nData Mashups in R - Leipzig J. et al. - 2011\n\n\nData Mining with Rattle and R The Art of Excavating Data for Knowledge Discovery [Williams 2011-08-04]\n\n\nData Science in R A Case Studies Approach to Computational Reasoning and Problem Solving [Nolan & Lang 2015-03-18]\n\n\nData Wrangling with R - Boehmke B. - 2016\n\n\nDoing Bayesian Data Analysis A Tutorial with R and BUGS [Kruschke 2010-11-10]\n\n\nDynamic Documents with R and knitr (2\nnd\n ed.) [Xie 2015-07-06]\n\n\nDynamic Linear Models with R - Petris G. et al. - 2009\n\n\nEnvStats An R Package for Environmental Statistics [Millard 2013-10-28]\n\n\nExploratory Multivariate Analysis by Example - Using R - Husson F. et al. - 2011\n\n\nExploring Everyday Things with R and Ruby - Sheong Chang S. - 2012\n\n\nFinancial Risk Modelling and Portfolio Optimization with R [Pfaff 2013-01-22]\n\n\nForest Analytics with R - An Introduction - Robinson A. et al. - 2011\n\n\nFoundational and Applied Statistics for Biologists using R [Aho 2013-12-17]\n\n\nFunctional and Phylogenetic Ecology in R [Swenson 2014-03-27]\n\n\nFunctional Data Analysis with R and MATLAB - Ramsay J. et al. - 2011\n\n\nGetting Started with RStudio - Verzani J. - 2011\n\n\nGraphical Models with R [H\u00f8jsgaard, Edwards & Lauritzen 2012-02-23]\n\n\nGraphics for Statistics and Data Analysis with R [Keen 2010-04-26]\n\n\nGraphing Data with R An Introduction - 2015\n\n\nGuidebook to R Graphics using Microsoft Windows [Takezawa 2012-03-13]\n\n\nHandbook of Statistical Analyses Using R, 2e - Everitt B. et al. - 2010\n\n\nHands-On Programming with R_ Write Your Own Functions and Simulations [Grolemund 2014-08-02]\n\n\nHidden Markov Models for Time Series_ An Introduction using R [Zucchini & MacDonald 2009-04-28]\n\n\nInstant R Starter [Teutonico 2013-04-23]\n\n\nInteractive and Dynamic Graphics for Data Analysis - With R and GGobi - Cook D. et al. - 2007\n\n\nIntroduction to Applied Multivariate Analysis with R - Everitt B. et al. -2011\n\n\nIntroduction to Data Analysis with R for Forensic Scientists [Curran 2010-07-30]\n\n\nIntroduction to Image Processing using R Learning by Examples [Frery & Perciano 2013-01-31]\n\n\nIntroduction to Probability and Statistics Using R - Kerns G. - 2010\n\n\nIntroduction to Probability Simulation and Gibbs Sampling with R - Suess E. et al. - 2010\n\n\nIntroduction to Probability with R [Baclawski 2008-01-24]\n\n\nIntroduction to R for Quantitative Finance [Dar\u00f3czi, Cs\u00f3ka, Tulassay, Puhle, Havran, V\u00e1radi, Berlinger, Michaletzky & Vidovics-Dancs 2013-11-22]\n\n\nIntroduction to Scientific Programming and Simulation using R (2\nnd\n ed.) [Jones, Maillardet & Robinson 2014-06-12]\n\n\nIntroduction to Statistics through Resampling Methods and R (2\nnd\n ed.) [Good 2013-02-11]\n\n\nIntroductory Statistics with R, 2e - Dalgaard P. - 2008\n\n\nIntroductory Time Series with R - Cowpertwait P. et al. - 2009\n\n\nLatent Variable Modeling using R_ A Step-by-Step Guide [Beaujean 2014-05-08]\n\n\nLatent Variable Modeling with R [Finch & French 2015-07-01]\n\n\nLearning Data Mining with R [Makhabel 2014-12-22]\n\n\nLearning Predictive Analytics with R- Eric Mayor - 2015\n\n\nLearning R A Step-by-Step Function Guide to Data Analysis [Cotton 2013-09-26]\n\n\nLinear Mixed-Effects Models using R_ A Step-by-Step Approach [Galecki & Burzykowski 2013-02-05]\n\n\nLinear Models with R (2\nnd\n ed.) [Faraway 2014-07-01]_\n\n\nMachine Learning with R Cookbook [Chiu 2015-03-31]\n\n\nMachine Learning with R [Lantz 2013-10-25]\n\n\nMaking Your Case Using R for Program Evaluation [Auerbach & Zeitlin 2015-07-06]\n\n\nMastering Predictive Analytics with R [Forte 2015-06-30]\n\n\nMastering Scientific Computing with R [Gerrard & Johnson 2015-02-27]\n\n\nMathematical Statistics with Resampling and R - Chihara Laura M., Hesterberg Tim C. - 2011\n\n\nMATLAB Graphics and Data Visualization Cookbook 2012\n\n\nMaximum Likelihood Estimation and Inference_ With Examples in R, SAS and ADMB [Millar 2011-09-19]\n\n\nMixed Models_ Theory and Applications with R (2\nnd\n ed.) [Demidenko 2013-08-05]\n\n\nModern Actuarial Risk Theory - Using R - Kaas R. et al. - 2008\n\n\nModern Analysis of Customer Surveys With Applications using R [Kenett & Salini 2012-01-30]\n\n\nModern Approach to Regression with R - Sheather S. - 2009\n\n\nModern Industrial Statistics_ With Applications in R, MINITAB and JMP (2\nnd\n ed.) [Kenett, Zacks & Amberti 2014-01-28]\n\n\nModern Optimization with R [Cortez 2014-09-07]\n\n\nModern Regression Techniques Using R - Wright D. et al. - 2009\n\n\nModern Statistical Methods for Astronomy_ With R Applications [Feigelson & Babu 2012-08-27]\n\n\nMorphometrics with R - Claude J. - 2008\n\n\nMultilevel Modeling using R [Finch, Bolin & Kelley 2014-06-13]\n\n\nMultiple Comparisons using R [Bretz, Hothorn & Westfall 2010-07-27]\n\n\nMultistate Analysis of Life Histories with R [Willekens 2014-09-12]\n\n\nMultivariate Generalized Linear Mixed Models using R [Berridge & Crouchley 2011-04-25]\n\n\nMultivariate Methods of Representing Relations in R for Prioritization Purposes [Myers & Patil 2012-03-24]\n\n\nMultivariate Nonparametric Regression and Visualization_ With R and Applications to Finance [Klemel\u00e4 2014-05-27]\n\n\nMultivariate Time Series Analysis_ With R and Financial Applications [Tsay 2013-12-09]\n\n\nNonlinear Regression with R - Ritz C. et al. - 2008\n\n\nNonparametric Hypothesis Testing_ Rank and Permutation Methods with Applications in R [Bonnini, Corain, Marozzi & Salmaso 2014-09-15]\n\n\nNonparametric Statistical Methods using R [Kloke & McKean 2014-10-09]\n\n\nNumerical Ecology with R - Borcard D. et al. - 2011\n\n\nPractical Data Science with R [Zumel & Mount 2014-04-13]\n\n\nPractical Graph Mining with R [Samatova, Hendrix, Jenkins, Padmanabhan & Chakraborty 2013-07-15]\n\n\nPractical Guide to Ecological Modelling - Using R as a Simulation Platform - Soetaert K. et al. - 2009\n\n\nPrimer of Ecology with R - Henry M. et al. - 2009\n\n\nPrimer to Analysis of Genomic Data using R [Gondro 2015-05-20]\n\n\nProbability With Applications and R [Dobrow 2013-11-04]\n\n\nProgramming Graphical User Interfaces in R - Lawrence M., Verzani J. - 2012\n\n\nQuantitative Trading with R Understanding Mathematical and Computational Tools from a Quant\u2019s Perspective [Georgakopoulos 2015-01-06]\n\n\nR Book - Crawley M. - 2007\n\n\nR by Example - Jim Albert, Maria Rizzo - 2012\n\n\nR Companion to Linear Statistical Models - Hay-Jahans C. - 2012\n\n\nR Cookbook - Proven Recipes for Data Analisys, Statistics, and Graphics - Teetor P. - 2011\n\n\nR Data Visualization Cookbook [Gohil 2015-01-29]\n\n\nR for Business Analytics [Ohri 2012-09-14]\n\n\nR for Cloud Computing An Approach for Data Scientists [Ohri 2014-11-15]\n\n\nR for Data Science [Toomey 2014-12-19]\n\n\nR for SAS and SPSS Users, 2e - Muenchen R - 2011\n\n\nR for Stata Users - Muenchen R. et al. - 2011\n\n\nR Graph Essentials [Lillis 2014-09-24]\n\n\nR Graphs Cookbook (2\nnd\n ed.) [Abedin & Mittal 2014-10-20]\n\n\nR Graphs Cookbook - Mittal H. - 2011\n\n\nR Graphs Cookbook [Mittal 2011-01-14]\n\n\nR High Performance Programming [Lim & Tjhi 2015-01-30]\n\n\nR in a Nutshell_ A Desktop Quick Reference (2\nnd\n ed.) [Adler 2012-10-19]\n\n\nR in Action - Kabacoff R. - 2011\n\n\nR in Action_ Data Analysis and Graphics with R [Kabacoff 2011-08-27]\n\n\nR Inferno - Burns P. - 2009\n\n\nR Machine Learning Essentials [Usuelli 2014-11-25]\n\n\nR Object-Oriented Programming [Black 2014-10-23]\n\n\nR Programming for Bioinformatics - Gentelman R. - 2009\n\n\nR Quick Syntax Reference [Tollefson 2014-04-25]\n\n\nR Recipes A Problem-Solution Approach [Pace 2014-12-19]\n\n\nR Statistical Application Development by Example Beginner\u2019s Guide [Tattar 2013-07-24]\n\n\nR Through Excel - Heiberger R. et al. - 2009\n\n\nReproducible Research with R and RStudio [Gandrud 2013-07-15]\n\n\nSAS and R Data Management, Statistical Analysis, and Graphics (2\nnd\n ed.) [Kleinman & Horton 2014-07-17]\n\n\nSimulation and Inference for Stochastic Differential Equations_ With R Examples [Iacus 2008-05-05]\n\n\nSix Sigma with R - Statistical Engineering for Process Improvement - Cano E. et al. - 2012\n\n\nSocial Media Mining with R [Danneman & Heimann 2014-03-24]\n\n\nSoftware for Data Analysis - Programming with R - Chambers J. - 2008\n\n\nSolving Differential Equations in R [Soetaert, Cash & Mazzia 2012-06-07]\n\n\nStated Preference Methods using R [Aizaki, Nakatani & Sato 2014-08-15]\n\n\nStatistical Analysis of Financial Data in R (2\nnd\n ed.) [Carmona 2013-12-14]\n\n\nStatistical Analysis of Network Data with R [Kolaczyk & Cs\u00e1rdi 2014-05-23]\n\n\nStatistical Analysis of Questionnaires_ A Unified Approach Based on R and Stata [Bartolucci, Bacci & Gnaldi 2015-08-07]\n\n\nStatistical Analysis with R - Beginner\u2019s Guide - Quick J. - 2010\n\n\nStatistical Bioinformatics with R - Mathur S. - 2010\n\n\nStatistical Data Analysis Explained - Applied Environmental Statistics with R - Reimann C. et al. - 2008\n\n\nStatistical Methods for Environmental Epidemiology with R_ A Case Study in Air Pollution and Health [Peng & Dominici 2008-07-25]\n\n\nStatistical Tools for Nonlinear Regression_ A Practical Guide with S-PLUS and R Examples (2\nnd\n ed.) [Huet, Bouvier, Poursat & Jolivet 2003-09-12]\n\n\nStatistics An Introduction using R (2\nnd\n ed.) [Crawley 2014-11-24]\n\n\nStatistics and Data Analysis for Financial Engineering With R Examples (2\nnd\n ed.) [Ruppert & Matteson 2015-04-22]\n\n\nStatistics and Data Analysis for Microarrays using R and Bioconductor (2\nnd\n ed.) [Draghici 2016-06-15]\n\n\nStatistics and Data with R - An Applied Approach Through Examples - Cohen Y. et al.- 2008\n\n\nStatistics for Censored Environmental Data using Minitab and R (2\nnd\n ed.) [Helsel 2012-02-01]\n\n\nStatistics for Linguistics with R A Practical Introduction (2\nnd\n ed.) [Gries 2013-03-15]\n\n\nStatistics Using R with Biological Examples - Seefeld K. et al. - 2007\n\n\nText Analysis with R for Students of Literature [Jockers 2014-06-11]\n\n\nThe Essential R Reference [Gardener 2012-11-19]\n\n\nThe R Book (2\nnd\n ed.) [Crawley 2012-12-26]\n\n\nThe R Primer [Ekstr\u00f8m 2011-08-29]\n\n\nThe R Software_ Fundamentals of Programming and Statistical Analysis [de Micheaux, Drouilhet & Liquet 2014-02-28]\n\n\nTime Series - Application to Finance with R and S-Plus, 2e - Chan N. - 2010\n\n\nTime Series Analysis - With Applications in R, 2e - Cryer J. et al. - 2008\n\n\nTime Series Analysis and Its Applications - With R Examples, 3e - Shumway R. et al. - 2010\n\n\nUnderstanding Statistics using R [Schumacker & Tomek 2013-01-24]\n\n\nUsing R and RStudio for Data Management, Statistical Analysis, and Graphics (2\nnd\n ed.) [Horton & Kleinman 2015-03-17]\n\n\nUsing R for Data Management, Statistical Analysis, and Graphics - Horton N. et al. - 2011\n\n\nUsing R for Numerical Analysis in Science and Engineering [Bloomfield 2014-04-24]\n\n\nUsing R for Statistics [Stowell 2014-06-24]\n\n\nWavelet Methods in Statistics with R - Nason G. - 2008\n\n\nXML and Web Technologies for Data Sciences with R [Nolan & Lang 2014-01-27]\n\n\n\n\nSQL, NoSQL\n\u00b6\n\n\nHigh Performance MySQL, Second Edition, O\u2019Reilly, 2008\n\n\nMySQL High Availability, O\u2019Reilly, 2010\n\n\nPostgreSQL Administration Cookbook, Package, 2017\n\n\nStorytelling, Visualization\n\u00b6\n\n\nGood Charts, The HBR Guide, HBR Press, 2016\n\n\nHow to Use Excel Sparklines (notes)\n\n\nInformation Dashboard Design, The Effective Visual Communication of Data, O\u2019Reilly, 2006\n\n\nL\u2019histoire du monde en infographie, Marabout, 2017\n\n\nStorytelling with data, Wiley, 2015\n\n\nExcel Sparklines (notes)\n\n\nVIS BOOKS\u2026\n\n\nTableau\n\u00b6\n\n\ndesktop Tableau Getting Started 9.0\n\n\nTableau 10 Business Intelligence Cookbook, Packt, 2016\n\n\nWhich chart or graph is right, Tableau (article), 2012",
            "title": "Collection"
        },
        {
            "location": "/collection/#bash-powershell",
            "text": "Learning the bash Shell, Third Edition, O\u2019Reilly, 2005  Managing Projects with GNU Make, Third Edition, O\u2019Reilly, 2005  The Linux Command Line (completed course, notes, snapshots, codes, manual, exercises), No Starch Press, living book!",
            "title": "Bash, Powershell"
        },
        {
            "location": "/collection/#big-data-spark-cloud",
            "text": "Advanced Analytics with Spark, Patterns for learning from Data at Scale, O\u2019Reilly, 2017  Apache Spark 2 Cookbook, 2 nd  Edition  AWS Amazon Web Services, CC, 2017  Introducing Data Science, Big Data, Machine Learning and more, using Python tools, Manning, 2016,  big data, project management, machine learning, large data, steps, nosql, graph databases, text mining, text analytics, data visualization, user  Large Scale Machine Learning with Spark, Packt, 2016  Learning PySpark, Packt, 2017  Spark 2.0 for Beginners, Packt, 2016",
            "title": "Big Data, Spark, Cloud"
        },
        {
            "location": "/collection/#blockchain",
            "text": "Diverse\u2026",
            "title": "Blockchain"
        },
        {
            "location": "/collection/#datacamp",
            "text": "Introduction to R (Beta) (completed course, slides),  variable, workspace, comments, scripts, data type, logical, numeric, character, double, complex, raw, coercion, coerce, as., create name, vectors, calculus, element, sum, subset, matrix, rbind, cbind, bind, rownames, colnames, colsums, rowsums, scalar, factor, categorical, order, levels, nominal, ordinal, order, list, extend, data frame, data.frame, sort, add column, row, sorting, graphics, plot numerical, plot categorical, hist, bins, barplot, boxplot, pairs, par, parameters, line type, lty, plot symbol, pch, multiple plots, mfrow, mfcol, grid, layout, stack  Intermediate R (completed course, slides, snapshots),  relational operators, greater, lower, equal, logical operators, and , or, not, conditional, if, else, else if, loops, while, infinite, break, for, list, function, print, function documentation, na.rm, built-in functions, arguments, args, create functions, optional argument, return, packages, install, load, search, apply, lapply, sapply, vapply, identical, abs, absolute, round, sum, mean, sequence, seq, repeat, rep, sort, string, str, is., as., append, reverse, rev, regular expressions, grepl, grep, sub, gsub, subset, time, date, sys.date, sys.time, posixct, lubridate, zoo, xts, time series packages  Importing Data into R (completed course, snapshots),  flat files, excel files, spss files, sas files, stata files, postgresql database, mysql database, sqlite database, web, utils, read.table, header, separator, stringasfactors,  read.csv, comma, dot, , semi-colon, tab, delimited, states_eu.csv, readr, data.table, read_delim, col_names, col_types, skip, n_max, n_min, fread, readxl, excel_sheets, gdata, xlsx, xlconnect, loadworkbook, getsheets, readworksheet, createsheet, writeworksheet, saveworkbook, haven, foreign, read_sas, read_stata, read_dta,  as_factor, read_spss, read.dta, convert.factors, read.spss, rmysql, dbconnect, dblisttables, dbreadtable, dbgetquery, dbfetch, dbdisconnect, rpostgresql, web, path, download.file, url, authentication, dest_path, httr, api, json, twitter, jsonlite, array, nesting, tojson, prettify, minify  Cleaning Data in R (completed course, snapshots),  explore, structure, class, dim, names, str, glimpse, summary, visualize, head, tail, print, hist, plot, tidy, dirty, wide datasets, long datasets, tidyr, key, value, gather column, spread column, separate, unite, convert, conversion, as., lubridate, date, tmd, mdy, hms, ymd_hms, string manipulation, stringr, str_trim, trim, str_pad, str_detect, str_replace, replace, tolower, toupper, lowercase, uppercase, missing, na, not available, empty, dot, infinite value, inf, not a number, nan, outlier, errors, extreme, unexpected  Writing Functions in R (completed course),  define, return value, object, scope, scoping, lookup, data structures, atomic, list, missing very, not available, na, subset, less code, clearer, snippet, temporary, name, naming, argument, order, good function, functional programming, domain, variable, common code, functions as arguments, map, map_dbl, map_lgl, map_int, map_chr, failure, safely, possibly, quietly, map2, iterate, pmap, invoke_map, mapping, map2_dbl, map2_lgl, pmap_dbl, walk, pipe, pipeline, robust, errors, hidden argument, getting, setting options  Data Manipulation in R with dplyr (completed course, snapshots),  pipe, pipeline, tbl, select, filter, arrange, mutate, summarize, summarise, tidyr, tidy, starts_with, ends_with, contains, matches, num_range, one_of, is.na, !is.na, min, sum, sd, max, mean, var, length, median, iqr, first, last, nth, n, n_distinct, %>%, magrittr, group_by, data frame, data table, database, tbl_dt, tbl  Data Analysis in R, the data.table Way (completed course, snapshots, cheat sheet),  data table, dt, select row, i, column, j, compute, by, group, subset, chain, chaining, :=, set, setnames, setcolorder, index, indexing, key, mult, nomatch, two-column key, join, setkey  Data Visualization in R with ggvis (completed course, snapshots),  points, bars, maps, histograms, scatter, density, data, coordinate system, mark, properties, html, javascript, js, THE%>%, magrittr, pipe, pipeline, :=, sets, maps, =, stroke, width, opacity, dash, fill, lines, paths, ribbons, smooths, prediction, model_predictions, later, layer_smooths,  dplyr, layer_paths, layer_lines, group_by, mapping, chart, charting, interactivity, multilayered, axes, axis, legends  Data Visualization with ggplot2 (1) (completed course, snapshots, videos),  graphical data analysis, design, communication, exploratory, explanatory, anscombe plot, fit, fitting, grammar of graphics, layers, data, aesthetics, statistics, geometries, geom_, facets, coordinates, themes, base plot, points, tidy, color, size, shape, attribute, alpha, line type, label, shape, axes, positions, identity, dodge, stack, fill, jitter, jitterdodge, scale, scale functions, limit, breaks, expand, labs, abline, area, bar, bin2d, blank, boxplot, contour, crossbar, density, density2d, dotplot, errorbar, errorbar, freqpoly, hex, histogram, hline, line, linerange, map, path, pointrange, polygon, quantile, raster, rect, vline, ribbon, rug, segment, smooth, step, text, tile, violin, eas, geom, pch, crosshairs, remarks, time series, linetype, size, qplot  Data Visualization with ggplot2 (2) (completed course, snapshots, videos),  statistics, coordinates, facets, themes, best practices, function, geom, geom_, stat_, fill,  bin, histogram, bar, freqpoly, smooth, boxplot, bindot, bin2d, binhex, contour, quantile, sum, boxplot, dotplot, bin2d, hex, contour, quantile, count, summary, confidence interval, qq, quantile-quantile, coord_, coord_cartesian, scale_x_continuous, xlim, coord_cartesian, aspect ratio, facets, edward tufte, tidy, theme layer, element_, text, line, rectangle inheritance, blank, recycling, save theme, reuse theme, discrete x-axis, derivative theme, built-in theme templates, ggthemes, theme update, theme_set, pitfalls, dynamic plot, error bar, pointrange, pie charts, stacked bar chart, haircol, horizontal, heat maps  Reporting with R Markdown (completed course, snapshots, videos),  reproducible, research, knitr, pandoc, shiny,  interactive, web-based, rstudio, html, css, pdf, word, beamer, slidy, ioslides, markdown::render, rmd, latex  Introduction to Machine Learning (completed course, snapshots, notes),  regression, shopping basket analysis, recommendation systems, decision-making, classification, clustering, cluster, k-means, supervised, unsupervised, model performance, error, accuracy, competition time, interpretability, limits, confusion matrix, precision, recall, true, false, real mean squared error, cluster similarity, between cluster sum of squares, inter-cluster distance, dunn\u2019s index, training, train, testing, test, predictive power, split the dataset, cross validation, n-fold, number of validation, bias, variance, quadratic data, overfitting, underfitting, decision tree, numerical, categorical, classify, choose, split, sticking criteria, information gain, pruning, k-nearest neighbours, k-nn, distance, euclidean, scaling, dummy, roc curve, receiver operator characteristic curve, probability, simple regression, linear regression, r-squared, multiple linear regression, adjusted, predictors, assumptions, non-parametric, kernel regression, regression trees, generalized regression, within cluster sums of squares, between cluster sums of squares, k, scree plot, choosing, intercluster, hierarchical clustering, dendrogram, pros and cons  Intro to Statistics with R (completed course, snapshots, videos):   Course One Introduction,  types of variables, nominal, ordinal, interval, ratio, histogram, distribution, bimodal, skewed, skewness, uniform, platykurtic, leptokurtic, scales, z-score, percentile rank, measure of central tendency, average, median, mode, variability, variance, standard deviation  Course Two Student\u2019s T-test,  z-test, t-test,  single sample, dependent, independent, cohen, cohen\u2019s d, confidence interval, t-value, upper bound, lower bound  Course Three Analysis of Variance (ANOVA),  anova, analysis of variance, continuous, independent t-test, dependent t-test, between groups, repeated measures, f-test, f-ratio, post-hoc, tukey\u2019s procedure, tukey, factorial anova, two independent variables, one dependent variable, main effect, interaction effect, simple effect, effect size, homogeneity of variance, normal distribution  Course Four Repeated Measures (ANOVA),  statistical power, order effects, counterbalancing, missing data, extra assumption, homogeneity of variance, homogeneity of covariance, sphericity, systematic, between, unsystematic, within, subjects, f-test, mean-squared, ms, post-hoc test, holm  Course Five Correlation and Regression,  correlation, r, sum of cross products, sp, covariance, variance, magnitude, causation, causality, sampling, sample, measurement, regression, r-squared, estimation, coefficients, assumptions, normal, linear, homoscedasticity, anscombe\u2019s quartet, anscombe, scatterplot, residuals  Course Six Multiple Regression,  regression, simple, multiple, predictors, r-squared, dummy, dummies, matrix algebra, data frame, variance, covariance, standard deviation, correlation, estimation, coefficients, unweighted coding, weighted coding  Course Seven Moderation and Mediation,  experimental manipulation, moderator, moderation, enhance, test for moderation, centering predictors, avoid multicollinearity, constant, mediation, partial, full, sobel test   Big Data Analysis with Revolution R Enterprise (completed course, snapshots, videos),  challenge, move, merge, manage, munge, large datasets, inspect, transform, create available, summarize, xdf, parallel, visualize, histogram, correlation, subset, linear model, nonlinear relationship, categorical, revoscaler, revolution, distributed environments, cluster, information, dayofweek, summary, quantile, crosstabs, facets, weights, scatterplot, log, data frame, import, min, max, apply, means, drop, predict, machine learning, split, train, test, split, cross validation, standard errors, covariance, logit, logistic regression, generalized least square, cluster, clustering, decision trees, regression tree, revotree, roc curve, classification tree, confusion matrix  DataCamp, Data Analysis and Statistical Inference (completed course, snapshots, videos, notes, manuals, codes, datasets),  import, plot, inspect, web, load, table, barplot, mosaicplot, mosaic, summarize, structure, nrow, head, tail, str, summary, boxplot, names, descriptive statistics, histogram, sample, simulation, distribution, population, sampling distribution, for loop, sample size, confidence intervals, standard error, dotplot of samples, inferential statistics, bootstrap, interference, confidence level, bootstrap method, parameter of interest, relationships between two variables, by, anova, continuous, categorical, margin of error, proportion, regression, correlation, plot, moneyball, abline, linearity, normal residuals, constant variability, qqnorm, qqline, boxplot, mosaicplot, kitter, adding, removing, residuals, openintro statistics manual  Data Exploration With Kaggle Scripts (completed course, snapshots, videos, codes, datasets),  portfolio, github, cases, map, mapping, spanish production of silver, chopsticks effectiveness, pigeon data, kaggle account, phd earnings, american community survey  DrivenData Water Pumps Challenge (case),  data mining, water table, train, test, ggplots, visualization, map, well, location, random forest  Exploring Polling Data in R (case),  visualization, pools, election  Having Fun with googleVis (case),  visualization, gapminder, interactive, graphs  How to work with Quandl in R (completed course, snapshots),  data connection, api, python, excel, web, r, database aggregator, library, stocks, finance, economics, census, public services, government, world organizations  Intro to Computational Finance with R (completed course, snapshots, notes, codes, datasets, manuals),  tseries, stock returns, zoo, performanceanalytics, econometrics, risk analysis, financial data, returns, plot, chart, matrix, histogram, boxplot, plot, qqnorm, return distribution, descriptive statistics, args, apply, annualized monthly estimates, graphical analysis, covariance matrix, mvtnorm, simulate, set.seed, sigma, joint probability, rho, probability, expected return model, global minimum variance portfolio, efficient portfolio, efficiency frontier, tangency portfolio, calculate returns, standard error of the variances, hypothesis test of mean, of the correlation, normality, asset returns, class, portfolio theory, t-bills, sharpe slope, the global minimum variance portfolio, efficiency portfolio, quantiles, densities, normal curve, value-at-risk, continuously compounded monthly returns, simple total returns, dividend yields, annual returns, portfolio shares, portfolio returns, price data, index, indices, subset, continuously compounded 1-month returns, monthly compounding, xts, time series, moving average, ma, autoregressive, ar  Kaggle R Tutorial on Machine Learning (completed course, snapshot, codes),  titanic, prediction, decision trees, interpret, predict, summit, overfitting, reengineering, survival rates, pruning, random forest  Plotly Tutorial Plotly and R (case)  R for the Intimidated (introductory course, videos)  R, Yelp and the Search for Good Indian Food (case),  data wrangling, web scraping, filter, sieve, sift, data mining, dplyr  Assessing Tank Production (case),  monte carlo, bootstrap  Credit Risk Modeling in R (completed course, IPython notebook, slides, datasets),  expected loss, probability, default, exposure, loss, crosstable, loan, mortgage, gmodel, interest, grade, ownership, annual income, age, histogram, outlier, missing, confusion matrix, model prediction, classification accuracy, sensitivity, specificity, logistic regression, logit, generalized linear model, glm, table, logarithmic, predicting, prediction, discriminative, train, training, test, testing, probabilistic regression, probit, log-log, loglog, cutoff, decision tree, gini-measure, gini, rpart, sample, dataset, set, glass matrix, pruning, plotcp, printcp, complexity parameter, cp, prune, plotcp, prp, bad rate, fixed acceptance rate, strategy curve, table, bank, roc, comparison, proc, area under curve, auc, model reduction, discriminant analysis, random forest, neural networks, support vector machine, survival analysis  Exploring Pitch Data with R (completed course, IPython notebook, slides, notes, images, manuals, datasets),  velocity, graphical skills, distribution, fastball, hitting, outcomes, game, date, subset, histogram, abline, vertical line, ifelse, tapply, plot, time series, overlap, jitter, multimodal, bimodal, mix, change, table, prop.table, ball-strike cout, pitch usage, expectancy, paste, concatenate, pitch location, strike zone, locational variable, horizontal, vertical, binning, grid, loop, plot, visual interpretation, bat, batting, batted, contact rate, ggplot2, wide, long, locgrid, layer  Introduction to Python & Machine Learning (completed course, IPython notebook, slides, notes, images, manuals, datasets)  Kaggle Python Tutorial on Machine Learning (completed course, IPython notebook, slides, notes, images, manuals, datasets)  Python 3 (including Importing Data, completed courses, IPython notebook, slides, notes, datesets),  data types, float, int, str, bool, list, subset, slice, change, add, remove, function, type, round, method, built-in, max, len, capitalize, replace, bit_length, index, count, index, append, package, library, pip, python3, numpy, array, pip3, 3d array, ndarray, mean, median, corrcoef, std, round, column_stack, print, concatenation, inline, offline, matplotlib, pyplot, plot, show, scatter, hist, xlabel, ylabel, title, yticks, xticks, dictionary, dictionaries, keys, pandas, tabular, table, data frame, dataframe, index, read_csv, select, bracket, row, column, label-based, loc, integer position-based, iloc, comparison, greater, lower, equal, true, false, boolean, and, or, not, conditional, if, else, elif, filter, compare, loop, while, for, enumerate, in, list, dictionary, array, 2d array, my_dict.items, nditer(my_array), iterrows, apply, random, rand, seed, randint, random walk, range, tails, append, recfromcsv, xscale, clean frame, clf, transpose, flat file, csv, txt, comma, tab, delimiters, semi-colon, numpy, np, vector, array, import, essential for scikit-learn, loadtxt, genfromtxt, datatypes, pandas, data frame, dataframe, scipy, excel, matlab, sas, stata, hdf5, pickled, serialize, bytestream, spreadsheet, pickle, pickle.load, pd.excelfile, parse, sas7bdat, file.to_data_frame, pd.read_stata, h5py.file, data.keys, key, keys, value, values, scipy.io, sci.io.loadmat, scipy.io.savemat, .mat, relational database, postgresql, mysql, sqlite, sql, sqlalchemy, create_engine, engine.connect, con.execute, engine_table_name, query, execute, pd.dataframe, fetch, fetchall, close, rs.keys, engine.connect, join, read_sql_query, web, urllib, urlopen, urlretrieve, request, read, read_csv, close,  pd.read_excel, requests, get, scrape, scraping, beautifulsoup, prettify, find_all, api, json, tweepy, authentification, oauthhandler, os, getcsd, listdir  Statistical Thinking in Python (Part 1) (completed course, IPython notebook, slides, notes, datasets)  Text Mining, Bag of Words (completed course, IPython notebook, slides, notes, datasets),  workflow, problem definition, specific goals, identify text, text organization, feature extraction, analysis, insight, recommendation, bag of words, semantic parsing, Documents, corpus, corpora, vcorpus, read.csv, cleaning, preprocessing, tolower, removepunctuation, removenumbers, stripwhitespace, removewords, word stemming, stemdocument, stem_words, stem_completion, turn document matrix, tdm, document time matrix, dtm, word frequency matrix, wfm, tm, rowsums, sort, colsums, term_frequency, qdap, freq_terms, word cloud, stop word, tm_map, function, commonality clouds, commonality.cloud, union, paste, vectorsource, vcorpus, as.matrix, comparison cloud, pyramid plot, pyramid.plot, subset, abs, cbind, data.frame, rownames, word network, word clustering,  hclust, dendrogram, unigram, bifram, trigram, ngram, term weight, single word count, tfldf, penalize word, metadata, readtabular, hr analytics  Tidy Data in Python (completed courses, IPython notebook, datasets)  Time Series in R The Power of xts and zoo (completed courses, IPython notebook, datasets),  extensible time, xts, zoo, matrix, date, seq, core, coredata, convert, as.xts, as.matrix, import, export, iso8601, date, time, intraday, interval, extraction, first, last, coredata, as.numeric, merge, index, inner join, outer, left, right, rbind, cbind, na.locf, na.approx, lag, difference, diff, endpoints, apply, lapply, split, do.call, convert, to.period, to.quarterly, rolling value, time zone, timezones, indexclass, indextz, indexformat, time, periodicity, to.yearly, nmonths, nquarters, nyears, timestamps, indexwday, unique  Visualization with Bokeh (completed courses, IPython notebook, datasets)",
            "title": "DataCamp"
        },
        {
            "location": "/collection/#digital-humanities",
            "text": "Diverse\u2026",
            "title": "Digital Humanities"
        },
        {
            "location": "/collection/#docker",
            "text": "Docker Cookbook, Packt, 2015  Docker in Action, Manning, 2016",
            "title": "Docker"
        },
        {
            "location": "/collection/#econometrics-spatial-gis",
            "text": "A Primer for Spatial Econometrics with Applications in R, Palgrave, 2014  An Introduction to R for Quantitative Economics, Springer, 2015,  rstudio, crude oil price, supply, demand, fish, function, derivative, elasticity, linear, log-log, cobb,douglas, matrix, statistics, regression, simulation, normal, uniform, binomial, central limit theorem, t-test, logit, anscombe, graphs, scatter, growth, time series, time, random walks, cycles, stochastic, difference, air passengers, inflation, phillips curve, stock market  cas (notes)  Mathematics for Marketing Models (notes)  Econometrics by Example, Palgrave Macmillan, 2014  Event History Analysis with R, CRC Press, 2012 (PAPER)  Introduction to Python for Econometrics, Statistics and Data Analysis, CC, 2014  Machine-learning Techniques in Economics, New Tools for Predicting Economics Growth, Springer, 2017  Introducing Survival and Event History Analysis, SAGE, 2011,  survival, event, history, r, data, exploration, descriptive statistics, data structures, nonparametric, kaplan-meier estimator,  \nIntroduction to Stochastic Process with R, Wiley, 2016,  markov chain, stationary, periodicity, ergodic, time, absorbing, branching, probability, extinction, monte carlo, gibbs sampler, sampler, eigenvalue, card shuffling, poisson process, arrival, interarrival, thinning, superposition, uniform distribution, spatial poisson, nonhomogeneous, parting, continuous-time, brownian motion, gaussian process, transformation, properties, variations, applications, ito integral, discrete random variable, joint distribution, continuous random variable, common probability distributions, moment-generating functino, matrix algebra  Spatial Analysis, Statistics, Visualization, and Computational Methods, CRC Press, 2016  Spatial Statistics & Geostatistics, SAGE, 2013 (PAPER)  Statistics Done Wrong, No Starch Press, 2015  The Cox Model and Its Applications, Springer, 2016  The Economics of Obesity, Poverty, Income Inequality and Health, Springer, 2017  Think Stat 2, Exploratory Data Analysis in Python, Green Tea Press, 2014",
            "title": "Econometrics, Spatial, GIS"
        },
        {
            "location": "/collection/#employment-education",
            "text": "21 cl\u00e9s pour activer la transformation num\u00e9rique de votre entreprise, Eyrolles, 2017  Cracking the Coding Interview, 150 Programming Interview Questions and Solutions, 4 th  Edition, CareerCup, 2010,  interview, behind the scenes, while stories, before, resume, behavioural, preparation, questions, mistakes, frequently asked questions, data structure, algorithms, programming languages, problem solving, knowledge  Pr\u00e9venir plut\u00f4t que gu\u00e9rir la r\u00e9volution de la e-sant, Eyrolles, 2017",
            "title": "Employment, Education"
        },
        {
            "location": "/collection/#industry",
            "text": "Big Data Analytics, A Practical Guide for Managers, Auerbach Publications, 2015",
            "title": "Industry"
        },
        {
            "location": "/collection/#latex-markdown",
            "text": "A Crash Course in LaTeX, 2018  LaTeX pour l\u2019impatient, 3e \u00e9dition, MiniMax, 2009  LaTeX pour l\u2019impatient, 4e \u00e9dition, MiniMax, 2016 (BANQ)  LaTeX appliqu\u00e9 aux sciences humaines, Atramenta, 2012  Petit guide pour les d\u00e9butants en LaTeX, 2008  TikZ pour l\u2019impatient, CC, 2017  Tout ce que vous avez toujours voulu savoir sur LaTeX sans jamais oser le demander, CC, 2018",
            "title": "LaTeX, Markdown"
        },
        {
            "location": "/collection/#linux",
            "text": "Python for Unix and Linux System Administration, O\u2019Reilly,",
            "title": "Linux"
        },
        {
            "location": "/collection/#nlp-text",
            "text": "Analyzing Linguistic Data, A Practical Introduction to Statistics using R, cambridge, 2008  Natural Language Processing: Python and NLTK, Packt, 2016  NLTK Essentials, Packt, 2015  Statistics for Linguistics with R, A Practical Introduction, 2 nd  edition, 2013  Text Analysis with R for Students of Literature, Springer, 2014",
            "title": "NLP, Text"
        },
        {
            "location": "/collection/#neural-networks",
            "text": "diverse\u2026  Hands-On Machine Learning with Scikit-Learn & TensorFlow, O\u2019Reilly, 2017",
            "title": "Neural Networks"
        },
        {
            "location": "/collection/#no-starch-press-collection",
            "text": "Absolute OpenBSD, Unix For The Practical Paranoid  Book of Ruby  Building a Server with FreeBSD 7  Cisco Routers for the Desperate, Router Management the Easy Way  FreeBSD Device Drivers  How Linux Works What Every Superuser Should Know-viny \nLinux Appliance Design, A Hands-On Guide to Building Linux Appliances  System and Network Monitoring  Absolute FreeBSD, The Complete Guide to FreeBSD, 2 nd  Edition  Linux Firewalls  The Tangled Web  PGP & GPG, Email for the Practical Paranoid  Practical Packet Analysis  Ruby by Example  The Book of CSS3, A Developer\u2019s Guide to the Future of Web Design  The Book of PF, A No-Nonsense Guide to the OpenBSD Firewall  The Book of Postfix  The Linux Command Line  Webbots Spiders and Screen Scrapers, 2 nd  Edition  Wicked Cool Shell Scripts, 101 Scripts for Linux Mac OS X and Unix Systems  Write Great Code, Volume II, Thinking Low-Level, Writing High-Level  Write Great Code, Volume I, Understanding The Machine",
            "title": "No Starch Press Collection"
        },
        {
            "location": "/collection/#orange",
            "text": "software\u2026",
            "title": "Orange"
        },
        {
            "location": "/collection/#predictive-modeling-data-science-data-mining-marketing-analytics",
            "text": "Analytics for Insurance, The Real Business of Big Data, Wiley, 2016,  risk, management, underwriting, claim, marketing, property, liability, life, pensions, people  Artificial Intelligence for Marketing, Practival Applications, Wiley, 2017  Big Data opportunit\u00e9 ou menace pour l\u2019assurance, RB \u00c9dition, 2016  Marketing Analytics, A Practical Guide to Real Marketing Science, Wiley, 2015,  statistics, consumer, behaviour, strategy, regression, business case, segmentation, elasticity, test, control, lift, collinearity, logistic, market basket analysis, survival, lifetime, value, ltv, descriptive, predictive, simultaneous equations, segment, k0means, lca, rfm, behavioural, missing value, research, conjoint analysis, structural equation, sem, sample, a/b, testing, factorial, engagement  (BANQ)  Data Science for Business, O\u2019Reilly, 2013  Marketing Data Science, Modeling Techniques in Predictive Analytics with R and Python, Wiley, 2015,  market, consumer choice, customer, retention, acquisition, positioning, promiting, recommending, brands, prices, social networks, competitors, predicting, sales, database, regression, bayesian, data mining, machine learning, data visualization, text, sentiment analysis, time series, market response, sampling, www, social media, surveys, experiments, interviews, focus groups, field search cases  Marketing analytics : a practical guide to real marketing science, Kogan Page, 2015 (BANQ)  Real-World Machine Learning, Manning, 2017  Think Like a Data Scientist, Manning, 2017",
            "title": "Predictive Modeling, Data Science, Data Mining, Marketing Analytics"
        },
        {
            "location": "/collection/#programmation-pour-les-kids",
            "text": "Cahier d\u2019activit\u00e9s 3D pour les kids, Eyrolles, 2017  Arduino pour les Nuls poche, 2e \u00e9dition, 2017, First  Sylvia pr\u00e9sente Super Projets Arduino  Programmer avec Arduino en s\u2019amusant, 2017, First",
            "title": "Programmation pour les kids"
        },
        {
            "location": "/collection/#project-management",
            "text": "An Entire MBA in 1 Book, 2016  Effective DevOps, O\u2019Reilly, 2015  Negotiating 101, From Planning Your Strategy  Project Management Metrics, KPIs, and Dashboards, A Guide to Measuring and Monitoring Project Performance, 3 rd  Edition, Wiley, 2017",
            "title": "Project Management"
        },
        {
            "location": "/collection/#pmi-acp",
            "text": "\u2026",
            "title": "PMI-ACP"
        },
        {
            "location": "/collection/#python",
            "text": "An Introduction to Statistics with Python, With Applications in the Life Sciences, Springer, 2016  Apprendre \u00e0 programmer en Python 3, CC, 2012  Automate the Boring Stuff with Python, No Starch Press, 2015  A Whirlwind Tour of Python, O\u2019Reilly, 2016  Bayesian Methods for Hackers, Addison-Wesley \n, 2016,  bayesian, inference, pymc, python, a/b testing, cases, distribution, algorithms, plot, graphs, statistics, probability, loss function, machine learning  (https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)  Building Machine Learning Systems with Python, Get more from your data through creating practical machine learning systems with Python, 2 nd  Edition, Packt, 2015,  iris, classifying, scikit-learn, clustering, bag of words, k-means, topic modelling, data mining, knn, logistic, regression, sentiment analysis, naive bayes, classifier, tweets, clean, word,  cross-validation, penalize, regulatize, lasso, elasticnet, text, predictions, recommendations, basket analysis, classification, music, computer vision, dimension reduction, big data, amazon web service  Building Recommendation Engines, Packt, 2017  Data Science from Scratch First Principles with Python, O\u2019Reilly, 2015,  visualization, linear algebra, statistics, probability, hypothesis, inference, gradient descent, fetch, read, file, scraping, web, api, twitter, exploring, machine learning, k-nearest neighbours, knn, naive bayes, regression, logistic, decision tree, neural networks, clustering, natural language processing, network analysis, recommender system, databases, sql, mapreduce  Data Wrangling with Python Tips and Tools to Make Your Life Easier, O\u2019Reilly, 2016,  excel, spreadsheet, pdf, storing, clean, format, outilier, bad, duplicates, match, regex, standardizing, scripting, exploration, presenting, reporting, web, scraping, screen, spiders, api, automation, scaling    Flask Building Python Web Services, Packt, 2017  Hands-On Machine Learning with Scikit-Learn & TensorFlow, O\u2019Reilly, 2017  Python Geospatial Analysis Cookbook, Packt, 2015  Introducing Data Science, Big Data, Machine Learning and more, using Python tools, Manning, 2016  Geoprocessing with Python, Manning, 2016  Introduction to Machine Learning with Python, O\u2019Reilly, 2017  Learning Predictive Analytics with Python, Packt, 2016  Mastering Python Regular Expressions, Packt, 2014  Managing your Biological Data with Python, CRC Press, 2014 (PAPER)  Mastering Python Regular Expressions, Leverage regular expressions in Python even for the most complex features, Packt, 2014  Practical Data Analysis, Packt, 2013  Practical Data Science Cookbook, 2 nd  Edition, Packt, 2017  Python 3, Apprendre \u00e0 programmer en Python avec Pyzo et Jupyter Notebook, Dunod, 2017  Python in 8 hours, 2017  Python Data Analysis, 2 nd  Edition, Packt, 2017  Practical Statistics for Data Scientists 50 Essential Concepts, O\u2019Reilly, 2017  Python Data Analysis, 2 nd  edition, Packt, 2017  Python Data Analysis Cookbook, Packt, 2016  Python Data Analytics and Visualization, Packt, 2017  Python Data Science Handbook, O\u2019Reilly, 2017  Python Data Visualization, 2 nd  edition, Packt, 2015  Python End-to-end Data Analysis, Packt, 2016  Python for R Users, Wiley, 2018  Python Geospatial Analysis Essentials, Packt 2015  Python Geospatial Development, Third Edition, Packt, 2016  Python Machine Learning Cookbook, Packt, 2016  Real-World Machine Learning, Manning, 2017  Une introduction \u00e0 Python 3, CC   Web Scraping with Python Collecting Data from the Modern Web, O\u2019reilly, 2015",
            "title": "Python"
        },
        {
            "location": "/collection/#r-statistics",
            "text": "An Introduction to Statistical Learning with Applications in R, Springer, 2015  Analyse de donn\u00e9es avec R, PUR, 2016,  pca, principal components, fa, factorial, factor, analysis, multiple, correspondence, classifier, classification, visualization  (BANQ)  Applied Spatial Data Analysis with R, Springer, 2 nd  edition, 2013  A Tiny Handbook of R Springer, 2011  Basic Data Analysis for Time Series with R, Wiley, 2014  Biostatistique: une approche intuitive, De Boeck, 2013,  tests, hypothesis, p-value, proportion confidence interval, survival analysis, censored, poisson, error, bias, continuous, categorical, spread, gauss, log-normal, geometric, significance, tests, type i, type ii, equivalence, comparison, group, outlier, khi-squared, propective, experimental, relative risk, survey, two mean, groups, correlation, regression, model, nonlinear, logistic, anova, nonparametric, sensitivity, roc, bayes, sample, sampling, cases  (BANQ)  Comprendre et r\u00e9aliser les tests statistiques \u00e0 l\u2019aide de R: manuel de biostatistique, De Boeck, 2014,  r, descriptive statistics, random variable, data table, scientific methodology, survey, outliers, tests, probability distribution, binomial, multinomial, pascal, negative binomial, geometric, hypergeometric, poisson, laplace-gauss, normal, exponential, gamma, chi-squared, fisher-snedecor, student, mann-whitney, wilcoxon, hypothesis, type i, type ii, bias, error, proportion comparison, conformity, homogeneity, g, mantel-haenszel, mac nemar, mean comparison, welch, anova, median comparison, mann-whitney-wilcoxon, kruskal-wallis, variance comparison, ansari-bradley, bartlett, fligner-killeen, correlation comparison, pearson, spearman, kendall, distribution comparison, kolmogorov-smirnov, shapiro-wilk, regression, survival, covariance, central limit  (BANQ)  Data Mining with R, Learning with Case Studies, CRC Press, 2011 (PAPER)  Data Scientist et langage R, ENI, (Table des mati\u00e8res)  Large Scale Machine Learning with Spark, Packt, 2016  Learning Data Mining with R, Develop key skills and techniques with R to create and customize data mining algorithms, Packt, 2015  Learning Predictive Analytics with R, Packt, 2015  Mastering RStudio, Packt, 2015  Modeling Techniques in Predictive Analytics, Business Problems and Solutions with R, Pearson, 2014 (BANQ)  Mod\u00e9lisation pr\u00e9dictive et apprentissage statistique avec R, Technip, 2015 (BANQ)  Political Analysis Using R, Springer, 2015 (PAPER)  R for Data Science, Import, Tidy, Transform, Visualize, and Model Data, O\u2019Reilly, 2017,  ggplots2, workflow, transformation, dplyr, filter, arrange, select, add, group, mutate, summary, script, exploratory, question, variation, missing value, covariation, patterns, models, wrangle, tibbles, data frames, older code, readr, parsing, writing, tidy, spreading, gathering, separating, pull, missing value, case, nontidy, mutating, joins, stringr, string, regular expression, regex, pattern, factor, farcats, social survey, factor order, levels, dates, times, spans, zones, pipe, magrittr, function, conditional, arguments, return, vector, atomic recursive vector, iteration, purrr, loops, map, failure, modelr, visualizing, formulas, families, missing values, model, broom, gapminder, list-columns, markdown, output, document, notebook, presentation, dashboard, interactivity, website, format  =R for Marketing Research and Analytics, Springer, 2015  R for Microsoft Excel Users, Making the Transition, Pearson Education, 2017   R Graphics Cookbook, O\u2019Reilly, 2013 (PAPER)  R in Action, Data Analysis and Graphics with R, 2 nd  Edition, Manning, 2015  Regression Modeling Strategies With Applications to Linear Models \n, Logistic and Ordinal Regression, and Survival Analysis, 2 nd  edition, 2015,  linear, cubic spline, knot, nonparametric, tree-based, interaction, ordinal, missing data, reduction, clustering, scaling, simultaneous, transformation, scoring, predictive modeling, influential, observations, bootstrap, resampling, sampling, validating, simplifying, generalized, principal components, smoothers, hypothesis test, maximum likelihood, ninary, logistic, ordinal, transform, survival analysis, nonparametric, parametric, cox, proportinal hazards, accelerated failure time, aft  S\u00e9ries temporelles avec R, Springer, 2011 (PAPER)  Statistiques en sciences humaines avec R, de boeck, 2014 (PAPER)  Text Mining and Visualization, Case Studies Using Open-Source ToolsCRC Press (RapidMiner, Knime, Python, R), 2016,  rapidminer, text analytics, corpus, token, repository, mining, visualization, documents, rank-frequency, sequential window, zipf-mandelbrot, knime, preprocessing, frequencies, transformation, data table, social media, network mining, slashdot, python, mongodb, sparse matrix, character encoding, web scraping, cleansing, visualization, exploration, classification, clustering, pca, principal compenent analysis, sentiment, mining search, logs, r  (BANQ)  Text Mining with R, O\u2019Reilly, 2017  Web Application Development with R Using Shiny, Packt, 2013",
            "title": "R, Statistics"
        },
        {
            "location": "/collection/#r-compilation",
            "text": "A Beginner\u2019s Guide to R [Zuur, Ieno & Meesters 2009-07-02]  A First Course in Statistical Programming with R [Braun & Murdoch 2008-01-28]  A Handbook of Statistical Analyses using R (2 nd  ed.) [Everitt & Hothorn 2009-07-20]  A Modern Approach to Regression with R [Sheather 2009-03-11]  A Practical Guide to Ecological Modelling Using R as a Simulation Platform [Soetaert & Herman 2008-11-21]  Adaptive Design Theory and Implementation using SAS and R [Chang 2007-06-27]  Advances in Social Science Research Using R - Vinod H. - 2011  An Introduction to Analysis of Financial Data with R [Tsay 2012-10-29]  An Introduction to Bootstrap Methods with Applications to R [Chernick & LaBudde 2011-11-01]  Analysis of Categorical Data with R [Bilder & Loughin 2014-08-11]  Analysis of Correlated Data with SAS and R (3 rd  ed.) [Shoukri & Chaudhary 2007-05-17]  Analysis of Financial Time Series  Analysis of Integrated and Cointegrated Time Series with R - Pfaff B. - 2008  Analysis of Phylogenetics and Evolution with R (2 nd  ed.) [Paradis 2011-11-09]  Analyzing Baseball Data with R [Marchi & Albert 2013-10-29]  Analyzing Linguistic Data - A Practical Introduction to Statistics using R - Baayen R. - 2008  Analyzing Sensory Data with R [L\u00ea & Worch 2014-10-09]  Analyzing Spatial Models of Choice and Judgment with R [Armstrong, Bakker, Carroll, Hare, Poole & Rosenthal 2014-02-07]  Applied Bayesian Statistics With R and OpenBUGS Examples [Cowles 2013-01-03]  Applied Statistical Genetics with R - Foulkes A. - 2009  Applied Statistics Using SPSS, STATISTICA, MATLAB and R - Marques J. - 2007  Basic R for Finance [W\u00fcrtz, Lam, Ellis & Chalabi 2010]  Bayesian Essentials with R (2 nd  ed.) [Marin & Robert 2013-10-29]  Bayesian Networks in R With Applications in Systems Biology [Nagarajan, Scutari & L\u00e8bre 2013-04-27]  Bayesian Networks With Examples in R [Scutari & Denis 2014-06-20]  Beginner\u2019s Guide to R - Zuur A. et al. - 2009  Beginning Data Science with R [Pathak 2014-12-09]  Beginning R - An Introduction to Statistical Programming - Pace L.  Beginning R - The Statistical Programming Language - Gardener M. - 2012  Behavioral Research Data Analysis with R - Li Y. et al. - 2012  Bioinformatics and Computational Biology Solutions using R and Bioconductor [Gentleman, Irizarry, Carey, Dudolt & Huber 2005-08-31]  Bioinformatics with R Cookbook [Sinha 2014-07-23]  Biostatistical Design and Analysis Using R - A Practical Guide - Logan M. - 2010  Biostatistical Design and Analysis using R_ A Practical Guide [Logan 2010-05-10]  Biostatistics with R An Introduction to Statistics through Biological Data [Shahbaba 2011-12-17]  Business Analytics for Managers - Jank W. - 2011  Chemometrics with R Multivariate Data Analysis in the Natural Sciences and Life Sciences [Wehrens 2011-01-31]  Clinical Trial Data Analysis - Using R - Din Chen, Karl E. Peace - 2010  Competing Risks and Multistate Models with R - Beyersmann J. et al. - 2012  Computational Actuarial Science with R [Charpentier 2014-08-26]  Computational Finance An Introductory Course with R [Arratia 2014-05-09]  Computational Statistics An Introduction to R [Sawitzki 2009-01-26]  Contingency Table Analysis Methods and Implementation using R [Kateri 2014-06-15]  Data Analysis and Graphics Using R - An Example-Based Approach, 3e - Maindonald J. et al. - 2010  Data Analysis using Regression and Multilevel_Hierarchical Models [Gelman & Hill 2006-12-18]  Data Manipulation with R (2 nd  ed.) [Abedin & Das 2015-03-31]  Data Manipulation with R - Spector P. - 2008  Data Mashups in R - Leipzig J. et al. - 2011  Data Mining with Rattle and R The Art of Excavating Data for Knowledge Discovery [Williams 2011-08-04]  Data Science in R A Case Studies Approach to Computational Reasoning and Problem Solving [Nolan & Lang 2015-03-18]  Data Wrangling with R - Boehmke B. - 2016  Doing Bayesian Data Analysis A Tutorial with R and BUGS [Kruschke 2010-11-10]  Dynamic Documents with R and knitr (2 nd  ed.) [Xie 2015-07-06]  Dynamic Linear Models with R - Petris G. et al. - 2009  EnvStats An R Package for Environmental Statistics [Millard 2013-10-28]  Exploratory Multivariate Analysis by Example - Using R - Husson F. et al. - 2011  Exploring Everyday Things with R and Ruby - Sheong Chang S. - 2012  Financial Risk Modelling and Portfolio Optimization with R [Pfaff 2013-01-22]  Forest Analytics with R - An Introduction - Robinson A. et al. - 2011  Foundational and Applied Statistics for Biologists using R [Aho 2013-12-17]  Functional and Phylogenetic Ecology in R [Swenson 2014-03-27]  Functional Data Analysis with R and MATLAB - Ramsay J. et al. - 2011  Getting Started with RStudio - Verzani J. - 2011  Graphical Models with R [H\u00f8jsgaard, Edwards & Lauritzen 2012-02-23]  Graphics for Statistics and Data Analysis with R [Keen 2010-04-26]  Graphing Data with R An Introduction - 2015  Guidebook to R Graphics using Microsoft Windows [Takezawa 2012-03-13]  Handbook of Statistical Analyses Using R, 2e - Everitt B. et al. - 2010  Hands-On Programming with R_ Write Your Own Functions and Simulations [Grolemund 2014-08-02]  Hidden Markov Models for Time Series_ An Introduction using R [Zucchini & MacDonald 2009-04-28]  Instant R Starter [Teutonico 2013-04-23]  Interactive and Dynamic Graphics for Data Analysis - With R and GGobi - Cook D. et al. - 2007  Introduction to Applied Multivariate Analysis with R - Everitt B. et al. -2011  Introduction to Data Analysis with R for Forensic Scientists [Curran 2010-07-30]  Introduction to Image Processing using R Learning by Examples [Frery & Perciano 2013-01-31]  Introduction to Probability and Statistics Using R - Kerns G. - 2010  Introduction to Probability Simulation and Gibbs Sampling with R - Suess E. et al. - 2010  Introduction to Probability with R [Baclawski 2008-01-24]  Introduction to R for Quantitative Finance [Dar\u00f3czi, Cs\u00f3ka, Tulassay, Puhle, Havran, V\u00e1radi, Berlinger, Michaletzky & Vidovics-Dancs 2013-11-22]  Introduction to Scientific Programming and Simulation using R (2 nd  ed.) [Jones, Maillardet & Robinson 2014-06-12]  Introduction to Statistics through Resampling Methods and R (2 nd  ed.) [Good 2013-02-11]  Introductory Statistics with R, 2e - Dalgaard P. - 2008  Introductory Time Series with R - Cowpertwait P. et al. - 2009  Latent Variable Modeling using R_ A Step-by-Step Guide [Beaujean 2014-05-08]  Latent Variable Modeling with R [Finch & French 2015-07-01]  Learning Data Mining with R [Makhabel 2014-12-22]  Learning Predictive Analytics with R- Eric Mayor - 2015  Learning R A Step-by-Step Function Guide to Data Analysis [Cotton 2013-09-26]  Linear Mixed-Effects Models using R_ A Step-by-Step Approach [Galecki & Burzykowski 2013-02-05]  Linear Models with R (2 nd  ed.) [Faraway 2014-07-01]_  Machine Learning with R Cookbook [Chiu 2015-03-31]  Machine Learning with R [Lantz 2013-10-25]  Making Your Case Using R for Program Evaluation [Auerbach & Zeitlin 2015-07-06]  Mastering Predictive Analytics with R [Forte 2015-06-30]  Mastering Scientific Computing with R [Gerrard & Johnson 2015-02-27]  Mathematical Statistics with Resampling and R - Chihara Laura M., Hesterberg Tim C. - 2011  MATLAB Graphics and Data Visualization Cookbook 2012  Maximum Likelihood Estimation and Inference_ With Examples in R, SAS and ADMB [Millar 2011-09-19]  Mixed Models_ Theory and Applications with R (2 nd  ed.) [Demidenko 2013-08-05]  Modern Actuarial Risk Theory - Using R - Kaas R. et al. - 2008  Modern Analysis of Customer Surveys With Applications using R [Kenett & Salini 2012-01-30]  Modern Approach to Regression with R - Sheather S. - 2009  Modern Industrial Statistics_ With Applications in R, MINITAB and JMP (2 nd  ed.) [Kenett, Zacks & Amberti 2014-01-28]  Modern Optimization with R [Cortez 2014-09-07]  Modern Regression Techniques Using R - Wright D. et al. - 2009  Modern Statistical Methods for Astronomy_ With R Applications [Feigelson & Babu 2012-08-27]  Morphometrics with R - Claude J. - 2008  Multilevel Modeling using R [Finch, Bolin & Kelley 2014-06-13]  Multiple Comparisons using R [Bretz, Hothorn & Westfall 2010-07-27]  Multistate Analysis of Life Histories with R [Willekens 2014-09-12]  Multivariate Generalized Linear Mixed Models using R [Berridge & Crouchley 2011-04-25]  Multivariate Methods of Representing Relations in R for Prioritization Purposes [Myers & Patil 2012-03-24]  Multivariate Nonparametric Regression and Visualization_ With R and Applications to Finance [Klemel\u00e4 2014-05-27]  Multivariate Time Series Analysis_ With R and Financial Applications [Tsay 2013-12-09]  Nonlinear Regression with R - Ritz C. et al. - 2008  Nonparametric Hypothesis Testing_ Rank and Permutation Methods with Applications in R [Bonnini, Corain, Marozzi & Salmaso 2014-09-15]  Nonparametric Statistical Methods using R [Kloke & McKean 2014-10-09]  Numerical Ecology with R - Borcard D. et al. - 2011  Practical Data Science with R [Zumel & Mount 2014-04-13]  Practical Graph Mining with R [Samatova, Hendrix, Jenkins, Padmanabhan & Chakraborty 2013-07-15]  Practical Guide to Ecological Modelling - Using R as a Simulation Platform - Soetaert K. et al. - 2009  Primer of Ecology with R - Henry M. et al. - 2009  Primer to Analysis of Genomic Data using R [Gondro 2015-05-20]  Probability With Applications and R [Dobrow 2013-11-04]  Programming Graphical User Interfaces in R - Lawrence M., Verzani J. - 2012  Quantitative Trading with R Understanding Mathematical and Computational Tools from a Quant\u2019s Perspective [Georgakopoulos 2015-01-06]  R Book - Crawley M. - 2007  R by Example - Jim Albert, Maria Rizzo - 2012  R Companion to Linear Statistical Models - Hay-Jahans C. - 2012  R Cookbook - Proven Recipes for Data Analisys, Statistics, and Graphics - Teetor P. - 2011  R Data Visualization Cookbook [Gohil 2015-01-29]  R for Business Analytics [Ohri 2012-09-14]  R for Cloud Computing An Approach for Data Scientists [Ohri 2014-11-15]  R for Data Science [Toomey 2014-12-19]  R for SAS and SPSS Users, 2e - Muenchen R - 2011  R for Stata Users - Muenchen R. et al. - 2011  R Graph Essentials [Lillis 2014-09-24]  R Graphs Cookbook (2 nd  ed.) [Abedin & Mittal 2014-10-20]  R Graphs Cookbook - Mittal H. - 2011  R Graphs Cookbook [Mittal 2011-01-14]  R High Performance Programming [Lim & Tjhi 2015-01-30]  R in a Nutshell_ A Desktop Quick Reference (2 nd  ed.) [Adler 2012-10-19]  R in Action - Kabacoff R. - 2011  R in Action_ Data Analysis and Graphics with R [Kabacoff 2011-08-27]  R Inferno - Burns P. - 2009  R Machine Learning Essentials [Usuelli 2014-11-25]  R Object-Oriented Programming [Black 2014-10-23]  R Programming for Bioinformatics - Gentelman R. - 2009  R Quick Syntax Reference [Tollefson 2014-04-25]  R Recipes A Problem-Solution Approach [Pace 2014-12-19]  R Statistical Application Development by Example Beginner\u2019s Guide [Tattar 2013-07-24]  R Through Excel - Heiberger R. et al. - 2009  Reproducible Research with R and RStudio [Gandrud 2013-07-15]  SAS and R Data Management, Statistical Analysis, and Graphics (2 nd  ed.) [Kleinman & Horton 2014-07-17]  Simulation and Inference for Stochastic Differential Equations_ With R Examples [Iacus 2008-05-05]  Six Sigma with R - Statistical Engineering for Process Improvement - Cano E. et al. - 2012  Social Media Mining with R [Danneman & Heimann 2014-03-24]  Software for Data Analysis - Programming with R - Chambers J. - 2008  Solving Differential Equations in R [Soetaert, Cash & Mazzia 2012-06-07]  Stated Preference Methods using R [Aizaki, Nakatani & Sato 2014-08-15]  Statistical Analysis of Financial Data in R (2 nd  ed.) [Carmona 2013-12-14]  Statistical Analysis of Network Data with R [Kolaczyk & Cs\u00e1rdi 2014-05-23]  Statistical Analysis of Questionnaires_ A Unified Approach Based on R and Stata [Bartolucci, Bacci & Gnaldi 2015-08-07]  Statistical Analysis with R - Beginner\u2019s Guide - Quick J. - 2010  Statistical Bioinformatics with R - Mathur S. - 2010  Statistical Data Analysis Explained - Applied Environmental Statistics with R - Reimann C. et al. - 2008  Statistical Methods for Environmental Epidemiology with R_ A Case Study in Air Pollution and Health [Peng & Dominici 2008-07-25]  Statistical Tools for Nonlinear Regression_ A Practical Guide with S-PLUS and R Examples (2 nd  ed.) [Huet, Bouvier, Poursat & Jolivet 2003-09-12]  Statistics An Introduction using R (2 nd  ed.) [Crawley 2014-11-24]  Statistics and Data Analysis for Financial Engineering With R Examples (2 nd  ed.) [Ruppert & Matteson 2015-04-22]  Statistics and Data Analysis for Microarrays using R and Bioconductor (2 nd  ed.) [Draghici 2016-06-15]  Statistics and Data with R - An Applied Approach Through Examples - Cohen Y. et al.- 2008  Statistics for Censored Environmental Data using Minitab and R (2 nd  ed.) [Helsel 2012-02-01]  Statistics for Linguistics with R A Practical Introduction (2 nd  ed.) [Gries 2013-03-15]  Statistics Using R with Biological Examples - Seefeld K. et al. - 2007  Text Analysis with R for Students of Literature [Jockers 2014-06-11]  The Essential R Reference [Gardener 2012-11-19]  The R Book (2 nd  ed.) [Crawley 2012-12-26]  The R Primer [Ekstr\u00f8m 2011-08-29]  The R Software_ Fundamentals of Programming and Statistical Analysis [de Micheaux, Drouilhet & Liquet 2014-02-28]  Time Series - Application to Finance with R and S-Plus, 2e - Chan N. - 2010  Time Series Analysis - With Applications in R, 2e - Cryer J. et al. - 2008  Time Series Analysis and Its Applications - With R Examples, 3e - Shumway R. et al. - 2010  Understanding Statistics using R [Schumacker & Tomek 2013-01-24]  Using R and RStudio for Data Management, Statistical Analysis, and Graphics (2 nd  ed.) [Horton & Kleinman 2015-03-17]  Using R for Data Management, Statistical Analysis, and Graphics - Horton N. et al. - 2011  Using R for Numerical Analysis in Science and Engineering [Bloomfield 2014-04-24]  Using R for Statistics [Stowell 2014-06-24]  Wavelet Methods in Statistics with R - Nason G. - 2008  XML and Web Technologies for Data Sciences with R [Nolan & Lang 2014-01-27]",
            "title": "R Compilation"
        },
        {
            "location": "/collection/#sql-nosql",
            "text": "High Performance MySQL, Second Edition, O\u2019Reilly, 2008  MySQL High Availability, O\u2019Reilly, 2010  PostgreSQL Administration Cookbook, Package, 2017",
            "title": "SQL, NoSQL"
        },
        {
            "location": "/collection/#storytelling-visualization",
            "text": "Good Charts, The HBR Guide, HBR Press, 2016  How to Use Excel Sparklines (notes)  Information Dashboard Design, The Effective Visual Communication of Data, O\u2019Reilly, 2006  L\u2019histoire du monde en infographie, Marabout, 2017  Storytelling with data, Wiley, 2015  Excel Sparklines (notes)  VIS BOOKS\u2026",
            "title": "Storytelling, Visualization"
        },
        {
            "location": "/collection/#tableau",
            "text": "desktop Tableau Getting Started 9.0  Tableau 10 Business Intelligence Cookbook, Packt, 2016  Which chart or graph is right, Tableau (article), 2012",
            "title": "Tableau"
        },
        {
            "location": "/Edition_CS/",
            "text": "Foreword\n\n\nCheat sheets.\n\n\n\n\nMarkdown\n\u00b6\n\n\n\n\nGitHub Markdown Syntax\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\nGit\n\u00b6\n\n\nPrincipal commands\n\u00b6\n\n\n\n\ngit init\n: creates a new Git repository.\n\n\ngit status\n: inspects the contents of the working directory and staging area.\n\n\ngit add <filename_1> <filename_2>\n: adds files from the working directory to the staging area.\n\n\ngit add .\n, \ngit add --all\n.\n\n\ngit diff <filename_1> <filename_2>\n: shows the difference between the working directory and the staging area.\n\n\ngit commit -m \"<comments>\"\n: permanently stores file changes from the staging area in the repository.\n\n\ngit log\n: shows a lis t of all previous commits.\n\n\ngit show HEAD\n: show the most recent commit.\n\n\ngit checkout HEAD <filename_1> <filename_2>\n: discards changes in the working directory.\n\n\ngit checkout <filename_1> <filename_2>\n: idem.\n\n\ngit reset HEAD <filename_1> <filename_2>\n: unstages file changes in the staging area, restore the file in your working directory.\n\n\ngit reset <SHA> (7 characters)\n: can be used to reset to a previous commit in your commit history.\n\n\ngit branch\n: lists all a Git project\u2019s branches.\n\n\ngit branch <branch_name>\n: creates a new branch.\n\n\ngit checkout <branch_name>\n: used to switch from one branch to another (the branch_name).\n\n\ngit merge <branch_name>\n: used to join file changes from one branch to another (your in branch A, you merge branch_name with A).\n\n\ngit branch -d <branch_name>\n: deletes the branch specified.\n\n\ngit clone <remote_location> <clone_name>\n: creates a local copy of a remote.\n\n\ngit remote -v\n: lists a Git project\u2019s remotes.\n\n\ngit fetch\n: fetches work from the remote into the local copy (no merging).\n\n\ngit merge origin/master\n: Merges origin/master into your local branch.\n\n\ngit push origin <branch_name>\n: pushes a local branch to the origin remote.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit Cheat Sheet\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit Cheat Sheet\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit Cheat Sheet\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\nLaTeX\n\u00b6\n\n\n\n\nLaTeX Font Packages\n. PDF only.\n\n\nLaTeX Cheat Sheet, 4p\n. PDF only.\n\n\nLaTeX Cheat Sheet, 2p\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegex\n\u00b6\n\n\n\n\nRegex\n. PDF.\n\n\n\n\n\n\n\n\n\n\nTableau\n\u00b6\n\n\n\n\nTableau Cheat Sheet\n. PDF only.\n\n\n\n\nVi/Vim\n\u00b6\n\n\n\n\nVi Editor\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVim Visual\n. PDF.\n\n\n\n\n\n\n\n\n\n\nWikipedia\n\u00b6\n\n\n\n\nWikipedia Syntax\n. PDF.",
            "title": "Edition Cheat Sheets"
        },
        {
            "location": "/Edition_CS/#git",
            "text": "",
            "title": "Git"
        },
        {
            "location": "/Edition_CS/#principal-commands",
            "text": "git init : creates a new Git repository.  git status : inspects the contents of the working directory and staging area.  git add <filename_1> <filename_2> : adds files from the working directory to the staging area.  git add . ,  git add --all .  git diff <filename_1> <filename_2> : shows the difference between the working directory and the staging area.  git commit -m \"<comments>\" : permanently stores file changes from the staging area in the repository.  git log : shows a lis t of all previous commits.  git show HEAD : show the most recent commit.  git checkout HEAD <filename_1> <filename_2> : discards changes in the working directory.  git checkout <filename_1> <filename_2> : idem.  git reset HEAD <filename_1> <filename_2> : unstages file changes in the staging area, restore the file in your working directory.  git reset <SHA> (7 characters) : can be used to reset to a previous commit in your commit history.  git branch : lists all a Git project\u2019s branches.  git branch <branch_name> : creates a new branch.  git checkout <branch_name> : used to switch from one branch to another (the branch_name).  git merge <branch_name> : used to join file changes from one branch to another (your in branch A, you merge branch_name with A).  git branch -d <branch_name> : deletes the branch specified.  git clone <remote_location> <clone_name> : creates a local copy of a remote.  git remote -v : lists a Git project\u2019s remotes.  git fetch : fetches work from the remote into the local copy (no merging).  git merge origin/master : Merges origin/master into your local branch.  git push origin <branch_name> : pushes a local branch to the origin remote.        Git Cheat Sheet . PDF.        Git Cheat Sheet . PDF.        Git Cheat Sheet . PDF.",
            "title": "Principal commands"
        },
        {
            "location": "/Edition_CS/#latex",
            "text": "LaTeX Font Packages . PDF only.  LaTeX Cheat Sheet, 4p . PDF only.  LaTeX Cheat Sheet, 2p . PDF.",
            "title": "LaTeX"
        },
        {
            "location": "/Edition_CS/#regex",
            "text": "Regex . PDF.",
            "title": "Regex"
        },
        {
            "location": "/Edition_CS/#tableau",
            "text": "Tableau Cheat Sheet . PDF only.",
            "title": "Tableau"
        },
        {
            "location": "/Edition_CS/#vivim",
            "text": "Vi Editor . PDF.         Vim Visual . PDF.",
            "title": "Vi/Vim"
        },
        {
            "location": "/Edition_CS/#wikipedia",
            "text": "Wikipedia Syntax . PDF.",
            "title": "Wikipedia"
        },
        {
            "location": "/latex_snippets/",
            "text": "Foreword\n\n\nSnippets and notes.\n\n\n\n\nCheat Sheet\n\u00b6\n\n\n\n\nLaTeX Font Packages\n. PDF only.\n\n\nLaTeX Cheat Sheet, 4p\n. PDF only.\n\n\nLaTeX Cheat Sheet, 2p\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResources\n\u00b6\n\n\n\n\nThe LATEX Project\n.\n\n\nTEX\n stack exchange.\n\n\nWrite memos\n.\n\n\nTeXcount web service\n.\n\n\nPense-bete pour natbib\n.\n\n\nTEXample\n.\n\n\nCTAN\n.\n\n\nLaTeX Wikibook\n.\n\n\nTeX\n.\n\n\n\n\nClasses\n\u00b6\n\n\nArticle\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n\\documentclass\n[12pt]\n{\narticle\n}\n\n\n\\title\n{}\n\n\n\\author\n{}\n\n\n\\date\n{}\n\n\n\\begin\n{\ndocument\n}\n\n\n\\maketitle\n\n\n\\begin\n{\nabstract\n}\n\n...\n\n\\end\n{\nabstract\n}\n\n\n\\section\n{}\n\n\n\\subsection\n{}\n\n\n\\subsubsection\n{}\n\n\n\\paragraph\n{}\n\n\n\\subparagraph\n{}\n\n\n\\end\n{\ndocument\n}\n\n\n\n\n\n\n\nReport\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\\documentclass\n[12pt]\n{\nreport\n}\n\n\n\\title\n{}\n\n\n\\author\n{}\n\n\n\\date\n{}\n\n\n\\begin\n{\ndocument\n}\n\n\n\\maketitle\n\n\n\\begin\n{\nabstract\n}\n\n...\n\n\\end\n{\nabstract\n}\n\n\n\\tableofcontents\n\n\n\\listoffigures\n\n\n\\listoftables\n\n\n\\chapter\n{}\n\n\n\\section\n{}\n\n\n\\subsection\n{}\n\n\n\\subsubsection\n{}\n\n\n\\paragraph\n{}\n\n\n\\subparagraph\n{}\n\n\n\\end\n{\ndocument\n}\n\n\n\n\n\n\n\nBook\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n\\documentclass\n[12pt]\n{\nbook\n}\n\n\n\\title\n{}\n\n\n\\author\n{}\n\n\n\\date\n{}\n\n\n\\begin\n{\ndocument\n}\n\n\n\\maketitle\n\n\n\\tableofcontents\n\n\n\\listoffigures\n\n\n\\listoftables\n\n\n\\part\n{}\n\n\n\\chapter\n{}\n\n\n\\section\n{}\n\n\n\\subsection\n{}\n\n\n\\subsubsection\n{}\n\n\n\\paragraph\n{}\n\n\n\\subparagraph\n{}\n\n\n\\end\n{\ndocument\n}\n\n\n\n\n\n\n\nFigure\n\u00b6\n\n\nStandard figure\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n%Preamble\n\n\n\\usepackage\n{\ngraphicx\n}\n\n\n\n%Document\n\n\n\\begin\n{\nfigure\n}\n[htbp]\n\n\\centering\n\n\n\\includegraphics\n[width=\\linewidth]\n{\nfigure.png\n}\n\n\n\\caption\n[short for lof]\n{\nlong figure caption\n}\n\n\n\\label\n{\nfig:default\n}\n\n\n\\end\n{\nfigure\n}\n\n\n\\ref\n{\nfig:default\n}\n\n\n\n\n\n\n\nSide-by-side figure (1x2)\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n%Preamble\n\n\n\\usepackage\n[lofdepth]\n{\nsubfig\n}\n\n\n\\usepackage\n{\ngraphicx\n}\n\n\n\n%Document\n\n\n\\begin\n{\nfigure\n}\n[htbp] \n% h:here; t:top; b:bottom; p:page; default:ht\n\n\n\\centering\n\n \n\\subfloat\n[short for lof][long subfigure1 caption]\n{\n\n   \n\\includegraphics\n[width=0.45\\linewidth]\n{\nfigure1.png\n}\n\n   \n\\label\n{\nsubfig:fig1\n}\n\n \n}\n\n \n\\subfloat\n[short for lof][long subfigure2 caption]\n{\n\n   \n\\includegraphics\n[width=0.45\\linewidth]\n{\nfigure2.png\n}\n\n   \n\\label\n{\nsubfig:fig2\n}\n\n\n}\n\n\n\\caption\n[short for lof]\n{\nlong figure caption\n}\n\n\n\\label\n{\nfig:fig1\n}\n\n\n\\end\n{\nfigure\n}\n\n\n\\ref\n{\nfig:fig1\n}\n and \n\\subref\n{\nsubfig:fig1\n}\n\n\n\n\n\n\n\nSide-by-side figure (2x2)\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n%Preamble\n\n\n\\usepackage\n[lofdepth,lotdepth]\n{\nsubfig\n}\n\n\n\\usepackage\n{\ngraphicx\n}\n\n\n\n%Document\n\n\n\\begin\n{\nfigure\n}\n[ht]\n\n\\centering\n\n \n\\subfloat\n[short for lof][long subfig1 caption]\n{\n\n   \n\\includegraphics\n[width=0.45\\linewidth]\n{\nfigure1.png\n}\n\n   \n\\label\n{\nsubfig:fig1\n}\n\n \n}\n\n \n\\subfloat\n[short for lof][long subfig2 caption]\n{\n\n   \n\\includegraphics\n[width=0.45\\linewidth]\n{\nfigure2.png\n}\n\n   \n\\label\n{\nsubfig:fig2\n}\n\n\n}\n\n\n \n\\subfloat\n[short for lof][long subfig3 caption]\n{\n\n   \n\\includegraphics\n[width=0.45\\linewidth]\n{\nfigure3.png\n}\n\n   \n\\label\n{\nsubfig:fig3\n}\n\n \n}\n\n \n\\subfloat\n[short for lof][long subfig4 caption]\n{\n\n   \n\\includegraphics\n[width=0.45\\linewidth]\n{\nfigure4.png\n}\n\n   \n\\label\n{\nsubfig:fig4\n}\n\n\n}\n\n\n\\caption\n[short for lof]\n{\nlong figure caption\n}\n\n\n\\label\n{\nfig:fig1\n}\n\n\n\\end\n{\nfigure\n}\n\n\n\\ref\n{\nfig:fig1\n}\n and \n\\subref\n{\nsubfig:fig1\n}\n\n\n\n\n\n\n\nSideways-figure\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n% Preamble\n\n\n\\usepackage\n{\nrotating\n}\n\n\n\n% Document\n\n\n\\begin\n{\nsidewaysfigure\n}\n\n\n\\centering\n\n\n\\includegraphics\n{\nfigure.png\n}\n\n\n\\caption\n{\nSideways figure.\n}\n\n\n\\label\n{\nfig:swfig\n}\n\n\n\\end\n{\nsidewaysfigure\n}\n\n\n\n\n\n\n\nWrap text around figure\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n%Preamble\n\n\n\\usepackage\n{\nwrapfig, graphicx\n}\n\n\n\n% Document\n\n\n\\begin\n{\nwrapfigure\n}{\nl\n}{\n0.5\n\\textwidth\n}\n \n% l for left, r for right\n\n\n\\centering\n\n\n\\includegraphics\n[width=0.45\\textwidth]\n{\ntest\n}\n\n\n\\caption\n{\nText wrapped around figure\n}\n\n\n\\label\n{\nfig:wrapfig\n}\n\n\n\\end\n{\nwrapfigure\n}\n\n\n\n\n\n\n\nTables\n\u00b6\n\n\nStandard table\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n\\begin\n{\ntable\n}\n[htbp] \n% h:here; t:top; b:bottom; p:page; default:ht\n\n    \n\\caption\n{\ndefault\n}\n\n    \n\\centering\n\n    \n\\begin\n{\ntabular\n}{\nlccr\n}\n\n        \n\\hline\n\n        - \n&\n - \n&\n - \n&\n - \n\\\\\n\n        \n\\hline\n\n        - \n&\n - \n&\n - \n&\n - \n\\\\\n\n        - \n&\n - \n&\n - \n&\n - \n\\\\\n\n        \n\\hline\n\n    \n\\end\n{\ntabular\n}\n\n    \n\\label\n{\ntab:def\n}\n\n\n\\end\n{\ntable\n}\n%\n\n\n\n\n\n\n\nSide-by-side table\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n%Preamble\n\n\n\\usepackage\n[lotdepth]\n{\nsubfig\n}\n\n\n\n%Document\n\n\n\\begin\n{\ntable\n}\n[htbp] \n% h:here; t:top; b:bottom; p:page; default:ht\n\n    \n\\centering\n\n    \n\\subfloat\n[short for lot subtable1][long subtable1 caption]\n{\n\n        \n\\begin\n{\ntabular\n}{\nlccr\n}\n\n            \n\\hline\n\n            - \n&\n - \n&\n - \n&\n - \n\\\\\n\n            \n\\hline\n\n            - \n&\n - \n&\n - \n&\n - \n\\\\\n\n            - \n&\n - \n&\n - \n&\n - \n\\\\\n\n            \n\\hline\n\n        \n\\end\n{\ntabular\n}\n\n        \n\\label\n{\nsubtab:tab1\n}\n\n    \n}\n\n    \n\\subfloat\n[short for lot subtable2][long subtable2 caption]\n{\n\n        \n\\begin\n{\ntabular\n}{\nlccr\n}\n\n            \n\\hline\n\n            - \n&\n - \n&\n - \n&\n - \n\\\\\n\n            \n\\hline\n\n            - \n&\n - \n&\n - \n&\n - \n\\\\\n\n            - \n&\n - \n&\n - \n&\n - \n\\\\\n\n            \n\\hline\n\n        \n\\end\n{\ntabular\n}\n\n        \n\\label\n{\nsubtab:tab2\n}\n\n    \n}\n\n    \n\\caption\n[short for lot]\n{\nlong table caption\n}\n\n    \n\\label\n{\ntab:tab1\n}\n\n\n\\end\n{\ntable\n}\n\n\n\\ref\n{\ntab:tab1\n}\n and \n\\subref\n{\nsubtab:tab1\n}\n\n\n\n\n\n\n\nSideways-table\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n% Preamble\n\n\n\\usepackage\n{\nrotating\n}\n\n\n\n% Document\n\n\n\\begin\n{\nsidewaystable\n}\n\n    \n\\caption\n{\nSideways table.\n}\n\n    \n\\centering\n\n    \n\\begin\n{\ntabular\n}{\nlccr\n}\n\n        \n\\hline\n\n        - \n&\n - \n&\n - \n&\n - \n\\\\\n\n        \n\\hline\n\n        - \n&\n - \n&\n - \n&\n - \n\\\\\n\n        - \n&\n - \n&\n - \n&\n - \n\\\\\n\n        \n\\hline\n\n    \n\\end\n{\ntabular\n}\n\n    \n\\label\n{\ntab:swtab\n}\n\n\n\\end\n{\nsidewaystable\n}\n\n\n\n\n\n\n\nMulti-page table (longtable)\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n%Preamble\n\n\n\\usepackage\n{\nlongtable\n}\n\n\n\n% Document\n\n\n\\begin\n{\ncenter\n}\n\n\n\\begin\n{\nlongtable\n}{\n|c|c|c|c|\n}\n\n\n\\caption\n{\nA simple longtable example\n}\n\\\\\n\n\n\\hline\n\n\n\\textbf\n{\nFirst entry\n}\n \n&\n \n\\textbf\n{\nSecond entry\n}\n \n&\n \n\\textbf\n{\nThird entry\n}\n \n&\n \n\\textbf\n{\nFourth entry\n}\n \n\\\\\n\n\n\\hline\n\n\n\\endfirsthead\n\n\n\\multicolumn\n{\n4\n}{\nc\n}\n%\n\n\n{\n\\tablename\\ \\thetable\\ \n-- \n\\textit\n{\nContinued from previous page\n}}\n \n\\\\\n\n\n\\hline\n\n\n\\textbf\n{\nFirst entry\n}\n \n&\n \n\\textbf\n{\nSecond entry\n}\n \n&\n \n\\textbf\n{\nThird entry\n}\n \n&\n \n\\textbf\n{\nFourth entry\n}\n \n\\\\\n\n\n\\hline\n\n\n\\endhead\n\n\n\\hline\n \n\\multicolumn\n{\n4\n}{\nr\n}{\n\\textit\n{\nContinued on next page\n}}\n \n\\\\\n\n\n\\endfoot\n\n\n\\hline\n \n\\multicolumn\n{\n4\n}{\nr\n}{\n\\textit\n{\nEnd of table\n}}\n \n\\\\\n\n\n\\endlastfoot\n\n1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n\n1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n\n1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n\n1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n\n1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n\n1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n\n1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n\n1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n\n1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n\n1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n\n1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n\n1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n 1 \n&\n 2 \n&\n 3 \n&\n 4 \n\\\\\n\n\n\\end\n{\nlongtable\n}\n\n\n\\end\n{\ncenter\n}\n\n\n\n\n\n\n\nList\n\u00b6\n\n\nBulleted list: itemize\n\u00b6\n\n\n1\n2\n3\n4\n5\n\\begin\n{\nitemize\n}\n\n\n\\item\n\n\n\\item\n\n\n\\item\n\n\n\\end\n{\nitemize\n}\n\n\n\n\n\n\n\nNumbered list: enumerate\n\u00b6\n\n\n1\n2\n3\n4\n5\n\\begin\n{\nenumerate\n}\n\n\n\\item\n\n\n\\item\n\n\n\\item\n\n\n\\end\n{\nenumerate\n}\n\n\n\n\n\n\n\nDescription\n\u00b6\n\n\n1\n2\n3\n4\n5\n\\begin\n{\ndescription\n}\n\n\n\\item\n[]\n\n\n\\item\n[]\n\n\n\\item\n[]\n\n\n\\end\n{\ndescription\n}\n\n\n\n\n\n\n\nInline enumeration\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n%Preamble\n\n\n\\usepackage\n{\nparalist\n}\n\n\n\n%Document\n\n\n\\begin\n{\ninparaenum\n}\n\n\n\\item\n\n\n\\item\n\n\n\\item\n\n\n\\end\n{\ninparaenum\n}\n\n\n\n\n\n\n\nBeamer Presentation\n\u00b6\n\n\nTable of Content\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\\documentclass\n{\nbeamer\n}\n\n\n\\usetheme\n{\nBerkeley\n}\n\n\n\\begin\n{\ndocument\n}\n\n\n\n\\begin\n{\nframe\n}\n\n    \n\\tableofcontents\n\n\n\\end\n{\nframe\n}\n\n\n\n\\section\n{\nFirst\n}\n\n\n\\begin\n{\nframe\n}{\nFirst section slide\n}\n\n    ...\n\n\\end\n{\nframe\n}\n\n\n\n\\section\n{\nSecond\n}\n\n\n%Contents with current section highlighted\n\n\n\\frame\n{\n\\tableofcontents\n[currentsection]\n}\n\n\n\\begin\n{\nframe\n}{\nSecond section slide\n}\n\n    ...\n\n\\end\n{\nframe\n}\n\n\n\n\\end\n{\ndocument\n}\n\n\n\n\n\n\n\nList\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n\\begin\n{\nframe\n}{\nEnumerate\n}\n\n    \n\\begin\n{\nenumerate\n}\n\n        \n\\item\n\n        \n\\item\n\n        \n\\item\n\n    \n\\end\n{\nenumerate\n}\n\n\n\\end\n{\nframe\n}\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\\begin\n{\nframe\n}{\nItemize\n}\n\n    \n\\begin\n{\nitemize\n}\n\n        \n\\item\n\n        \n\\item\n\n        \n\\item\n\n    \n\\end\n{\nitemize\n}\n\n\n\\end\n{\nframe\n}\n\n\n\n\n\n\n\nSide-by-side figure/table/list with columns\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n\\begin\n{\nframe\n}{\nSide-by-side figure/table/text with columns\n}\n\n    \n\\begin\n{\ncolumns\n}\n\n        \n\\begin\n{\ncolumn\n}\n[C]\n{\n.45\n\\textwidth\n}\n\n\n% the T argument instead of C will align the top of the two columns, instead of the centers\n\n            \n\\rule\n{\n\\textwidth\n}{\n0.75\n\\textwidth\n}\n\n        \n\\end\n{\ncolumn\n}\n\n        \n\\begin\n{\ncolumn\n}\n[C]\n{\n.45\n\\textwidth\n}\n\n            \n\\begin\n{\nenumerate\n}\n \n% numbered list\n\n                \n\\item\n\n                \n\\item\n\n                \n\\item\n\n            \n\\end\n{\nenumerate\n}\n\n        \n\\end\n{\ncolumn\n}\n\n    \n\\end\n{\ncolumns\n}\n\n\n\\end\n{\nframe\n}\n\n\n\n\n\n\n\nSide-by-side figure/table/list with minipage\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n\\begin\n{\nframe\n}{\nSide-by-side figure/table/text with minipage\n}\n\n    \n\\begin\n{\nminipage\n}{\n0.45\n\\textwidth\n}\n \n% figure\n\n        \n\\rule\n{\n\\textwidth\n}{\n0.75\n\\textwidth\n}\n\n    \n\\end\n{\nminipage\n}\n\n    \n\\qquad\n\n    \n\\begin\n{\nminipage\n}{\n0.45\n\\textwidth\n}\n\n        \n\\begin\n{\nenumerate\n}\n \n% numbered list\n\n            \n\\item\n\n            \n\\item\n\n            \n\\item\n\n        \n\\end\n{\nenumerate\n}\n\n    \n\\end\n{\nminipage\n}\n\n\n\\end\n{\nframe\n}\n\n\n\n\n\n\n\nInstallation\n\u00b6\n\n\n\n\nWindows.\n\n\nproTeXt\n (MiKTeX-based + the editor TeXnicCenter).\n\n\nMiKTeX\n.\n\n\nTeXnicCenter (from proTeXt).\n\n\nLEd\n.\n\n\nTeXstudio.\n\n\nTexmaker.\n\n\nLyX.\n\n\nJabRef.\n\n\n\n\n\n\nLinux.\n\n\nTeX Live\n.\n\n\nTeXstudio.\n\n\nTexmaker.\n\n\nKile.\n\n\nLatexila.\n\n\nLyX.",
            "title": "LaTeX Snippets"
        },
        {
            "location": "/latex_snippets/#resources",
            "text": "The LATEX Project .  TEX  stack exchange.  Write memos .  TeXcount web service .  Pense-bete pour natbib .  TEXample .  CTAN .  LaTeX Wikibook .  TeX .",
            "title": "Resources"
        },
        {
            "location": "/latex_snippets/#classes",
            "text": "",
            "title": "Classes"
        },
        {
            "location": "/latex_snippets/#article",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 \\documentclass [12pt] { article }  \\title {}  \\author {}  \\date {}  \\begin { document }  \\maketitle  \\begin { abstract } \n... \\end { abstract }  \\section {}  \\subsection {}  \\subsubsection {}  \\paragraph {}  \\subparagraph {}  \\end { document }",
            "title": "Article"
        },
        {
            "location": "/latex_snippets/#report",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 \\documentclass [12pt] { report }  \\title {}  \\author {}  \\date {}  \\begin { document }  \\maketitle  \\begin { abstract } \n... \\end { abstract }  \\tableofcontents  \\listoffigures  \\listoftables  \\chapter {}  \\section {}  \\subsection {}  \\subsubsection {}  \\paragraph {}  \\subparagraph {}  \\end { document }",
            "title": "Report"
        },
        {
            "location": "/latex_snippets/#book",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 \\documentclass [12pt] { book }  \\title {}  \\author {}  \\date {}  \\begin { document }  \\maketitle  \\tableofcontents  \\listoffigures  \\listoftables  \\part {}  \\chapter {}  \\section {}  \\subsection {}  \\subsubsection {}  \\paragraph {}  \\subparagraph {}  \\end { document }",
            "title": "Book"
        },
        {
            "location": "/latex_snippets/#figure",
            "text": "",
            "title": "Figure"
        },
        {
            "location": "/latex_snippets/#standard-figure",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 %Preamble  \\usepackage { graphicx }  %Document  \\begin { figure } [htbp] \\centering  \\includegraphics [width=\\linewidth] { figure.png }  \\caption [short for lof] { long figure caption }  \\label { fig:default }  \\end { figure }  \\ref { fig:default }",
            "title": "Standard figure"
        },
        {
            "location": "/latex_snippets/#side-by-side-figure-1x2",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 %Preamble  \\usepackage [lofdepth] { subfig }  \\usepackage { graphicx }  %Document  \\begin { figure } [htbp]  % h:here; t:top; b:bottom; p:page; default:ht  \\centering \n  \\subfloat [short for lof][long subfigure1 caption] { \n    \\includegraphics [width=0.45\\linewidth] { figure1.png } \n    \\label { subfig:fig1 } \n  } \n  \\subfloat [short for lof][long subfigure2 caption] { \n    \\includegraphics [width=0.45\\linewidth] { figure2.png } \n    \\label { subfig:fig2 }  }  \\caption [short for lof] { long figure caption }  \\label { fig:fig1 }  \\end { figure }  \\ref { fig:fig1 }  and  \\subref { subfig:fig1 }",
            "title": "Side-by-side figure (1x2)"
        },
        {
            "location": "/latex_snippets/#side-by-side-figure-2x2",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 %Preamble  \\usepackage [lofdepth,lotdepth] { subfig }  \\usepackage { graphicx }  %Document  \\begin { figure } [ht] \\centering \n  \\subfloat [short for lof][long subfig1 caption] { \n    \\includegraphics [width=0.45\\linewidth] { figure1.png } \n    \\label { subfig:fig1 } \n  } \n  \\subfloat [short for lof][long subfig2 caption] { \n    \\includegraphics [width=0.45\\linewidth] { figure2.png } \n    \\label { subfig:fig2 }  } \n\n  \\subfloat [short for lof][long subfig3 caption] { \n    \\includegraphics [width=0.45\\linewidth] { figure3.png } \n    \\label { subfig:fig3 } \n  } \n  \\subfloat [short for lof][long subfig4 caption] { \n    \\includegraphics [width=0.45\\linewidth] { figure4.png } \n    \\label { subfig:fig4 }  }  \\caption [short for lof] { long figure caption }  \\label { fig:fig1 }  \\end { figure }  \\ref { fig:fig1 }  and  \\subref { subfig:fig1 }    Sideways-figure   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 % Preamble  \\usepackage { rotating }  % Document  \\begin { sidewaysfigure }  \\centering  \\includegraphics { figure.png }  \\caption { Sideways figure. }  \\label { fig:swfig }  \\end { sidewaysfigure }    Wrap text around figure   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 %Preamble  \\usepackage { wrapfig, graphicx }  % Document  \\begin { wrapfigure }{ l }{ 0.5 \\textwidth }   % l for left, r for right  \\centering  \\includegraphics [width=0.45\\textwidth] { test }  \\caption { Text wrapped around figure }  \\label { fig:wrapfig }  \\end { wrapfigure }",
            "title": "Side-by-side figure (2x2)"
        },
        {
            "location": "/latex_snippets/#tables",
            "text": "",
            "title": "Tables"
        },
        {
            "location": "/latex_snippets/#standard-table",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 \\begin { table } [htbp]  % h:here; t:top; b:bottom; p:page; default:ht \n     \\caption { default } \n     \\centering \n     \\begin { tabular }{ lccr } \n         \\hline \n        -  &  -  &  -  &  -  \\\\ \n         \\hline \n        -  &  -  &  -  &  -  \\\\ \n        -  &  -  &  -  &  -  \\\\ \n         \\hline \n     \\end { tabular } \n     \\label { tab:def }  \\end { table } %",
            "title": "Standard table"
        },
        {
            "location": "/latex_snippets/#side-by-side-table",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32 %Preamble  \\usepackage [lotdepth] { subfig }  %Document  \\begin { table } [htbp]  % h:here; t:top; b:bottom; p:page; default:ht \n     \\centering \n     \\subfloat [short for lot subtable1][long subtable1 caption] { \n         \\begin { tabular }{ lccr } \n             \\hline \n            -  &  -  &  -  &  -  \\\\ \n             \\hline \n            -  &  -  &  -  &  -  \\\\ \n            -  &  -  &  -  &  -  \\\\ \n             \\hline \n         \\end { tabular } \n         \\label { subtab:tab1 } \n     } \n     \\subfloat [short for lot subtable2][long subtable2 caption] { \n         \\begin { tabular }{ lccr } \n             \\hline \n            -  &  -  &  -  &  -  \\\\ \n             \\hline \n            -  &  -  &  -  &  -  \\\\ \n            -  &  -  &  -  &  -  \\\\ \n             \\hline \n         \\end { tabular } \n         \\label { subtab:tab2 } \n     } \n     \\caption [short for lot] { long table caption } \n     \\label { tab:tab1 }  \\end { table }  \\ref { tab:tab1 }  and  \\subref { subtab:tab1 }",
            "title": "Side-by-side table"
        },
        {
            "location": "/latex_snippets/#sideways-table",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 % Preamble  \\usepackage { rotating }  % Document  \\begin { sidewaystable } \n     \\caption { Sideways table. } \n     \\centering \n     \\begin { tabular }{ lccr } \n         \\hline \n        -  &  -  &  -  &  -  \\\\ \n         \\hline \n        -  &  -  &  -  &  -  \\\\ \n        -  &  -  &  -  &  -  \\\\ \n         \\hline \n     \\end { tabular } \n     \\label { tab:swtab }  \\end { sidewaystable }",
            "title": "Sideways-table"
        },
        {
            "location": "/latex_snippets/#multi-page-table-longtable",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35 %Preamble  \\usepackage { longtable }  % Document  \\begin { center }  \\begin { longtable }{ |c|c|c|c| }  \\caption { A simple longtable example } \\\\  \\hline  \\textbf { First entry }   &   \\textbf { Second entry }   &   \\textbf { Third entry }   &   \\textbf { Fourth entry }   \\\\  \\hline  \\endfirsthead  \\multicolumn { 4 }{ c } %  { \\tablename\\ \\thetable\\  --  \\textit { Continued from previous page }}   \\\\  \\hline  \\textbf { First entry }   &   \\textbf { Second entry }   &   \\textbf { Third entry }   &   \\textbf { Fourth entry }   \\\\  \\hline  \\endhead  \\hline   \\multicolumn { 4 }{ r }{ \\textit { Continued on next page }}   \\\\  \\endfoot  \\hline   \\multicolumn { 4 }{ r }{ \\textit { End of table }}   \\\\  \\endlastfoot \n1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\ \n1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\ \n1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\ \n1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\ \n1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\ \n1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\ \n1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\ \n1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\ \n1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\ \n1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\ \n1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\ \n1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  1  &  2  &  3  &  4  \\\\  \\end { longtable }  \\end { center }",
            "title": "Multi-page table (longtable)"
        },
        {
            "location": "/latex_snippets/#list",
            "text": "",
            "title": "List"
        },
        {
            "location": "/latex_snippets/#bulleted-list-itemize",
            "text": "1\n2\n3\n4\n5 \\begin { itemize }  \\item  \\item  \\item  \\end { itemize }",
            "title": "Bulleted list: itemize"
        },
        {
            "location": "/latex_snippets/#numbered-list-enumerate",
            "text": "1\n2\n3\n4\n5 \\begin { enumerate }  \\item  \\item  \\item  \\end { enumerate }",
            "title": "Numbered list: enumerate"
        },
        {
            "location": "/latex_snippets/#description",
            "text": "1\n2\n3\n4\n5 \\begin { description }  \\item []  \\item []  \\item []  \\end { description }",
            "title": "Description"
        },
        {
            "location": "/latex_snippets/#inline-enumeration",
            "text": "1\n2\n3\n4\n5\n6\n7\n8\n9 %Preamble  \\usepackage { paralist }  %Document  \\begin { inparaenum }  \\item  \\item  \\item  \\end { inparaenum }",
            "title": "Inline enumeration"
        },
        {
            "location": "/latex_snippets/#beamer-presentation",
            "text": "",
            "title": "Beamer Presentation"
        },
        {
            "location": "/latex_snippets/#table-of-content",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 \\documentclass { beamer }  \\usetheme { Berkeley }  \\begin { document }  \\begin { frame } \n     \\tableofcontents  \\end { frame }  \\section { First }  \\begin { frame }{ First section slide } \n    ... \\end { frame }  \\section { Second }  %Contents with current section highlighted  \\frame { \\tableofcontents [currentsection] }  \\begin { frame }{ Second section slide } \n    ... \\end { frame }  \\end { document }",
            "title": "Table of Content"
        },
        {
            "location": "/latex_snippets/#list_1",
            "text": "1\n2\n3\n4\n5\n6\n7 \\begin { frame }{ Enumerate } \n     \\begin { enumerate } \n         \\item \n         \\item \n         \\item \n     \\end { enumerate }  \\end { frame }    1\n2\n3\n4\n5\n6\n7 \\begin { frame }{ Itemize } \n     \\begin { itemize } \n         \\item \n         \\item \n         \\item \n     \\end { itemize }  \\end { frame }",
            "title": "List"
        },
        {
            "location": "/latex_snippets/#side-by-side-figuretablelist-with-columns",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 \\begin { frame }{ Side-by-side figure/table/text with columns } \n     \\begin { columns } \n         \\begin { column } [C] { .45 \\textwidth }  % the T argument instead of C will align the top of the two columns, instead of the centers \n             \\rule { \\textwidth }{ 0.75 \\textwidth } \n         \\end { column } \n         \\begin { column } [C] { .45 \\textwidth } \n             \\begin { enumerate }   % numbered list \n                 \\item \n                 \\item \n                 \\item \n             \\end { enumerate } \n         \\end { column } \n     \\end { columns }  \\end { frame }    Side-by-side figure/table/list with minipage   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 \\begin { frame }{ Side-by-side figure/table/text with minipage } \n     \\begin { minipage }{ 0.45 \\textwidth }   % figure \n         \\rule { \\textwidth }{ 0.75 \\textwidth } \n     \\end { minipage } \n     \\qquad \n     \\begin { minipage }{ 0.45 \\textwidth } \n         \\begin { enumerate }   % numbered list \n             \\item \n             \\item \n             \\item \n         \\end { enumerate } \n     \\end { minipage }  \\end { frame }",
            "title": "Side-by-side figure/table/list with columns"
        },
        {
            "location": "/latex_snippets/#installation",
            "text": "Windows.  proTeXt  (MiKTeX-based + the editor TeXnicCenter).  MiKTeX .  TeXnicCenter (from proTeXt).  LEd .  TeXstudio.  Texmaker.  LyX.  JabRef.    Linux.  TeX Live .  TeXstudio.  Texmaker.  Kile.  Latexila.  LyX.",
            "title": "Installation"
        },
        {
            "location": "/Digital Humanities/",
            "text": "Foreword\n\n\nNotes. From:\n\n\n\n\nThe Historian\u2019s Macroscope\n\n\nBook exploring big historical data.\n\n\n\n\n\n\nDigital Humanities\n\n\nStandford Humanities Center.\n\n\n\n\n\n\n\n\n\n\nBlogs\n\u00b6\n\n\n\n\nHistoryonics\n.\n\n\nDay of Archaeology\n.\n\n\nThe Day of Archaeology is an event where archaeologists write about their activities on a group blog.\n\n\nCurrently there are over 1000 posts on the blog; a lot to read in one sitting. Rather than closely read each post, we can do a distant reading to get some insights into the corpus. Distant reading refers to efforts to understand texts through quantitative analysis and visualisation. \n\n\n\n\n\n\nGlobal Perspective on Digital History\n.\n\n\nThe Programming Historian\n\n\nLessons, projects, research, blog. \n\n\nThe Programming Historian offers novice-friendly, peer-reviewed tutorials that help humanists learn a wide range of digital tools, techniques, and workflows to facilitate their research.\n\n\n\n\n\n\nITHAKA S+R\n.\n\n\nProvides research and strategic guidance to help the academic community navigate economic and technological change.\n\n\n\n\n\n\n\n\nCases\n\u00b6\n\n\n\n\nBig Data + Old History\n.\n\n\nVideo; dig into documents without reading them.\n\n\n\n\n\n\nOld Bailey\n.\n\n\nThe proceedings of the Old Bailey, 1674-1913. A fully searchable edition of the largest body of texts detailing the lives of non-elite people ever published, containing 197,745 criminal trials held at London\u2019s central criminal court.\n\n\nWhite paper; Data Mining with Criminal Intent\n.\n\n\n\n\n\n\n\n\nCourses\n\u00b6\n\n\n\n\nPython Programming for the Humanities\n.\n\n\nThe Programming Historian Lessons\n.\n\n\n\n\nData and Models\n\u00b6\n\n\n\n\nOpen Data\n.\n\n\n\n\nDatasets and Projects\n\u00b6\n\n\n\n\nLe Programme de recherche en d\u00e9mographie historique\n.\n\n\nDonn\u00e9es g\u00e9n\u00e9alogiques de la Nouvelle France au Qu\u00e9bec contemporain.\n\n\n\n\n\n\nORBIS\n.\n\n\nThe Stanford Geospatial Network Model of the Roman World.\n\n\nCould become a boardgame (or an app)\n.\n\n\n\n\n\n\nPelagios\n.\n\n\nPelagios Commons provides online resources and a community forum for using open data methods to link and explore historical places.\n\n\nMaps\n.\n\n\nBlog\n.\n\n\n\n\n\n\nLOTR Project\n.\n\n\nLotrProject is dedicated to bringing J.R.R. Tolkien\u2019s works to life through various creative web projects (genealogy, interactive maps, timelines, and statistics).\n\n\n\n\n\n\nComputational Folkloristics\n.\n\n\nMapping folktales and linking themes.\n\n\nMagazine\n.\n\n\nArticle; Big Folklore: A Special Issue on Computational Folkloristics\n.\n\n\n\n\n\n\nDataverse\n.\n\n\nDataverse collects data on social-scientific, health, and environmental data for the world as a whole for the past four or five centuries. \n\n\n\n\n\n\nCLIWOC\n.\n\n\nClimatological Database for the World\u2019s Oceans 1750-1850.\n\n\nWikipedia\n.\n\n\nRoyal Netherlands Meteorological Institute\n.\n\n\n\n\n\n\n\n\nNetworks\n\u00b6\n\n\n\n\nAccess Linked Open Data\n.\n\n\nRDF databases, graph databases, and how researchers can access these data though the query language called SPARQL. \n\n\nRDF represents information in a series of three-part \u2018statements\u2019 that comprise a subject, predicate, and an object.  \n\n\n\n\n\n\nNetwork visualizations\n.\n\n\nData extraction and network visualization of historical sources.\n\n\n\n\n\n\nUCINETS\n.\n\n\nUCINET 6 is a software package for the analysis of social network data.\n\n\n\n\n\n\nPajek\n.\n\n\nAnalysis and visualization of large networks. \n\n\nAnalyse des r\u00e9seaux : une introduction \u00e0 Pajek\n.\n\n\n\n\n\n\nNetwork Workbench\n.\n\n\nA Large-Scale Network Analysis, Modeling and Visualization Toolkit for Biomedical, Social Science and Physics Research.   \n\n\n\n\n\n\nSci2\n.\n\n\nThe Science of Science (Sci2) Tool is a modular toolset specifically designed for the study of science.\n\n\nIt supports the temporal, geospatial, topical, and network analysis and visualization of scholarly datasets at the micro (individual), meso (local), and macro (global) levels.   \n\n\n\n\n\n\nNodeXL\n.\n\n\nNetwork overview, discovery and exploration for Excel.\n\n\n\n\n\n\nGephi\n.\n\n\nVisualization and exploration software for all kinds of graphs and networks.\n\n\nViewer\n.\n\n\nExporter\n.\n\n\nWeb Export\n.\n\n\n\n\n\n\n\n\nTools, Data Mining and Analyzing\n\u00b6\n\n\n\n\nGoogle Ngram\n.\n\n\nMine Google Books.\n\n\nOnline and an API is available.\n\n\n\n\n\n\nR and Python\n.\n\n\nPackages for text mining, text analysis, NLP (natural language processing).\n\n\nTopic Modeling Tool\n.\n\n\n\n\n\n\nUnix\n.\n\n\nMining text and qualitative data with Unix.\n\n\nDownload, trim, dig into dir and subdir, find patterns, count files, lines, words, save into a new file or a subdir, count instances and other stats like concordances, use pipelines, create lists with  unix commands, etc.\n\n\n\n\n\n\nOpenRefine\n.\n\n\nClean data, remove duplicate records, separate multiple values contained in the same field, analyse the distribution of values throughout a data set, group together different representations of the same reality, etc.\n\n\n\n\n\n\nAntConc\n.\n\n\nCorpus analysis.\n\n\nCreate/download a corpus of texts, conduct a keyword-in-context search, identify patterns surrounding a particular word, use more specific search queries, look at statistically significant differences between corpora, make multi-modal comparisons using corpus lingiustic methods. \n\n\nMany other tools on \nLaurence Anthony\u2019s Website\n:\n\n\nAntConc : a freeware corpus analysis toolkit for concordancing and text analysis.\n\n\nAntPConc : a freeware \nparallel\n corpus analysis toolkit for concordancing and text analysis using UTF-8 encoded text files.\n\n\nAntWordProfiler : a freeware tool for profiling the vocabulary level and complexity of texts.\n\n\nAntFileConverter : a freeware tool to convert PDF and Word (DOCX) files into plain text for use in corpus tools like AntConc.\n\n\nAntMover : a freeware text structure (moves, outline, flow) analysis program.\n\n\nAntCLAWSGUI : a front-end interface to the CLAWS tagger developed at Lancaster University.\n\n\nEncodeAnt : a freeware tool for detecting and converting character encodings.\n\n\nFireAnt :  a freeware social media and data analysis toolkit (developed in collaboration with Claire Hardaker of Lancaster University).\n\n\nProtAnt : a freeware prototypical text analysis tool (developed in collaboration with Paul Baker of Lancaster University).\n\n\nSarAnt : a freeware batch search and replace tool.\n\n\nSegmentAnt : a freeware Japanese and Chinese segmenter (segmentation/tokenizing tool).\n\n\nTagAnt : a freeware Part-Of-Speech (POS) tagger built on TreeTagger (developed by Helmut Schmid).\n\n\nVariAnt : A freeware spelling VariAnt analysis program.\n\n\n\n\n\n\n\n\n\n\nBeautiful Soup\n.\n\n\nPython module.\n\n\nText parsing.\n\n\n\n\n\n\nTaPOR\n.\n\n\nGateway to the tools for sophisticated text analysis and retrieval.\n\n\n\n\n\n\nPaper Machines\n.\n\n\nVisualize thousands of texts.\n\n\nPlugin for the Zotero bibliographic management software.\n\n\n\n\n\n\nVoyant\n.\n\n\nOnline tool for concordances, wordles, stats, graphics. \n\n\nCan be set on a local server.\n\n\nDocumentation\n\n\n\n\n\n\nOverview\n.\n\n\nSearch, visualize, and review your documents. \n\n\nUp to hundreds of thousands of them, in any format.\n\n\n\n\n\n\nMALLET\n.\n\n\nNLP toolkit and machine learning.\n\n\nTopic Analysis, keywords, bags of words. \n\n\nTopic modeling tool takes a single text (or corpus) and looks for patterns in the use of words; it is an attempt to inject semantic meaning into vocabulary.\n\n\nTopic models represent a family of computer programs that extract topics from texts. A topic to the computer is a list of words that occur in statistically meaningful ways. A text can be an email, a blog post, a book chapter, a journal article, a diary entry \u2013 that is, any kind of unstructured text.\n\n\nDocumentation\n.\n\n\nArticle; Getting Started with Topic Modeling and MALLET\n.\n\n\n\n\n\n\nStanford Topic Modeling Toolbox\n.\n\n\nThe Stanford Topic Modeling Toolbox (TMT) brings topic modeling tools to social scientists and others who wish to perform analysis on datasets that have a substantial textual component:\n\n\nImport and manipulate text from cells in Excel and other spreadsheets.\n\n\nTrain topic models (LDA, Labeled LDA, and PLDA new) to create summaries of the text.\n\n\nSelect parameters (such as the number of topics) via a data-driven process.\n\n\nGenerate rich Excel-compatible outputs for tracking word usage across topics, time, and other groupings of data.\n\n\n\n\n\n\n\n\n\n\nRegexr\n.\n\n\nOnline tool for processing regular expression or regex.\n\n\n\n\n\n\nRegex can also be done\n.\n\n\nTutorial on how to process regular expressions in Notepad++, on TextWrangler or other text/code editors (Vim, Emacs, etc.).\n\n\n\n\n\n\n\n\nTools, Mining the Web\n\u00b6\n\n\n\n\nRetrieving Web Archive:\n\n\nInternet Archives, Wayback Machine\n.\n\n\nTime Travel\n.\n\n\n\n\n\n\nMining the Internet Archive Collection\n.\n\n\ninternetarchive\n Python package.\n\n\n\n\n\n\nWget\n.\n\n\nAutomated downloading with Wget. \n\n\nPull data from the web.\n\n\n\n\n\n\nQuery\n.\n\n\nDownloading many records using Python.\n\n\nHow to check if a directory exists and create it if necessary\n.\n\n\n\n\n\n\nFigshare\n.\n\n\nWeb scraping. \n\n\n\n\n\n\nOutwit\n.\n\n\nFind, grab and organize all kinds of data and media from online sources.  \n\n\n\n\n\n\nXPath\n.\n\n\nWeb scraping, screen scraping, data parsing and other related things.\n\n\nAbout XPath\n.\n\n\n\n\n\n\nImportio\n.\n\n\nExtract web data the easy way.    \n\n\n\n\n\n\nTabula\n.\n\n\nExtract data from PDF.    \n\n\n\n\n\n\n\n\nTools, Referencing\n\u00b6\n\n\n\n\nZotero\n.\n\n\nStandalone and add-in to Mozilla Firefox. \n\n\nAdd-in to text processors (including LaTeX and Markdown editors). \n\n\nDig into journals and books primary sources.\n\n\nCollect, organize, cite, and share your research sources.\n\n\n\n\n\n\nJSOR\n.\n\n\nJournals, primary sources, and now BOOKS.\n\n\n\n\n\n\n\n\nTools, Scanning\n\u00b6\n\n\n\n\nOCR scanner\n\n\nDigitization material documents (from books, letters to maps).\n\n\n\n\n\n\nThe MNIST Database\n.\n\n\nThe MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples (used in machine learning to process NLP). \n\n\nIt is a subset of a larger set available from NIST. \n\n\nThe digits have been size-normalized and centered in a fixed-size image. \n\n\n\n\n\n\n\n\nTools, Visualization\n\u00b6\n\n\n\n\nSparklines are mini-graphics. Add-in to Excel.\n\n\nGIS: the best open-source database for mapping and GIS is PostgreSQL.\n\n\nGoogle Maps and Google Earth\n.\n\n\nGoogle My Maps and Google Earth provide an easy way to start creating digital maps. With a Google Account you can create and edit personal maps by clicking on My Places.\n\n\nIn My Maps you can choose between several different base maps (including the standard satellite, terrain, or standard maps) and add points, lines and polygons. It is also possible to import data from a spreadsheet, if you have columns with geographical information (i.e. longitudes and latitudes or place names). This automates a formerly complex task known as geocoding. Not only is this one of the easiest ways to begin plotting your historical data on a map, but it also has the power of Google\u2019s search engine. As you read about unfamiliar places in historical documents, journal articles or books, you can search for them using Google Maps. It is then possible to mark numerous locations and explore how they relate to each other geographically. Your personal maps are saved by Google (in their cloud), meaning you can access them from any computer with an internet connection. You can keep them private or embed them in your website or blog. Finally, you can export your points, lines, and polygons as KML files and open them in Google Earth or Quantum GIS.\n\n\n\n\n\n\nQGIS\n\n\nAdding layers to a map.\n\n\nInstalling QGIS 2.0 and Adding Layers\n.\n\n\nCreating New Vector Layers\n.\n\n\nGeoreferencing\n.\n\n\nA Free and Open Source Geographic Information System \n.\n\n\n\n\n\n\nOmeka\n.\n\n\nDisplays items, collections, like in museums, libraries, archives with narratives.\n\n\n\n\n\n\nAugmented Reality\n.\n\n\nOverlaying of digital content (images, video, text, sound, etc.) onto physical objects or locations, and it is typically experienced by looking through the camera lens of an electronic device such as a smartphone, tablet, or optical head-mounted display (e.g. Microsoft Hololens).  \n\n\nAlthough AR is a cutting-edge, complex technology, there are a number of user-friendly platforms that allow people with no previous coding experience to create compelling augmented reality experiences.\n\n\n\n\n\n\nD3.js\n.\n\n\nJavaScript visualization for the web; server-side.\n\n\nD3 Examples\n.\n\n\nA freelancer\u2019s gallery\n.\n\n\n\n\n\n\nBokeh\n.\n\n\nGallery\n.\n\n\n\n\n\n\nTop 7 Free Infographics Tools & Online Makers in 2016\n.\n\n\nSHANTI INTERACTIVE\n.\n\n\nSuite of tools that make it easy to create highly interactive web-based visualizations, videos, and maps. They are freely available from the University of Virginia\u2019s Sciences, Humanities & Arts Network of Technological Initiatives (SHANTI).\n\n\nQmedia provides new ways to use video for instructional and scholarly purposes. The viewer interacts with the whole screen and sees a wide array of web-based resources and offers an immersive experience that adds context.\n\n\nSHIVA takes a new approach that makes it easy to add graphical and data-driven visualizations to websites. Elements such as data, charts, network graphs, maps, image montages, and timelines are easily created.\n\n\nMapScholar is an online platform for geospatial visualization funded by the NEH. It enables humanities and social science scholars to create digital \u201catlases\u201d featuring high-resolution images of historic maps.\n\n\nVisualEyes is web-based authoring tool for historic visualization funded by the NEH to weave images, maps, charts, video and data into highly interactive and compelling dynamic visualizations.\n\n\nVisualEyes5 is a HTML5 version of the VisualEyes authoring tool for historic visualization to weave images, maps, charts, video and data into highly interactive and compelling dynamic visualizations.",
            "title": "Digital Humanities"
        },
        {
            "location": "/Digital Humanities/#cases",
            "text": "Big Data + Old History .  Video; dig into documents without reading them.    Old Bailey .  The proceedings of the Old Bailey, 1674-1913. A fully searchable edition of the largest body of texts detailing the lives of non-elite people ever published, containing 197,745 criminal trials held at London\u2019s central criminal court.  White paper; Data Mining with Criminal Intent .",
            "title": "Cases"
        },
        {
            "location": "/Digital Humanities/#courses",
            "text": "Python Programming for the Humanities .  The Programming Historian Lessons .",
            "title": "Courses"
        },
        {
            "location": "/Digital Humanities/#data-and-models",
            "text": "Open Data .",
            "title": "Data and Models"
        },
        {
            "location": "/Digital Humanities/#datasets-and-projects",
            "text": "Le Programme de recherche en d\u00e9mographie historique .  Donn\u00e9es g\u00e9n\u00e9alogiques de la Nouvelle France au Qu\u00e9bec contemporain.    ORBIS .  The Stanford Geospatial Network Model of the Roman World.  Could become a boardgame (or an app) .    Pelagios .  Pelagios Commons provides online resources and a community forum for using open data methods to link and explore historical places.  Maps .  Blog .    LOTR Project .  LotrProject is dedicated to bringing J.R.R. Tolkien\u2019s works to life through various creative web projects (genealogy, interactive maps, timelines, and statistics).    Computational Folkloristics .  Mapping folktales and linking themes.  Magazine .  Article; Big Folklore: A Special Issue on Computational Folkloristics .    Dataverse .  Dataverse collects data on social-scientific, health, and environmental data for the world as a whole for the past four or five centuries.     CLIWOC .  Climatological Database for the World\u2019s Oceans 1750-1850.  Wikipedia .  Royal Netherlands Meteorological Institute .",
            "title": "Datasets and Projects"
        },
        {
            "location": "/Digital Humanities/#networks",
            "text": "Access Linked Open Data .  RDF databases, graph databases, and how researchers can access these data though the query language called SPARQL.   RDF represents information in a series of three-part \u2018statements\u2019 that comprise a subject, predicate, and an object.      Network visualizations .  Data extraction and network visualization of historical sources.    UCINETS .  UCINET 6 is a software package for the analysis of social network data.    Pajek .  Analysis and visualization of large networks.   Analyse des r\u00e9seaux : une introduction \u00e0 Pajek .    Network Workbench .  A Large-Scale Network Analysis, Modeling and Visualization Toolkit for Biomedical, Social Science and Physics Research.       Sci2 .  The Science of Science (Sci2) Tool is a modular toolset specifically designed for the study of science.  It supports the temporal, geospatial, topical, and network analysis and visualization of scholarly datasets at the micro (individual), meso (local), and macro (global) levels.       NodeXL .  Network overview, discovery and exploration for Excel.    Gephi .  Visualization and exploration software for all kinds of graphs and networks.  Viewer .  Exporter .  Web Export .",
            "title": "Networks"
        },
        {
            "location": "/Digital Humanities/#tools-data-mining-and-analyzing",
            "text": "Google Ngram .  Mine Google Books.  Online and an API is available.    R and Python .  Packages for text mining, text analysis, NLP (natural language processing).  Topic Modeling Tool .    Unix .  Mining text and qualitative data with Unix.  Download, trim, dig into dir and subdir, find patterns, count files, lines, words, save into a new file or a subdir, count instances and other stats like concordances, use pipelines, create lists with  unix commands, etc.    OpenRefine .  Clean data, remove duplicate records, separate multiple values contained in the same field, analyse the distribution of values throughout a data set, group together different representations of the same reality, etc.    AntConc .  Corpus analysis.  Create/download a corpus of texts, conduct a keyword-in-context search, identify patterns surrounding a particular word, use more specific search queries, look at statistically significant differences between corpora, make multi-modal comparisons using corpus lingiustic methods.   Many other tools on  Laurence Anthony\u2019s Website :  AntConc : a freeware corpus analysis toolkit for concordancing and text analysis.  AntPConc : a freeware  parallel  corpus analysis toolkit for concordancing and text analysis using UTF-8 encoded text files.  AntWordProfiler : a freeware tool for profiling the vocabulary level and complexity of texts.  AntFileConverter : a freeware tool to convert PDF and Word (DOCX) files into plain text for use in corpus tools like AntConc.  AntMover : a freeware text structure (moves, outline, flow) analysis program.  AntCLAWSGUI : a front-end interface to the CLAWS tagger developed at Lancaster University.  EncodeAnt : a freeware tool for detecting and converting character encodings.  FireAnt :  a freeware social media and data analysis toolkit (developed in collaboration with Claire Hardaker of Lancaster University).  ProtAnt : a freeware prototypical text analysis tool (developed in collaboration with Paul Baker of Lancaster University).  SarAnt : a freeware batch search and replace tool.  SegmentAnt : a freeware Japanese and Chinese segmenter (segmentation/tokenizing tool).  TagAnt : a freeware Part-Of-Speech (POS) tagger built on TreeTagger (developed by Helmut Schmid).  VariAnt : A freeware spelling VariAnt analysis program.      Beautiful Soup .  Python module.  Text parsing.    TaPOR .  Gateway to the tools for sophisticated text analysis and retrieval.    Paper Machines .  Visualize thousands of texts.  Plugin for the Zotero bibliographic management software.    Voyant .  Online tool for concordances, wordles, stats, graphics.   Can be set on a local server.  Documentation    Overview .  Search, visualize, and review your documents.   Up to hundreds of thousands of them, in any format.    MALLET .  NLP toolkit and machine learning.  Topic Analysis, keywords, bags of words.   Topic modeling tool takes a single text (or corpus) and looks for patterns in the use of words; it is an attempt to inject semantic meaning into vocabulary.  Topic models represent a family of computer programs that extract topics from texts. A topic to the computer is a list of words that occur in statistically meaningful ways. A text can be an email, a blog post, a book chapter, a journal article, a diary entry \u2013 that is, any kind of unstructured text.  Documentation .  Article; Getting Started with Topic Modeling and MALLET .    Stanford Topic Modeling Toolbox .  The Stanford Topic Modeling Toolbox (TMT) brings topic modeling tools to social scientists and others who wish to perform analysis on datasets that have a substantial textual component:  Import and manipulate text from cells in Excel and other spreadsheets.  Train topic models (LDA, Labeled LDA, and PLDA new) to create summaries of the text.  Select parameters (such as the number of topics) via a data-driven process.  Generate rich Excel-compatible outputs for tracking word usage across topics, time, and other groupings of data.      Regexr .  Online tool for processing regular expression or regex.    Regex can also be done .  Tutorial on how to process regular expressions in Notepad++, on TextWrangler or other text/code editors (Vim, Emacs, etc.).",
            "title": "Tools, Data Mining and Analyzing"
        },
        {
            "location": "/Digital Humanities/#tools-mining-the-web",
            "text": "Retrieving Web Archive:  Internet Archives, Wayback Machine .  Time Travel .    Mining the Internet Archive Collection .  internetarchive  Python package.    Wget .  Automated downloading with Wget.   Pull data from the web.    Query .  Downloading many records using Python.  How to check if a directory exists and create it if necessary .    Figshare .  Web scraping.     Outwit .  Find, grab and organize all kinds of data and media from online sources.      XPath .  Web scraping, screen scraping, data parsing and other related things.  About XPath .    Importio .  Extract web data the easy way.        Tabula .  Extract data from PDF.",
            "title": "Tools, Mining the Web"
        },
        {
            "location": "/Digital Humanities/#tools-referencing",
            "text": "Zotero .  Standalone and add-in to Mozilla Firefox.   Add-in to text processors (including LaTeX and Markdown editors).   Dig into journals and books primary sources.  Collect, organize, cite, and share your research sources.    JSOR .  Journals, primary sources, and now BOOKS.",
            "title": "Tools, Referencing"
        },
        {
            "location": "/Digital Humanities/#tools-scanning",
            "text": "OCR scanner  Digitization material documents (from books, letters to maps).    The MNIST Database .  The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples (used in machine learning to process NLP).   It is a subset of a larger set available from NIST.   The digits have been size-normalized and centered in a fixed-size image.",
            "title": "Tools, Scanning"
        },
        {
            "location": "/Digital Humanities/#tools-visualization",
            "text": "Sparklines are mini-graphics. Add-in to Excel.  GIS: the best open-source database for mapping and GIS is PostgreSQL.  Google Maps and Google Earth .  Google My Maps and Google Earth provide an easy way to start creating digital maps. With a Google Account you can create and edit personal maps by clicking on My Places.  In My Maps you can choose between several different base maps (including the standard satellite, terrain, or standard maps) and add points, lines and polygons. It is also possible to import data from a spreadsheet, if you have columns with geographical information (i.e. longitudes and latitudes or place names). This automates a formerly complex task known as geocoding. Not only is this one of the easiest ways to begin plotting your historical data on a map, but it also has the power of Google\u2019s search engine. As you read about unfamiliar places in historical documents, journal articles or books, you can search for them using Google Maps. It is then possible to mark numerous locations and explore how they relate to each other geographically. Your personal maps are saved by Google (in their cloud), meaning you can access them from any computer with an internet connection. You can keep them private or embed them in your website or blog. Finally, you can export your points, lines, and polygons as KML files and open them in Google Earth or Quantum GIS.    QGIS  Adding layers to a map.  Installing QGIS 2.0 and Adding Layers .  Creating New Vector Layers .  Georeferencing .  A Free and Open Source Geographic Information System  .    Omeka .  Displays items, collections, like in museums, libraries, archives with narratives.    Augmented Reality .  Overlaying of digital content (images, video, text, sound, etc.) onto physical objects or locations, and it is typically experienced by looking through the camera lens of an electronic device such as a smartphone, tablet, or optical head-mounted display (e.g. Microsoft Hololens).    Although AR is a cutting-edge, complex technology, there are a number of user-friendly platforms that allow people with no previous coding experience to create compelling augmented reality experiences.    D3.js .  JavaScript visualization for the web; server-side.  D3 Examples .  A freelancer\u2019s gallery .    Bokeh .  Gallery .    Top 7 Free Infographics Tools & Online Makers in 2016 .  SHANTI INTERACTIVE .  Suite of tools that make it easy to create highly interactive web-based visualizations, videos, and maps. They are freely available from the University of Virginia\u2019s Sciences, Humanities & Arts Network of Technological Initiatives (SHANTI).  Qmedia provides new ways to use video for instructional and scholarly purposes. The viewer interacts with the whole screen and sees a wide array of web-based resources and offers an immersive experience that adds context.  SHIVA takes a new approach that makes it easy to add graphical and data-driven visualizations to websites. Elements such as data, charts, network graphs, maps, image montages, and timelines are easily created.  MapScholar is an online platform for geospatial visualization funded by the NEH. It enables humanities and social science scholars to create digital \u201catlases\u201d featuring high-resolution images of historic maps.  VisualEyes is web-based authoring tool for historic visualization funded by the NEH to weave images, maps, charts, video and data into highly interactive and compelling dynamic visualizations.  VisualEyes5 is a HTML5 version of the VisualEyes authoring tool for historic visualization to weave images, maps, charts, video and data into highly interactive and compelling dynamic visualizations.",
            "title": "Tools, Visualization"
        },
        {
            "location": "/Learn Git/",
            "text": "Foreword\n\n\nNotes.\n\n\n\n\nCrash Course\n\u00b6\n\n\n\n\nGot 15 minutes and want to learn Git?\n\n\n\n\nCourses\n\u00b6\n\n\n\n\nGit - SVN Crash Course\n.\n\n\nIntroduction to Git Extensions\n.\n\n\nThe Beginner\u2019s Guide for GIT Extensions: How to use GIT to clone repository from GitHub and make changes.\n\n\nManaging a repository.\n\n\nGenerating SSH Keys as one time activity.\n\n\nHow to Clone a Repository?\n\n\nHow to open a repository?\n\n\nHow to track the changes using Git Extensions?\n\n\nHow to perform Commit & Push?\n\n\n\n\n\n\n\n\n\n\nGit, Quick Guide\n.\n\n\nBy Tutorialspoint.\n\n\n\n\n\n\n\n\nGit\n\u00b6\n\n\n\n\nGit, Official Website\n.\n\n\nGood resources for learning Git and GitHub\n.\n\n\n\n\nReferences\n\u00b6\n\n\n\n\nGit Reference\n.\n\n\nSimple documentation webpage.\n\n\n\n\n\n\nGitHub Guides (Official)\n.\n\n\nUnderstanding the GitHub Flow.\n\n\nHello World.\n\n\nContributing to Open Source on GitHub.\n\n\nGetting Started with GitHub Pages.\n\n\nGetting your project on GitHub.\n\n\nForking Projects.\n\n\nBe Social.\n\n\nMaking You Code Citable.\n\n\nMastering Issues.\n\n\nMastering Markdown.\n\n\nDocumenting your projects on GitHub.",
            "title": "Learn Git"
        },
        {
            "location": "/Learn Git/#courses",
            "text": "Git - SVN Crash Course .  Introduction to Git Extensions .  The Beginner\u2019s Guide for GIT Extensions: How to use GIT to clone repository from GitHub and make changes.  Managing a repository.  Generating SSH Keys as one time activity.  How to Clone a Repository?  How to open a repository?  How to track the changes using Git Extensions?  How to perform Commit & Push?      Git, Quick Guide .  By Tutorialspoint.",
            "title": "Courses"
        },
        {
            "location": "/Learn Git/#git",
            "text": "Git, Official Website .  Good resources for learning Git and GitHub .",
            "title": "Git"
        },
        {
            "location": "/Learn Git/#references",
            "text": "Git Reference .  Simple documentation webpage.    GitHub Guides (Official) .  Understanding the GitHub Flow.  Hello World.  Contributing to Open Source on GitHub.  Getting Started with GitHub Pages.  Getting your project on GitHub.  Forking Projects.  Be Social.  Making You Code Citable.  Mastering Issues.  Mastering Markdown.  Documenting your projects on GitHub.",
            "title": "References"
        },
        {
            "location": "/Resources for Data Science/",
            "text": "Foreword\n\n\nNotes.\n\n\n\n\nData (Open)\n\u00b6\n\n\n\n\nDonn\u00e9es Qu\u00e9bec\n.\n\n\nOpen Data Canada\n.\n\n\nOpen Data US\n.\n\n\nMondo\n.\n\n\nData hub.\n\n\n\n\n\n\nThe Dataverse Project\n.\n\n\nOpen source research data repository software.\n\n\n\n\n\n\nGapminder\n.\n\n\nQuandl\n.\n\n\nHigh-quality financial and economic data in many formats.\n\n\n\n\n\n\nFind the Data\n.\n\n\nFindTheData is a reference site that uses Graphiq\u2019s semantic technology to deliver deep insights via data-driven articles, visualizations and research tools.\n\n\n\n\n\n\nKnoema\n.\n\n\nRecherche intelligente avec toutes les statistiques entre vos mains\n\n\n\n\n\n\n\n\nBoardgame Data\n\u00b6\n\n\n\n\nBGG Data Mining, Data Science Group\n.\n\n\nBGG Geek Tools\n.\n\n\n\n\nData Mining, Data Wrangling, and Data Munging\n\u00b6\n\n\n\n\nR and Data Mining\n.\n\n\nLinks.\n\n\n\n\n\n\nAn Introduction to Data Mining\n.\n\n\nIntro course.\n\n\n\n\n\n\nimport.io\n.\n\n\nExtract web data the easy way.\n\n\n\n\n\n\nThe Data Mining Page\n.\n\n\nLinks.\n\n\n\n\n\n\nScraPy\n.\n\n\nScrape web sites to get information off them. \n\n\nAn open source and collaborative framework for extracting the data you need from websites. In a fast, simple, yet extensible way.\n\n\n\n\n\n\n\n\nData Visualization and Storytelling\n\u00b6\n\n\n\n\nData Visualization with JavaScript\n.\n\n\nD3.js Gallery\n.\n\n\n\n\n\n\nR ggplot2 package\n.\n\n\nR ggvis package\n.\n\n\nR shiny package\n.\n\n\nDemo\n.\n\n\nGallery\n.\n\n\n\n\n\n\nGephi\n.\n\n\nVisualization and exploration software for all kinds of graphs and networks. Gephi is open-source and free.\n\n\n\n\n\n\nKaty B\u00f6rner\n.\n\n\nDesislava Hristova\n.\n\n\nLAB1100.com\n.\n\n\nIndependent research and software development firm. We built high quality applications driven by research questions, educational challenges and cultural encounters.\n\n\nNODEGOAT\n.\n\n\n\n\n\n\nParaView\n.\n\n\nOpen-source, multi-platform data analysis and visualization application. \n\n\n\n\n\n\nPeriscopic\n.\n\n\nTechnology to visualize solutions that engage the public and deliver messages of action. \n\n\n\n\n\n\nplotly\n.\n\n\nPlatform for agile business intelligence and data science.\n\n\n\n\n\n\nMicroStrategy.com\n.\n\n\nBuild dashboards.\n\n\n\n\n\n\nSAP Lumira\n.\n\n\nTake control and connect to multiple data sources, big and small, such as SAP HANA, SAP Business Warehouse, Excel spreadsheets and more.\n\n\n\n\n\n\nWeave\n.\n\n\nWeb-based analysis and visualization environment designed to enable visualization of any available data by anyone for any purpose.\n\n\n\n\n\n\nBlocks (examples)\n.\n\n\nD3.js gallery.\n\n\n\n\n\n\nBokeh\n.\n\n\nPython package for interactive and web-based data visualization.\n\n\nCallbacks\n.\n\n\n\n\n\n\nPanda3D\n.\n\n\nPanda3D is a game engine, a framework for 3D rendering and game development for Python and C++ programs. Panda3D is Open Source and free for any purpose, including commercial ventures, thanks to its liberal license.\n\n\n\n\n\n\nRen\u2019Py\n.\n\n\nRen\u2019Py is a visual novel engine used by hundreds of creators from around the world that helps you use words, images, and sounds to tell interactive stories that run on computers and mobile devices.\n\n\n\n\n\n\n\n\nGIS & Mapping\n\u00b6\n\n\n\n\nA Free and Open Source Geographic Information System \n.\n\n\nCARTO\n.\n\n\nCARTO is an open, powerful, and intuitive platform for discovering and predicting the key insights underlying the location data in our world.\n\n\n\n\n\n\n\n\nGUI & Interfaces\n\u00b6\n\n\n\n\nKivy\n.\n\n\nOpen source Python library for rapid development of applications that make use of innovative user interfaces, such as multi-touch apps.\n\n\n\n\n\n\n\n\nImage Processing\n\u00b6\n\n\n\n\nSimpleCV\n.\n\n\nComputer Vision platform using Python. \n\n\nMaking your computer see things in the real world.\n\n\nSimpleCV is an open source framework for building computer vision applications. With it, you get access to several high-powered computer vision libraries, such as OpenCV, without having to first learn about bit depths, file formats, color spaces, buffer management, eigenvalues, or matrix versus bitmap storage.\n\n\nBook : Practical Computer Vision, O\u2019Reilly.\n\n\n\n\n\n\n\n\nInfographics\n\u00b6\n\n\n\n\nPiktochart\n.\n\n\nInfographic maker.\n\n\n\n\n\n\nCanvas\n.\n\n\nEasy, drag-and-drop infographic creator.\n\n\n\n\n\n\nVizualize.me\n.\n\n\nCreate your infographic resume for free.\n\n\n\n\n\n\nGoogle Charts\n.\n\n\nGoogle chart tools are powerful, simple to use, and free;  rich gallery of interactive charts and data tools. \n\n\n\n\n\n\neasel.ly\n.\n\n\nCr\u00e9er et partager des id\u00e9es de visuels .\n\n\n\n\n\n\ninfogr.am\n.\n\n\nCreate and publish beautiful visualizations of your data. Interactive, responsive and engaging.\n\n\n\n\n\n\nVenngage\n.\n\n\nverything you need to create and publish infographics is right here.\n\n\n\n\n\n\n\n\nKids and Coding\n\u00b6\n\n\n\n\nCodecombat\n.\n\n\nMulti-language.\n\n\nEducative.\n\n\nPython and web languages for creating games.\n\n\n\n\n\n\nLifelong Kindergarten\n.\n\n\nMIT.\n\n\n\n\n\n\nScratch\n.\n\n\nIntroduction to programming.\n\n\n\n\n\n\nRobotique\n.\n\n\nFestival.\n\n\n\n\n\n\nHabiloM\u00e9dias\n.\n\n\nDigital literacy.\n\n\n\n\n\n\nClassCraft\n.\n\n\nFor schools.\n\n\n\n\n\n\n\n\nNLP\n\u00b6\n\n\n\n\nNLTK\n.\n\n\nNatural Language Tool Kit for analyzing written text and writing things like spam filters and chat bots. NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n\n\n\n\n\n\n\n\nOnline Programs, Courses, Lessons, and Tutorials\n\u00b6\n\n\n\n\nThinkful\n.\n\n\nPrograms: web development bootcamp, frontend, backend (Flask Python-based frameworks), mobile, design, UX, data science, individual, corporate.\n\n\n\n\n\n\nFreecodecamp\n.\n\n\n4 programs: web development, front-end, back-end, full stack, data visualization.\n\n\n\n\n\n\nCode School\n.\n\n\nPaths and courses: web and mobile development, Django Python-based frameworks, git, bash, regex, R, golang.\n\n\n\n\n\n\nCodecademy\n\n\nPaths and courses: web and mobile development, git, SQL, Java, Python.\n\n\n\n\n\n\nCoursera\n.\n\n\nMOOC.\n\n\n\n\n\n\nSkilledup\n.\n\n\neLearning for business.\n\n\n\n\n\n\nedX\n.\n\n\nMOOC.\n\n\n\n\n\n\nvideo2brain\n.\n\n\neLearning.\n\n\n\n\n\n\nnopaymba\n.\n\n\neLearning.\n\n\n\n\n\n\nUdemy\n.\n\n\neLearning.\n\n\n\n\n\n\nWiley Online Training\n.\n\n\neLearning. \n\n\nCertifications.\n\n\n\n\n\n\nSpringboard\n.\n\n\nPrograms: data science, data analytics, UX design.\n\n\n\n\n\n\nUdacity\n.\n\n\nPrograms and courses: vr, self-driving car, predictive analytics, Android and iOS development, machine learning, programming, front-end developper, full-stack developer, data analysis.\n\n\n\n\n\n\nSQL Teaching\n.\n\n\nSQL lessons.\n\n\n\n\n\n\nw3scholls\n.\n\n\nTutorials: web development.\n\n\n\n\n\n\nMongoDB University\n.\n\n\nCourses.\n\n\n\n\n\n\n\n\nStatistics.com\n.\n\n\n\n\nPrograms and courses. \n\n\nCerfications.\n\n\n\n\n\n\n\n\nTutorialspoint\n.\n\n\n\n\nAll topics.\n\n\n\n\n\n\nTizag\n.\n\n\nCoding.\n\n\n\n\n\n\nLearnshell\n.\n\n\n10 languages.\n\n\n\n\n\n\nLearn Python and\u2026\n.\n\n\n8 other languages.\n\n\n\n\n\n\nList of Bootcamps\n.\n\n\nList of Bootcamps\n.\n\n\nGeneral Assembly\n.\n\n\n\n\n\n\n\n\nOnline Reporting & Publishing\n\u00b6\n\n\n\n\nRPubs\n.\n\n\nWrite R Markdown documents in RStudio. Share them on RPubs.\n\n\n\n\n\n\nbl.ocks.org\n.\n\n\nSimple viewer for sharing code examples hosted on GitHub Gist.  \n\n\n\n\n\n\nGist GitHub\n.\n\n\nCode snippets.\n\n\n\n\n\n\nBitbucket\n.\n\n\nGitHub\n.\n\n\nGitLab\n.\n\n\nSourceforge\n.\n\n\n\n\nParallel and Distributed Computing (Big Data)\n\u00b6\n\n\n\n\nBases de donn\u00e9es documentaires et distribu\u00e9es\n.\n\n\nParallel computing, NoSQL, XML, MapReduce, distributed computing, indexation, Eleasticsearch, JSON, CouchBase, Pig, Spark, MongoDB, replication, scalability, cloud, virtual machine, markov chaine, page rank, Solr, CouchDB, XQuery, \n\n\n\n\n\n\nCloudera\n.\n\n\nHadoop ecosystem distribution.\n\n\n\n\n\n\nHortonworks\n.\n\n\nHadoop ecosystem distribution.\n\n\n\n\n\n\n\n\nPython Web Frameworks\n\u00b6\n\n\n\n\nDjango vs Flask vs Pyramid: Choosing a Python Web Framework\n.\n\n\n\n\nStatistics, Statistical & Machine Learning\n\u00b6\n\n\n\n\nCours de programmation sous R\n.\n\n\nLinks,\n\n\n\n\n\n\nStatistical Textbook\n.\n\n\nBasic and advanced concepts.\n\n\n\n\n\n\nAn Introduction to Statistical Learning with Applications in R\n.\n\n\nCases, codes, datasets.\n\n\n\n\n\n\nStatistical Consultants\n.\n\n\nBlog, datasets, cases, codes.\n\n\n\n\n\n\nIntroduction to Stats and Data Analysis\n.\n\n\nVideo courses.\n\n\n\n\n\n\nScikit\n.\n\n\nSciKit-Learn for machine learning applications. \n\n\nSimple and efficient tools for data mining and data analysis.  \n\n\nAccessible to everybody, and reusable in various contexts. \n\n\nBuilt on NumPy, SciPy, and matplotlib. \n\n\nOpen source, commercially usable. BSD license. \n\n\n\n\n\n\nR Documentation\n.\n\n\nSearch all CRAN, BioConductor and Github packages.\n\n\n\n\n\n\n\n\nVirtual Console and Virtual Coding\n\u00b6\n\n\n\n\nrepl.it\n.\n\n\nEverything you need to teach coding in your classroom.\n\n\nCode in the cloud and interactive environment. \n\n\n30 languages.\n\n\n\n\n\n\nCodeanywhere\n.\n\n\nCross platform cloud IDE.\n\n\n\n\n\n\nR-Fiddle\n.\n\n\nEnvironment to write, run and share R-code right inside your browser. \n\n\nIt even offers the option to include packages. \n\n\n\n\n\n\ndataiku\n.\n\n\nA collaborative data science platform\n\n\n\n\n\n\nHeroku\n.\n\n\nPlatform for building with modern architectures.\n\n\nInnovating quickly and scaling precisely to meet demand.\n\n\n\n\n\n\n\n\nVisualizing and Inspecting the Code\n\u00b6\n\n\n\n\nPython Tutor\n.\n\n\nOnline; Python, Java, JavaScript, TypeScript, Ruby, C, C++.\n\n\n\n\n\n\nPyDesk Visualizer\n.\n\n\nPyDesk Visualizer is the desktop based Python Visualizer. It helps you to visualize Python Code. So one can easily understand execution of program.\n\n\n\n\n\n\nbl.ocks.org\n.\n\n\nSimple viewer for sharing code examples hosted on GitHub Gist.",
            "title": "Resources for Data Science"
        },
        {
            "location": "/Resources for Data Science/#boardgame-data",
            "text": "BGG Data Mining, Data Science Group .  BGG Geek Tools .",
            "title": "Boardgame Data"
        },
        {
            "location": "/Resources for Data Science/#data-mining-data-wrangling-and-data-munging",
            "text": "R and Data Mining .  Links.    An Introduction to Data Mining .  Intro course.    import.io .  Extract web data the easy way.    The Data Mining Page .  Links.    ScraPy .  Scrape web sites to get information off them.   An open source and collaborative framework for extracting the data you need from websites. In a fast, simple, yet extensible way.",
            "title": "Data Mining, Data Wrangling, and Data Munging"
        },
        {
            "location": "/Resources for Data Science/#data-visualization-and-storytelling",
            "text": "Data Visualization with JavaScript .  D3.js Gallery .    R ggplot2 package .  R ggvis package .  R shiny package .  Demo .  Gallery .    Gephi .  Visualization and exploration software for all kinds of graphs and networks. Gephi is open-source and free.    Katy B\u00f6rner .  Desislava Hristova .  LAB1100.com .  Independent research and software development firm. We built high quality applications driven by research questions, educational challenges and cultural encounters.  NODEGOAT .    ParaView .  Open-source, multi-platform data analysis and visualization application.     Periscopic .  Technology to visualize solutions that engage the public and deliver messages of action.     plotly .  Platform for agile business intelligence and data science.    MicroStrategy.com .  Build dashboards.    SAP Lumira .  Take control and connect to multiple data sources, big and small, such as SAP HANA, SAP Business Warehouse, Excel spreadsheets and more.    Weave .  Web-based analysis and visualization environment designed to enable visualization of any available data by anyone for any purpose.    Blocks (examples) .  D3.js gallery.    Bokeh .  Python package for interactive and web-based data visualization.  Callbacks .    Panda3D .  Panda3D is a game engine, a framework for 3D rendering and game development for Python and C++ programs. Panda3D is Open Source and free for any purpose, including commercial ventures, thanks to its liberal license.    Ren\u2019Py .  Ren\u2019Py is a visual novel engine used by hundreds of creators from around the world that helps you use words, images, and sounds to tell interactive stories that run on computers and mobile devices.",
            "title": "Data Visualization and Storytelling"
        },
        {
            "location": "/Resources for Data Science/#gis-mapping",
            "text": "A Free and Open Source Geographic Information System  .  CARTO .  CARTO is an open, powerful, and intuitive platform for discovering and predicting the key insights underlying the location data in our world.",
            "title": "GIS &amp; Mapping"
        },
        {
            "location": "/Resources for Data Science/#gui-interfaces",
            "text": "Kivy .  Open source Python library for rapid development of applications that make use of innovative user interfaces, such as multi-touch apps.",
            "title": "GUI &amp; Interfaces"
        },
        {
            "location": "/Resources for Data Science/#image-processing",
            "text": "SimpleCV .  Computer Vision platform using Python.   Making your computer see things in the real world.  SimpleCV is an open source framework for building computer vision applications. With it, you get access to several high-powered computer vision libraries, such as OpenCV, without having to first learn about bit depths, file formats, color spaces, buffer management, eigenvalues, or matrix versus bitmap storage.  Book : Practical Computer Vision, O\u2019Reilly.",
            "title": "Image Processing"
        },
        {
            "location": "/Resources for Data Science/#infographics",
            "text": "Piktochart .  Infographic maker.    Canvas .  Easy, drag-and-drop infographic creator.    Vizualize.me .  Create your infographic resume for free.    Google Charts .  Google chart tools are powerful, simple to use, and free;  rich gallery of interactive charts and data tools.     easel.ly .  Cr\u00e9er et partager des id\u00e9es de visuels .    infogr.am .  Create and publish beautiful visualizations of your data. Interactive, responsive and engaging.    Venngage .  verything you need to create and publish infographics is right here.",
            "title": "Infographics"
        },
        {
            "location": "/Resources for Data Science/#kids-and-coding",
            "text": "Codecombat .  Multi-language.  Educative.  Python and web languages for creating games.    Lifelong Kindergarten .  MIT.    Scratch .  Introduction to programming.    Robotique .  Festival.    HabiloM\u00e9dias .  Digital literacy.    ClassCraft .  For schools.",
            "title": "Kids and Coding"
        },
        {
            "location": "/Resources for Data Science/#nlp",
            "text": "NLTK .  Natural Language Tool Kit for analyzing written text and writing things like spam filters and chat bots. NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.",
            "title": "NLP"
        },
        {
            "location": "/Resources for Data Science/#online-programs-courses-lessons-and-tutorials",
            "text": "Thinkful .  Programs: web development bootcamp, frontend, backend (Flask Python-based frameworks), mobile, design, UX, data science, individual, corporate.    Freecodecamp .  4 programs: web development, front-end, back-end, full stack, data visualization.    Code School .  Paths and courses: web and mobile development, Django Python-based frameworks, git, bash, regex, R, golang.    Codecademy  Paths and courses: web and mobile development, git, SQL, Java, Python.    Coursera .  MOOC.    Skilledup .  eLearning for business.    edX .  MOOC.    video2brain .  eLearning.    nopaymba .  eLearning.    Udemy .  eLearning.    Wiley Online Training .  eLearning.   Certifications.    Springboard .  Programs: data science, data analytics, UX design.    Udacity .  Programs and courses: vr, self-driving car, predictive analytics, Android and iOS development, machine learning, programming, front-end developper, full-stack developer, data analysis.    SQL Teaching .  SQL lessons.    w3scholls .  Tutorials: web development.    MongoDB University .  Courses.     Statistics.com .   Programs and courses.   Cerfications.     Tutorialspoint .   All topics.    Tizag .  Coding.    Learnshell .  10 languages.    Learn Python and\u2026 .  8 other languages.    List of Bootcamps .  List of Bootcamps .  General Assembly .",
            "title": "Online Programs, Courses, Lessons, and Tutorials"
        },
        {
            "location": "/Resources for Data Science/#online-reporting-publishing",
            "text": "RPubs .  Write R Markdown documents in RStudio. Share them on RPubs.    bl.ocks.org .  Simple viewer for sharing code examples hosted on GitHub Gist.      Gist GitHub .  Code snippets.    Bitbucket .  GitHub .  GitLab .  Sourceforge .",
            "title": "Online Reporting &amp; Publishing"
        },
        {
            "location": "/Resources for Data Science/#parallel-and-distributed-computing-big-data",
            "text": "Bases de donn\u00e9es documentaires et distribu\u00e9es .  Parallel computing, NoSQL, XML, MapReduce, distributed computing, indexation, Eleasticsearch, JSON, CouchBase, Pig, Spark, MongoDB, replication, scalability, cloud, virtual machine, markov chaine, page rank, Solr, CouchDB, XQuery,     Cloudera .  Hadoop ecosystem distribution.    Hortonworks .  Hadoop ecosystem distribution.",
            "title": "Parallel and Distributed Computing (Big Data)"
        },
        {
            "location": "/Resources for Data Science/#python-web-frameworks",
            "text": "Django vs Flask vs Pyramid: Choosing a Python Web Framework .",
            "title": "Python Web Frameworks"
        },
        {
            "location": "/Resources for Data Science/#statistics-statistical-machine-learning",
            "text": "Cours de programmation sous R .  Links,    Statistical Textbook .  Basic and advanced concepts.    An Introduction to Statistical Learning with Applications in R .  Cases, codes, datasets.    Statistical Consultants .  Blog, datasets, cases, codes.    Introduction to Stats and Data Analysis .  Video courses.    Scikit .  SciKit-Learn for machine learning applications.   Simple and efficient tools for data mining and data analysis.    Accessible to everybody, and reusable in various contexts.   Built on NumPy, SciPy, and matplotlib.   Open source, commercially usable. BSD license.     R Documentation .  Search all CRAN, BioConductor and Github packages.",
            "title": "Statistics, Statistical &amp; Machine Learning"
        },
        {
            "location": "/Resources for Data Science/#virtual-console-and-virtual-coding",
            "text": "repl.it .  Everything you need to teach coding in your classroom.  Code in the cloud and interactive environment.   30 languages.    Codeanywhere .  Cross platform cloud IDE.    R-Fiddle .  Environment to write, run and share R-code right inside your browser.   It even offers the option to include packages.     dataiku .  A collaborative data science platform    Heroku .  Platform for building with modern architectures.  Innovating quickly and scaling precisely to meet demand.",
            "title": "Virtual Console and Virtual Coding"
        },
        {
            "location": "/Resources for Data Science/#visualizing-and-inspecting-the-code",
            "text": "Python Tutor .  Online; Python, Java, JavaScript, TypeScript, Ruby, C, C++.    PyDesk Visualizer .  PyDesk Visualizer is the desktop based Python Visualizer. It helps you to visualize Python Code. So one can easily understand execution of program.    bl.ocks.org .  Simple viewer for sharing code examples hosted on GitHub Gist.",
            "title": "Visualizing and Inspecting the Code"
        },
        {
            "location": "/Storytelling with Data/",
            "text": "Foreword\n\n\nNotes.\n\n\n\n\nThe importance of context\n\u00b6\n\n\n\n\nExploratory vs explanatory graphics.\n\n\nExploratory is for the analyst.\n\n\nExploratory is for the audience.\n\n\n\n\n\n\nWho is the audience, who is the stakeholder?\n\n\nWhat do you need your audience to know or do?\n\n\nHow will you communicate to your audience? Live or written by email?\n\n\nWhat tone do you want your communication to set?\n\n\nWhat data is available that will help make your point?\n\n\nIf you had 3 minutes to tell a story, what would be the big idea : the unique point of view in one sentence?\n\n\nLet\u2019s now move to storyboarding.\n\n\n\n\nChoosing an effective visual\n\u00b6\n\n\n\n\nGo with a simple text or a figure.\n\n\nAvailable charts: scatterplot, table, lines, slopegraph, vertical bars, horizontal bars, stacked vertical bars, stacked horizontal bars, waterfall, square areas.\n\n\nTable? Never in a live presentation (too dense).\n\n\nHeatmap? Never! A better alternative is a slopegraph.\n\n\nColor saturation is for visual contrast: put everything  in B&W except the one thing you want to draw attention on.\n\n\nSpeak in relative, not in absolute: scatterplot, average, percentage, waterfall (change, increment), areas for comparing categories.\n\n\nAvoid clutter and 3D, avoid saturated series and secondary axes.\n\n\nPrefer a zero baseline and equal scales among several graphics.\n\n\nPrefer square forms over round forms.\n\n\nAvoid pie charts or donut charts.\n\n\nPrefer a horizontal bar chart.\n\n\n\n\n\n\n\n\nClutter is your enemy !\n\u00b6\n\n\n\n\nAvoid cognitive load: a simple chart, a few line, one image.\n\n\nAvoid borders (limited to the L axes), avoid gridlines and background shading. \n\n\nThe y-axis is often facultative.\n\n\nClean up axis labels; summarize labels (Jan vs January)\n\n\nBold and colors is for making the important stand out.\n\n\nLimit to 2 or 3 colors.\n\n\nAdd white space.\n\n\nAdd details to the important areas (annotate).\n\n\nAdd the legend on the graph, not outside.\n\n\n\n\nFocus your audience\u2019s attention\n\u00b6\n\n\n\n\nUse preattentive attributes in text : bold, italics, separate spatially, underlined, more tone or color (B&W), data stand out, numbered data, size.\n\n\nAvoid shades of red and green.\n\n\nUse preattentive attributes in visual : difference in orientation, shape, length, width, size, curvature, added marks, enclosure, hue, intensity, spatial position, motion\n\n\nPosition on the page follows the 1-2-3-4 loop.\n\n\nConsult:\n\n\nCultural Color Connotations, David McCandless.\n\n\nThe Visual Miscellaneum : A Colourful Guide to the World\u2019s Most Consequential Trivia.\n\n\n\n\n\n\n\n\nThink like a designer\n\u00b6\n\n\n\n\nEliminate distractions.\n\n\nNot all data are equally important.\n\n\nWhen detail is not needed, summarize.\n\n\nWould eliminating \u2018this\u2019 change anything?\n\n\nPush necessary, but non-message-impacting items to the background.\n\n\nThe power of subcategories, subdivisions.\n\n\nLegible, clean, straightforward language.\n\n\nAction titles.\n\n\n\n\nLessons in storytelling\n\u00b6\n\n\n\n\nWrite a story with a schema (in 3 acts).\n\n\nWrite a narrative structure (the hero\u2019s journey).\n\n\nUse the power of repetition: 3, 7, 12, 30, 40, 100, 500, 1000.\n\n\n\n\nFinal thoughts\n\u00b6\n\n\n\n\nLearn your tools well.\n\n\nGoogle Spreadsheet, Tableau, R, D3 (JavaScript), Processing, Python, etc.\n\n\n\n\n\n\nIterate.\n\n\nDevote time to storytelling with data.\n\n\nSeek inspiration throught good examples.\n\n\nFind your style.\n\n\nConsult websites.\n\n\nEagereyes, Robert Kosara\n.\n\n\nFiveThirdyEight\u2019s Data Lab\n.\n\n\nFlowing Data, Nathan Yau\n.\n\n\nThe Functional Art, Alberto Cairo\n.\n\n\nThe Gardian Data Blog\n.\n\n\nHelpMeViz, Jon Schwabish\n.\n\n\nJunk Carts, Kaiser Fung\n.\n\n\nMake a Powerful Point, Gavin McMahon\n.\n\n\nPerceptual Edge, Stephen Few\n.\n\n\nVisualising Data, Andy Kirk\n.\n\n\nVizWiz, Andy Kriebel\n.\n\n\nStorytelling with data, Cole Nussbaumer Knaflic\n.\n\n\nand many more.\n\n\n\n\n\n\n\n\nWrap up\n\u00b6\n\n\n\n\nUnderstand the context.\n\n\nChoose an appropriate visual display.\n\n\nEliminate clutter.\n\n\nFocus attention where you want it.\n\n\nThink like a designer.\n\n\nTell a story.",
            "title": "Storytelling with Data"
        },
        {
            "location": "/Storytelling with Data/#choosing-an-effective-visual",
            "text": "Go with a simple text or a figure.  Available charts: scatterplot, table, lines, slopegraph, vertical bars, horizontal bars, stacked vertical bars, stacked horizontal bars, waterfall, square areas.  Table? Never in a live presentation (too dense).  Heatmap? Never! A better alternative is a slopegraph.  Color saturation is for visual contrast: put everything  in B&W except the one thing you want to draw attention on.  Speak in relative, not in absolute: scatterplot, average, percentage, waterfall (change, increment), areas for comparing categories.  Avoid clutter and 3D, avoid saturated series and secondary axes.  Prefer a zero baseline and equal scales among several graphics.  Prefer square forms over round forms.  Avoid pie charts or donut charts.  Prefer a horizontal bar chart.",
            "title": "Choosing an effective visual"
        },
        {
            "location": "/Storytelling with Data/#clutter-is-your-enemy",
            "text": "Avoid cognitive load: a simple chart, a few line, one image.  Avoid borders (limited to the L axes), avoid gridlines and background shading.   The y-axis is often facultative.  Clean up axis labels; summarize labels (Jan vs January)  Bold and colors is for making the important stand out.  Limit to 2 or 3 colors.  Add white space.  Add details to the important areas (annotate).  Add the legend on the graph, not outside.",
            "title": "Clutter is your enemy !"
        },
        {
            "location": "/Storytelling with Data/#focus-your-audiences-attention",
            "text": "Use preattentive attributes in text : bold, italics, separate spatially, underlined, more tone or color (B&W), data stand out, numbered data, size.  Avoid shades of red and green.  Use preattentive attributes in visual : difference in orientation, shape, length, width, size, curvature, added marks, enclosure, hue, intensity, spatial position, motion  Position on the page follows the 1-2-3-4 loop.  Consult:  Cultural Color Connotations, David McCandless.  The Visual Miscellaneum : A Colourful Guide to the World\u2019s Most Consequential Trivia.",
            "title": "Focus your audience\u2019s attention"
        },
        {
            "location": "/Storytelling with Data/#think-like-a-designer",
            "text": "Eliminate distractions.  Not all data are equally important.  When detail is not needed, summarize.  Would eliminating \u2018this\u2019 change anything?  Push necessary, but non-message-impacting items to the background.  The power of subcategories, subdivisions.  Legible, clean, straightforward language.  Action titles.",
            "title": "Think like a designer"
        },
        {
            "location": "/Storytelling with Data/#lessons-in-storytelling",
            "text": "Write a story with a schema (in 3 acts).  Write a narrative structure (the hero\u2019s journey).  Use the power of repetition: 3, 7, 12, 30, 40, 100, 500, 1000.",
            "title": "Lessons in storytelling"
        },
        {
            "location": "/Storytelling with Data/#final-thoughts",
            "text": "Learn your tools well.  Google Spreadsheet, Tableau, R, D3 (JavaScript), Processing, Python, etc.    Iterate.  Devote time to storytelling with data.  Seek inspiration throught good examples.  Find your style.  Consult websites.  Eagereyes, Robert Kosara .  FiveThirdyEight\u2019s Data Lab .  Flowing Data, Nathan Yau .  The Functional Art, Alberto Cairo .  The Gardian Data Blog .  HelpMeViz, Jon Schwabish .  Junk Carts, Kaiser Fung .  Make a Powerful Point, Gavin McMahon .  Perceptual Edge, Stephen Few .  Visualising Data, Andy Kirk .  VizWiz, Andy Kriebel .  Storytelling with data, Cole Nussbaumer Knaflic .  and many more.",
            "title": "Final thoughts"
        },
        {
            "location": "/Storytelling with Data/#wrap-up",
            "text": "Understand the context.  Choose an appropriate visual display.  Eliminate clutter.  Focus attention where you want it.  Think like a designer.  Tell a story.",
            "title": "Wrap up"
        },
        {
            "location": "/Charts/",
            "text": "Foreword\n\n\nNotes.\n\n\n\n\nCharts\n\u00b6\n\n\n\n\nChart & Diagrams, 2 pages\n. PDF only.\n\n\nChart & Diagrams, 11x17\n. PDF only.\n\n\nWhich Chart v6\n. PDF only. \n\n\nCharts\n. PDF.\n\n\n\n\n\n\n\n\n\n\nColors\n\u00b6",
            "title": "Charts"
        },
        {
            "location": "/Charts/#colors",
            "text": "",
            "title": "Colors"
        },
        {
            "location": "/Data_Storytelling/",
            "text": "Foreword\n\n\nNotes. From Harvard Business Review.\n\n\n\n\nTime\n\u00b6\n\n\n1- \nReporting\n: perform descriptive analytics. Tell what happened.\n\n\n2- \nExplanatory survey\n: analyze what people or objects are up to. Ask people what they think about something. Conceive a statistical model; what factors drive others.\n\n\n3- \nPrediction\n: perform predictive analytics. Use historical data, add a statistical model, probabilities, and assumptions,  predict the future. Find out what customers are likely to buy. Assess how likely it is for an event to happen. Forecast economic conditions. \n\n\nFocus\n\u00b6\n\n\n4- \nWhat\n: tell what happened with a \nfocus\n on one issue (\nReporting\n is not as focused).\n\n\n5- \nWhy\n: tell what underlying factors caused the outcome. \nFocus\n on the outcome.\n\n\n6- \nHow\n to address the issue: explore various ways to improve the situation. \nFocus\n on the situation.\n\n\nDepth\n\u00b6\n\n\n7- \nCSI\n: run a small, ad hoc investigation. Find out why something is happening. Find out why some customer are dropping online transactions when they get to the postal code input form; what if some rural locations don\u2019t have postal codes?\n\n\n8- \nEureka\n: invest in long, analytically-driven searches for a solution to a complex problem. Discover the right way to refer and price potential buyers to real estate agents on a website. Since the project is core to the company\u2019s business model, it needs a corporate buy-in. The project involve several different analytical methods, false starts, dead-ends, discoveries.\n\n\nMethods\n\u00b6\n\n\n9- \nCorrelations\n: find why the relationships among variables rise and fall at the same time. \n\n\n10- \nCausation\n: argue that one variable caused the other. Controlled experiment.\n\n\nAfterword\n\u00b6\n\n\n\n\nThese 10 approaches are not mutually exclusive. \n\n\nBegin the report with the result and recommended outcome. Follow with the demonstration.\n\n\nFor the c-suite, keep technical terms to the minimum.",
            "title": "Ten Kinds of Stories to Tell with Data"
        },
        {
            "location": "/Data_Storytelling/#focus",
            "text": "4-  What : tell what happened with a  focus  on one issue ( Reporting  is not as focused).  5-  Why : tell what underlying factors caused the outcome.  Focus  on the outcome.  6-  How  to address the issue: explore various ways to improve the situation.  Focus  on the situation.",
            "title": "Focus"
        },
        {
            "location": "/Data_Storytelling/#depth",
            "text": "7-  CSI : run a small, ad hoc investigation. Find out why something is happening. Find out why some customer are dropping online transactions when they get to the postal code input form; what if some rural locations don\u2019t have postal codes?  8-  Eureka : invest in long, analytically-driven searches for a solution to a complex problem. Discover the right way to refer and price potential buyers to real estate agents on a website. Since the project is core to the company\u2019s business model, it needs a corporate buy-in. The project involve several different analytical methods, false starts, dead-ends, discoveries.",
            "title": "Depth"
        },
        {
            "location": "/Data_Storytelling/#methods",
            "text": "9-  Correlations : find why the relationships among variables rise and fall at the same time.   10-  Causation : argue that one variable caused the other. Controlled experiment.",
            "title": "Methods"
        },
        {
            "location": "/Data_Storytelling/#afterword",
            "text": "These 10 approaches are not mutually exclusive.   Begin the report with the result and recommended outcome. Follow with the demonstration.  For the c-suite, keep technical terms to the minimum.",
            "title": "Afterword"
        },
        {
            "location": "/Infographies/",
            "text": "Foreword\n\n\nNotes. An image is worth\u2026\n\n\n\n\nBig \u2013 Data \u2013 Science\n\u00b6\n\n\nThe rise\n\n\n\n\nWhat?\n\n\n\n\nProcess\n\n\n\n\nData Science\n\u00b6\n\n\nKnowledge base\n\n\n\n\n\n\n\n\n\n\n\n\nLearning\n\n\n\n\nTalent Big Picture\n\n\n\n\n\n\n\n\nData Scientist\n\n\nHybrid role\u2026\n\n\n\n\nCross Over to Data Science\n\n\n\n\nLanguages and Software\n\u00b6\n\n\n\n\nProgramming Languages\n\n\n\n\nData Software\n \n\n\n\n\nScience\n\u00b6",
            "title": "Infographies"
        },
        {
            "location": "/Infographies/#data-science",
            "text": "Knowledge base       Learning   Talent Big Picture     Data Scientist  Hybrid role\u2026   Cross Over to Data Science",
            "title": "Data Science"
        },
        {
            "location": "/Infographies/#languages-and-software",
            "text": "Programming Languages   Data Software",
            "title": "Languages and Software"
        },
        {
            "location": "/Infographies/#science",
            "text": "",
            "title": "Science"
        },
        {
            "location": "/OS_CS/",
            "text": "Foreword\n\n\nCheat sheets.\n\n\n\n\nLinux\n\u00b6\n\n\n\n\nUbuntu Reference\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinux Command Reference\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinux Command Line\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM\u00e9mo Console UNIX\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinux Bash Shell\n. PDF only.\n\n\nLearning the Shell\n.\n\n\nbc - An arbitrary precision calculator language\n.\n\n\n\n\n\n\n\n\nWindows\n\u00b6\n\n\n\n\nPowerShell Basics\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWindows PowerShell\n. PDF.",
            "title": "OS Cheat Sheets"
        },
        {
            "location": "/OS_CS/#windows",
            "text": "PowerShell Basics . PDF.        Windows PowerShell . PDF.",
            "title": "Windows"
        },
        {
            "location": "/Codecademy Learn the Command Line Notes/",
            "text": "Foreword\n\n\nCommands and code snippets. From Codecademy.\n\n\n\n\nNavigation\n\u00b6\n\n\n\n\nBash means \u2018Bourne again shell\u2019.\n\n\nThere exists other bash programs : ksh, tcsh, zsh, etc.\n\n\nCommon terminals: Ubuntu gnome, KDE konsole, eterm, txvt, kvt, nxterm, eterm.\n\n\nA CLI has its advantages over a GUI.\n\n\n\n\nTerminal\n\n\n\n\nbash\n; open another \u2018bash\u2019 in the same bash window.\n\n\nexit\n; close one instance of \u2018bash\u2019.\n\n\nexport\n; export variable to the next bash.\n\n\nexport MESSAGE=\"Hi\"\n.\n\n\nVariables are associated with one bash. \n\n\nOne change in one bash does not affect the other bash.\n\n\n\n\n\n\nman <cmd>\n; call the manual, help about a command.\n\n\nman cp\n; open the manual about command \ncp\n (or any other command).\n\n\n\n\n\n\ncp --help\n; command \ncp\n arguments (or any other command).\n\n\ncommand cp\n; get info on command \ncp\n (or any other command).\n\n\ntype cp\n; find where command \ncp\n is located and what kind of command (an \nalias\n or \nelse\n) (or any other command).\n\n\nwhich cp\n; locate command \ncp\n (or any other command).\n\n\nwhich make\n; check out where \nmake\n is. \n\n\npwd\n; print working directory.\n\n\ndir\n; list of directory.\n\n\nup\n/\ndown\n arrow; resume a past command.\n\n\ntab\n; autocomplete.\n\n\n\n\nChange dir\n\n\n\n\ncd/aaa\n; change to directory \naaa\n.\n\n\ncd ~\n, \ncd ~user\n; go back home to \nuser\n directory.\n\n\ncd ..\n; go up one level.\n\n\ncd ../..\n; go up two levels.\n\n\ncd ../../..\n; go up three levels.\n\n\ncd temp\n; go to directory \ntemp\n.\n\n\ncd stuff\n; if we are in dir \ntemp\n, switch to directory \nstuff\n.\n\n\ncd temp/stuff\n; switch directly to directory \nstuff\n.\n\n\ncd 'en co re'\n; switch to directory \nen co re\n.\n\n\n\n\nMake dir\n\n\n\n\nmkdir aaa\n; make directory \naaa\n.\n\n\nmkdir -p aaa/bbb\n; make directory \naaa\n with sub-directory \nbbb\n.\n\n\nmkdir 'en co re'\n; make directory \nen co re\n.\n\n\n\n\nRemove dir\n\n\n\n\nrmdir aaa\n; remove directory \naaa\n.\n\n\nrmdir 'en co re'\n; remove directory \nen co re\n.\n\n\nrmdir aaa/bbb\n; remove both directory and sub-directory (if both empty).\n\n\nrmdir -p aaa/bbb\n; remove all \n(you cannot remove a directory if it is non-empty (filled with sub-directories and files))\n.\n\n\nrmdir -p aaa/*\n; remove all.\n\n\n\n\n\n\n\n\nSwitch dir\n\n\n\n\npushd\n; save current location; then, change location (another directory).\n\n\npopd\n; return to pushd. \n(for example, you are in \naaa\n, type \npushd\n, go to \nbbb\n, type \npushb\n, go to \nccc\n, type \npopd\n, return to \nbbb\n, type \npopd\n, return to \naaa\n)\n.\n\n\n\n\nMake file\n\n\n\n\ntouch iamcoo.txt\n; create an empty text file.\n\n\ntouch temp/ismvoo.txt\n; create an empty text file in the \ntemp\n directory.\n\n\n\n\nEdit file\n\n\n\n\nless test.txt\n; open the file and display its content.\n\n\nWithin \nless\n :\n\n\nq\n; quit.\n\n\narrows, \npgUp\n, \npgDn\n; move.\n\n\nh\n; help.\n\n\n\n\n\n\nFor other files extensions like \ntar\n and GNU archive, use \ntvf\n.\n\n\n\n\nManipulation\n\u00b6\n\n\nList file & dir\n\n\n\n\nls\n; list of directories and files.\n\n\nls /bin/bash\n; list of a remote directory and files.\n\n\nls -a\n; list all.\n\n\nls -t\n; list in alphanumeric order, when they were last modified.\n\n\nls -l\n; list in long format.\n\n\nls -la\n; list all in long format.\n\n\nls -alt\n; list all in long format and ordered.\n\n\nls /aaa /bbb\n; list both directories.\n\n\nls -r\n; list in reverse.\n\n\nls ../paint/\n; list an upper directory.\n\n\n\n\nCopy file & dir\n\n\n\n\ncp aaa.txt bbb.txt\n; copy \naaa.txt\n file, paste it or create a copy named \nbbb.txt\n.\n\n\ncp aaa.txt dir/\n; copy \naaa.txt\n into directory \ndir\n.\n\n\ncp aaa.txt bbb.txt dir/\n; copy \naaa.txt\n and \nbbb.txt\ninto directory \ndir\n.\n\n\ncp \\*.txt dir/\n; copy all \n.txt\n files into directory \ndir\n.\n\n\ncp aaa.txt dir/dir2\n; copy \naaa.txt\n into sub-directory \ndir2\n.\n\n\ncp -r aaa bbb\n; copy \ndir1\n, create a copy names \ndir2\n with the exact same content.\n\n\ncp -i\n; interactive, to prompt the user.\n\n\ncp f\\* ../paint/\n; copy all files beginning with \nf\n to an upper directory.\n\n\n\n\nWildcards\n\n\n\n\n\\*\n; wildcard for any string.\n\n\n?\n; wildcard for any character.\n\n\n\\*.txt\n; all files finishing with \n.txt\n.\n\n\nr\\*\n; all files beginning with \nr\n.\n\n\n??a.txt\n; all files beginning with two characters + \na.txt\n.\n\n\nbackup[[:digit:]]\n; all file beginning with \nbackup\n + any digit.\n\n\n[abc]\\*\n; all files beginning with either \na\n, \nb\n or \nc\n\n\n[[:upper:]]\\*\n; all files beginning with an upper case.\n\n\n\\*[![:lower:]]\n; all file not finishing with a lower case.\n\n\n\n\nMove file & dir\n\n\n\n\nmv aaa.txt bbb.txt\n; move file or cut \naaa.txt\n and paste \nbbb.txt\n.\n\n\nmv aaa.txt dir/\n; move file \naaa.txt\n into directory \ndir\n.\n\n\nmv aaa.txt bbb.txt dir/\n; move files \naaa.txt\n and \nbbb.txt\n into directory \ndir\n.\n\n\nmv \\*.txt dir/\n; move all \n.txt\n files into directory \ndir\n.\n\n\nmv aaa.txt dir/dir2\n; move file \naaa.txt\n into sub-directory \ndir2\n.\n\n\nmv -i\n; interactive, prompt the user.\n\n\n\n\nRemove file & dir\n\n\n\n\nrm aaa.txt\n; remove file \naaa.txt\n.\n\n\nrm test1.txt test2.txt\n; remove both files.\n\n\nrm aaa/\\*\n; remove all files in the directory \naaa\n.\n\n\nrm -r aaa\n; remove directory \naaa\n, must be empty.\n\n\nrm -rf aaa\n; remove directory \naaa\n and its files.\n\n\nrm -i\n; interactive, prompt the user.\n\n\n\n\nRedirection\n\u00b6\n\n\n\n\nLet\u2019s begin by taking a closer look at input and output. In the terminal, after the shell prompt, type :\n\n\n\n\n1\n$ \necho\n \n'Hello'\n\n\n\n\n\n\n\n\n\nThe \necho\n command accepts the string \u2018Hello\u2019 as standard input, and echoes the string \n'Hello'\n back to the terminal as standard output. \n\n\nstandard input, abbreviated as \nstdin\n, is information inputted into the terminal through the keyboard or input device.\n\n\nstandard output, abbreviated as \nstdout\n, is the information outputted after a process is run.\n\n\nstandard error, abbreviated as \nstderr\n, is an error message outputted by a failed process.\n\n\n\n\n\n\nRedirection (\n>\n) reroutes standard input, standard output, and standard error to or from a different location. Type :\n\n\n\n\n1\n$ \necho\n \n'Hello'\n > hello.txt\n\n\n\n\n\n\n\n\nType :\n\n\n\n\n1\n$ cat hello.txt\n\n\n\n\n\n\n\n\nThe \n>\n command redirects the standard output to a file. The standard output \n'Hello'\n is redirected by \n>\n to the file \nhello.txt\n, and entered as the standard input.\n\n\n\n\nThe cat command outputs the contents of a file to the terminal. When you type \ncat hello.txt\n, the contents of \nhello.txt\n are displayed.\n\n\n\n\n\n\nType :\n\n\n\n\n\n\n1\n$ ls -l\n\n\n\n\n\n\n\n\nThis is the filesystem we\u2019ll work with. Create a file. Type :\n\n\n\n\n1\n$ touch ocean.txt\n\n\n\n\n\n\n\n\nFill the file with values (ocean names). Type :\n\n\n\n\n1\n$ cat oceans.txt > continents.txt\n\n\n\n\n\n\n\n\nUse \ncat\n to view the contents of \ncontinents.txt\n. Notice that we only see oceans as output:\n\n\n\n\n1\n$ cat continents.txt\n\n\n\n\n\n\n\n\n>\n takes the standard output of the command on the left, and redirects it to the file on the right. Here the standard output of cat \noceans.txt\n is redirected to \ncontinents.txt\n. Note that \n>\n OVERWRITES all original content in \ncontinents.txt\n. When you view the output data by typing cat on \ncontinents.txt\n, you will see only the contents of \noceans.txt\n. Type :\n\n\n\n\n1\n$ cat glaciers.txt >> rivers.txt\n\n\n\n\n\n\n\n\nUse \ncat\n to view the contents of \nrivers.txt\n:\n\n\n\n\n1\n$ cat rivers.txt\n\n\n\n\n\n\n\n\nNotice that we see both rivers and glaciers as output. Type:\n\n\n\n\n1\n$ cat glaciers.txt >> rivers.txt\n\n\n\n\n\n\n\n\n\n\n>>\n takes the standard output of the command on the left and APPENDS it to the file on the right. You can view the output data of the file with cat and the filename. Here, the output data of \nrivers.txt\n will contain the original contents of \nrivers.txt\n with the content of \nglaciers.txt\n appended to it. \n\n\n\n\n\n\nType :\n\n\n\n\n\n\n1\n$ touch lakes.txt\n\n\n\n\n\n\n\n\nFill the files with values (lake names). Type :\n\n\n\n\n1\n$ cat < lakes.txt\n\n\n\n\n\n\n\n\n<\n takes the standard input from the file on the right and inputs it into the program on the left. Here, \nlakes.txt\n is the standard input for the \ncat\n command. The standard output appears in the terminal. Let\u2019s try some more redirection commands. Type :\n\n\n\n\n1\n$ touch volcanoes.txt\n\n\n\n\n\n\n\n\nFill the files with values (volcano names). Type :\n\n\n\n\n1\n$ cat volcanoes.txt \n|\n wc\n\n\n\n\n\n\n\n\nYou get the count for: lines, words, bytes. Type :\n\n\n\n\n1\n$ cat volcanoes.txt \n|\n wc \n|\n cat > islands.txt\n\n\n\n\n\n\n\n\nUse \ncat\n to output the contents in \nislands.txt\n. The next command should now equals \n$ cat volcanoes.txt \n|\n wc\n.\n\n\n\n\n1\n$ cat volcanoes.txt \n|\n wc\n\n\n\n\n\n\n\n\n|\n is a \u2018pipe\u2019 or \u2018pipeline\u2019 The \n|\n takes the standard output of the command on the left, and PIPES it as standard input to the command on the right. You can think of this as \u2018command to command\u2019 redirection. Here, again, the output of cat \nvolcanoes.txt\n is the standard input of \nwc\n. In turn, the \nwc\n command outputs the number of lines, words, and characters in \nvolcanoes.txt\n, respectively:\n\n\n\n\n1\n2\n$ cat volcanoes.txt \n|\n wc \n|\n cat > islands.txt\n$ less islands.txt\n\n\n\n\n\n\n\n\nMultiple \n|\ns can be chained together. Here the standard output of cat \nvolcanoes.txt\n is \u2018piped\u2019 to the \nwc\n command. The standard output of \nwc\n is then \u2018piped\u2019 to cat. Finally, the standard output of cat is redirected to \nislands.txt\n. You can view the output data of this chain by typing cat \nislands.txt\n. \n\n\nA few commands are particularly powerful when combined with redirection. Let\u2019s try them out. First, use \ncat\n to output the contents of \nlakes.txt\n. Then, \nsort\n it. Type :\n\n\n\n\n1\n2\n$ cat lakes.txt\n$ sort lakes.txt\n\n\n\n\n\n\n\n\nThe lakes in \nlakes.txt\n are listed in alphabetical order. Type:\n\n\n\n\n1\n$ cat lakes.txt \n|\n sort\n\n\n\n\n\n\n\n\nsort\n takes the standard input and orders it alphabetically for the standard output. The lakes in \nlakes.txt\n are listed in alphabetical order.\n\n\nUse cat to output the contents of \nsorted-lakes.txt\n.\n\n\n\n\n1\n$ cat lakes.txt \n|\n sort > sorted-lakes.txt\n\n\n\n\n\n\n\n\nThe command takes the standard output from cat \nlakes.txt\n and \u2018pipes\u2019 it to sort in ascending order. The standard output of \nsort\n is redirected to \nsorted-lakes.txt\n. You can view the output data by typing:\n\n\n\n\n1\n$ cat sorted-lakes.txt\n\n\n\n\n\n\n\n\nType :\n\n\n\n\n1\n$ touch deserts.txt\n\n\n\n\n\n\n\n\nFill the file with values. Type :\n\n\n\n\n1\n2\n$ cat deserts.txt\n$ uniq deserts.txt\n\n\n\n\n\n\n\n\n\n\nYou get to see all entries and unique entries.\n\n\n\n\n\n\nType :\n\n\n\n\n\n\n1\n$ sort deserts.txt \n|\n uniq\n\n\n\n\n\n\n\n\nYou get to see  unique entries sorted. Type :\n\n\n\n\n1\n$ sort deserts.txt \n|\n uniq > uniq-deserts.txt\n\n\n\n\n\n\n\n\nYou save the result in \nuniq-deserts.txt\n.\n\n\nUse \ncat\n to output the contents of \nuniq-deserts.txt\n.\n\n\nuniq\n stands for \u2018unique\u2019 and filters out ADJACENT, duplicate lines in a file. Here \nuniq deserts.txt\n filters out duplicates of \u2018Sahara Desert\u2019, because the duplicate of \u2018Sahara Desert\u2019 directly follows the previous instance. The \u2018Kalahari Desert\u2019 duplicates are not adjacent, and thus remain.\n\n\nType :\n\n\n\n\n1\n$ grep Mount mountains.txt\n\n\n\n\n\n\n\n\ngrep\n stands for \u2018global regular expression print\u2019. It searches files for lines that match a pattern and returns the results. It is also case sensitive. Here, \ngrep\n searches for \u2018Mount\u2019 in \nmountains.txt\n. Type:\n\n\n\n\n1\n$ grep -i Mount mountains.txt\n\n\n\n\n\n\n\n\ngrep -i\n enables the command to be case insensitive. Here, \ngrep\n searches for capital or lowercase strings that match \u2018Mount\u2019 in \nmountains.txt\n. The above commands are a great way to get started with \ngrep\n. If you are familiar with regular expressions, you can use regular expressions to search for patterns in files. \ngrep\n can also be used to search within a directory. Type :\n\n\n\n\n1\n$ grep -R Arctic /home/ccuser/workspace/geography\n\n\n\n\n\n\n\n\nype :\n\n\n\n\n1\n$ grep -R Arctic /home/ccuser/workspace/geography\n\n\n\n\n\n\n\n\ngrep -R\n searches files in a directory and outputs filenames and lines containing matched results. \n-R\n stands for \u2018recursive\u2019. Here \ngrep -R\n searches the \n/home/ccuser/workspace/geography\n directory for the string \u2018Arctic\u2019 and outputs filenames and lines with matched results. Type:\n\n\n\n\n1\n$ grep -R Gambino .\n\n\n\n\n\n\n\n\ngrep -R\n searches ALL files recursively and outputs filenames and lines containing matched results.\n\n\n\n\n1\n$ grep -Rl Arctic /home/ccuser/workspace/geography\n\n\n\n\n\n\n\n\ngrep -Rl\n searches all files in a directory and outputs only filenames with matched results. \n-R\n stands for \u2018recursive\u2019 and \nl\n stands for \u2018files with matches\u2019. Here \ngrep -Rl\n searches the \n/home/ccuser/workspace/geography\n directory for the string \u2018Arctic\u2019 and outputs filenames with matched results. Use \ncat\n to display the contents of \nforests.txt\n. \n\n\ngrep this file.txt\n; search for \nthis\n in \nfile.txt,\n print the output on screen.\n\n\ngrep this < file.txt\n; feed \nfile.txt\n to process \ngrep\n to look for \nthis\n, print on screen\n\n\ngrep this file.txt > file_this.txt\n; write, overwrite the output in \nfile_this.txt\n.\n\n\ngrep this file.txt >> file_this.txt\n; write, append the output in \nfile_this.txt\n.\n\n\ngrep \"is\" file.txt\n; search pattern \nis\n, precisely, in words or alone.\n\n\ngrep line file.txt\n; search pattern \nline\n.\n\n\ngrep -n line file.txt\n; show the line number where it finds pattern \nline\n.\n\n\ngrep -i line file.txt\n; case insensitive\n\n\n\n\ngrep -v line file.txt\n; inverse or when it does not have the pattern line\n\n\n\n\n\n\nIf you search for one \u201cfile\u201d in the current directory, type:\n\n\n\n\n\n\n1\nfind . _name \n\"file.txt\"\n\n\n\n\n\n\n\n\n\nfind / _name \"file.txt\"\n searches \u201cfile\u201d in multiple directories.\n\n\nfind dirA dirB dirC _name \"file.txt\"\n search \u201cfile\u201d in one or more directories. Type :\n\n\n\n\n1\n$ sed \n's/snow/rain/'\n forests.txt\n\n\n\n\n\n\n\n\nsed\n stands for \u2018stream editor\u2019. It accepts standard input and modifies it based on an expression, before displaying it as output data. It is similar to \u2018find and replace\u2019. \n\n\nLet\u2019s look at the expression \n's/snow/rain/'\n:\n\n\ns\n: stands for \u2018substitution\u2019. it is always used when using sed for substitution.\n\n\nsnow\n: the search string, the text to find.\n\n\nrain\n: the replacement string, the text to add in place.\n\n\n\n\n\n\nIn this case, \nsed\n searches \nforests.txt\n for the word \n'snow'\n and replaces it with \n'rain'\n. Importantly, the above command will only replace the FIRST instance of \n'snow'\n on a line.\n\n\n\n\n1\n$ sed \n's/snow/rain/g'\n forests.txt\n\n\n\n\n\n\n\n\n\n\nThe above command uses the \ng\n expression, meaning \u2018global\u2019. Here \nsed\n searches \nforests.txt\n for the word \n'snow'\n and replaces it with \n'rain'\n, globally. ALL instances of \n'snow'\n on a line will be turned to \n'rain'\n.\n\n\n\n\n\n\nLet\u2019s summarize what we\u2019ve done so far.\n\n\n\n\nThe common redirection commands are:\n\n\n>\n; redirects standard output of a command to a file, overwriting previous content.\n\n\n>>\n; redirects standard output of a command to a file, appending new content to old content.\n\n\n<\n; redirects standard input to a command.\n\n\n|\n; redirects standard output of a command to another command.\n\n\n\n\n\n\nA number of other commands are powerful when combined with redirection commands:\n\n\necho 'a'\n; display \u2018a\u2019 in the terminal.\n\n\ntouch\n; create a file.\n\n\ncat\n; display the content of a file in the terminal.\n\n\nless\n; edit the content of a file in the terminal; prefer \nnano\n, it\u2019s more user-friendly.\n\n\nsort\n; sorts lines in a file in alphabetical order.\n\n\nuniq\n; filters duplicates in a file.\n\n\ngrep\n; searches for a text pattern in files and outputs it.\n\n\nfind\n; searches for files, not the content.\n\n\nsed\n; searches for a text pattern in files, modifies it, and outputs it.\n\n\nhead\n/\ntail\n; \nls\n, but only the top/bottom results.\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\necho 'thisthat'\n; show \nthisthat\n.\n\n\necho $USER\n; show user variable.\n\n\necho \\*\n, \nd\\*\n, \ns\\*\n, \n[[:upper]]\\*\n, \n/usr/\\*/share\n; show the content of the current directory according to the specified string (wildcards and characters).\n\n\necho ~\n; show \n/home/user\n.\n\n\necho .\\*\n; show hidden files.\n\n\necho 2 + 2\n; show 2 + 2.\n\n\necho $((2 + 2))\n; show 4.\n\n\necho $(($((5 \\*\\* 2)) \\* 3))\n; show 75.\n\n\necho Front-{A,B,C}-Back\n; show Front-A-Back Front-B-Back Front-C-Back.\n\n\necho Number-{1...5}\n; show Number-1 Number-2 Number-3 Number-4 Number-5.\n\n\necho {Z...A}\n; show Z Y X W\u2026\n\n\necho a{A{1,2}, B{3,4}}b\n; show aS1b aA2b aB3b aB4b.\n\n\nmkdir {2007...2009}-0{1...9} {2007...2009}-{10...12}\n; show 2007-01 2007-02 2007-03 2007-04 2007-05 2007-06 2007-07 2007-08 2007-09 2007-10 2007-11 2007-12 2008-01\u2026\n\n\necho $(ls)\n; show, not a list, but a paragraph of files and directories.\n\n\necho $USER $((2 + 2))\n; show user 4.\n\n\necho $(cal) or echo ' $(cal) '\n; show a calendar.\n\n\nWith \n.\n, special characters become ordinary characters except for \n$\n, \n\\\n or \n'\n; and with \n.\n, it doesn\u2019t suppress commands, variables and aliases as with \n.\n. Try :\n\n\necho text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))\n .\n\n\necho 'text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))'\n .\n\n\necho 'text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))'\n.\n\n\nprint env | less\n; show a list of available variables.\n\n\ncat test.txt\n; showthe content of file \ntext.txt\n.\n\n\nls > file.txt\n; send command and results into a file, if the file exists, it overwrite it, use \ncat file.txt\n to edit it.\n\n\nls >> file.txt\n; send command and results into a file, if the file exists, it appends the new content to the existing one, use \ncat file.txt\n to edit it.\n\n\ncat test.txt > bbb.txt\n; write (overwrite) the (existing) content of \ntest.txt\n into \nbbb.txt\n.\n\n\ncat test.txt >> bbb.txt\n; write the content of \ntest.txt\n into \nbbb.txt\n or append it to \nbbb.txt\n.\n\n\nwc test.txt\n; count lines, words and bytes of \ntest.txt\n.\n\n\ncat test.txt | wc\n; show the wc of \ntest.txt\n.\n\n\ncat test.txt | wc > bbb.txt\n; write the \nwc\n of \ntest.txt\n into \nbbb.txt\n.\n\n\nsort < file.txt\n; push the content into command sort.\n\n\ncat < aaa.txt\n; open file \naaa.txt\n to edit.\n\n\nsort file.txt\n; sort the content.\n\n\ncat aaa.txt | sort > sorted_aaa.txt\n; in addition, send the results in a new files.\n\n\nuniq file.txt\n; extract unique values of the content.\n\n\nls -l | less\n; the list goes into the reader.\n\n\nls -l | head\n; the list shows the 10 top lines only.\n\n\nls -l | tail\n; the list shows the 10 bottom lines only.\n\n\n\n\nEnvironment\n\u00b6\n\n\n\n\nEach time we launch the terminal application, it creates a new session. The session immediately loads settings and preferences that make up the command line environment.\n\n\n\n\nWe can configure this environment to support the commands and programs, customize greetings and command aliases, and create variables to share across commands and programs.\n\n\n\n\n\n\nA simple, command line text editor: \nnano\n. It is more powerful than \nless\n. Type :\n\n\n\n\n\n\n1\n$ nano hello.txt\n\n\n\n\n\n\n\n\nIn \nnano\n, at the top of the window, type :\n\n\n\n\n1\n$ \n'Hello, I am nano.'\n\n\n\n\n\n\n\n\n\nUsing the menu at the bottom of the terminal for reference, type \nCtrl\n+\nO\n (the letter, not the number) to save the file. Press \nEnter\n, when prompted about the filename to write. Then type \nCtrl\n+\nX\n to exit \nnano\n. Finally, type \nclear\n to clear the terminal window. The command prompt should now be at the top of the window. You just edited a file in the \nnano\n text editor. Type:\n\n\n\n\n1\n$ nano hello.txt\n\n\n\n\n\n\n\n\nnano\n is a command line text editor. It works just like a desktop text editor, except that it is accessible from the command line and only accepts keyboard input. The command \nnano hello.txt\n opens a new text file named \nhello.txt\n in the \nnano\n text editor. \u2018Hello, I am nano\u2019 is a text string entered in \nnano\n through the cursor.\n\n\nThe menu of keyboard commands at the bottom of the window allow us to save changes to \nhello.txt\n and exit \nnano\n. The \n^\n stands for the \nCtrl\n key.\n\n\nCtrl\n+\nO\n saves a file. \no\n stands for output.\n\n\nCtrl\n+\nX\n exits the \nnano\n program. \nx\n stands for exit.\n\n\nCtrl\n+\nG\n opens a help menu.\n\n\nClear\n clears the terminal window, moving the command prompt to the top of the screen.\n\n\n\n\n\n\nnano editor\n\n\nNow that you are familiar with editing text in \nnano\n, let\u2019s create a file to store environment settings. Type :\n\n\n\n\n1\n$ nano ~/.bash_profile\n\n\n\n\n\n\n\n\nThis opens up a new file in \nnano\n. In \n~/.bash_profile\n, at the top of the file, type :\n\n\n\n\n1\n$ \necho\n \n'Welcome, Jane Doe'\n\n\n\n\n\n\n\n\n\nYou can use your name in place of \u2018Jane Doe\u2019. Type \nCtrl\n+\nO\n to save the file. Press \nEnter\n to write the filename. Type \nCtrl\n+\nX\n to exit. Finally, type \nclear\n to clear the terminal window. Type :\n\n\n\n\n1\n$ \nsource\n ~/.bash_profile\n\n\n\n\n\n\n\n\nYou should see the greeting you entered. You created a file in \nnano\n called \n~/.bash_profile\n and added a greeting.\n\n\n\n\n1\n$ nano ~/.bash_profile\n\n\n\n\n\n\n\n\n~/.bash_profile\n is the name of file used to store environment settings. It is commonly called the \u2018bash profile\u2019. When a session starts, it will load the contents of the bash profile before executing commands.\n\n\nThe \n~\n represents the user\u2019s home directory.\n\n\nThe \n.\n indicates a hidden file.\n\n\n\n\n\n\nThe name \n~/.bash_profile\n is important, since this is how the command line recognizes the bash profile.\n\n\nThe command \nnano ~/.bash_profile\n opens up \n~/.bash_profile\n in \nnano\n. The text echoes \u2018Welcome, Jane Doe\u2019 and creates a greeting in the bash profile, which is saved. It tells the command line to echo the string \u2018Welcome, Jane Doe\u2019 when a terminal session begins. The command source \n~/.bash_profile\n ACTIVATES the changes in \n~/.bash_profile\n for the current session.\n\n\nNow that we know what bash profile is, let\u2019s continue configuring the environment by adding command aliases. Open \n~/.bash_profile\n in \nnano\n. In \n~/.bash_profile\n, beneath the greeting you created, type :\n\n\n\n\n1\n$ \nalias\n \npd\n=\n'pwd'\n\n\n\n\n\n\n\n\n\nSave the file. Press \nEnter\n to write the filename. Exit \nnano\n. Clear the terminal window.\n\n\nIn the command line, use the source command to activate the changes in the current session.\n\n\n\n\n1\n$ \nsource\n ~/.bash_profile\n\n\n\n\n\n\n\n\nLet\u2019s try out the alias. Type :\n\n\n\n\n1\n$ pd\n\n\n\n\n\n\n\n\nYou should see the same output as you would by typing the \npwd\n command. What happens when you store this alias in \n~/.bash_profile\n?\n\n\n\n\n1\n$ \nalias\n \npd\n=\n'pwd'\n\n\n\n\n\n\n\n\n\nThe alias command allows you to create keyboard shortcuts, or aliases, for commonly used commands. Here alias \npd='pwd'\n creates the alias \npd\n for the \npwd\n command, which is then saved in the bash profile. Each time you enter \npd\n, the output will be the same as the pwd command. The command source \n~/.bash_profile\n makes the alias \npd\n available in the current session. Each time we open up the terminal, we can use the \npd\n alias.\n\n\nLet\u2019s practice aliases some more. Open \n~/.bash_profile\n in \nnano\n. In the bash profile, beneath the previous alias, add :\n\n\n\n\n1\n$ \nalias\n \nhy\n=\n'history'\n\n\n\n\n\n\n\n\n\nSave the file. Press \nEnter\n to write the filename.\n\n\nAdd another alias:\n\n\n\n\n1\n$ \nalias\n \nll\n=\n'ls -la'\n\n\n\n\n\n\n\n\n\nSave the file.\n\n\nPress \nEnter\n to write the filename.\n\n\nExit \nnano\n.\n\n\nClear the terminal window.\n\n\nIn the command line, use source to activate the changes to the bash profile for the current session.\n\n\nLet\u2019s try out the aliases. Type:\n\n\n\n\n1\n$ hy\n\n\n\n\n\n\n\n\nWhat happens when you store the following aliases in \n~/.bash_profile\n?\n\n\n\n\n1\n$ \nalias\n \nhy\n=\n'history'\n\n\n\n\n\n\n\n\n\nhy\n is set as alias for the history command in the bash profile. The alias is then made available in the current session through source. By typing \nhy\n, the command line outputs a history of commands that were entered in the current session. Type:\n\n\n\n\n1\n$ \nalias\n \nll\n=\n'ls -la'\n\n\n\n\n\n\n\n\n\nll\n is set as an alias for \nls -la\n and made available in the current session through source. By typing \nll\n, the command line now outputs all contents and directories in long format, including all hidden files.\n\n\nNow that you are familiar with configuring greetings and aliases, let\u2019s move on to setting environment variables.\n\n\nOpen \n~/.bash_profile\n in \nnano\n.\n\n\nIn the bash profile, beneath the aliases, on a new line, type:\n\n\n\n\n1\n$ \nexport\n \nUSER\n=\n'Jane Doe'\n\n\n\n\n\n\n\n\n\nFeel free to use your own name. Save the file. Press \nEnter\n to write the filename. Exit \nnano\n. Finally, clear the terminal.\n\n\nIn the command line, use source to activate the changes in the bash profile for the current session. Type :\n\n\n\n\n1\n$ \necho\n \n$USER\n\n\n\n\n\n\n\n\n\nThis should return the value of the variable that you set. What happens when you store this in \n~/.bash_profile\n?\n\n\n\n\n1\n$ \nexport\n \nUSER\n=\n'Jane Doe'\n\n\n\n\n\n\n\n\n\nEnvironment variables are variables that can be used across commands and programs and hold information about the environment. The line \nUSER='Jane Doe'\n sets the environment variable \nUSER\n to a name \n'Jane Doe'\n. Usually the \nUSER\n variable is set to the name of the computer\u2019s owner. The line export makes the variable to be available to all child sessions initiated from the session you are in. This is a way to make the variable persist across programs. At the command line, the command echo \n$USER\n returns the value of the variable. Note that \n$\n is always used when returning a variable\u2019s value. Here, the command echo \n$USER\n returns the name set for the variable.\n\n\nLet\u2019s learn a few more environment variables, starting with the variable for the command prompt. Open \n~/.bash_profile\n in \nnano\n. On a new line, beneath the last entry, type\n\n\n\n\n1\n$ \nexport\n \nPS1\n=\n'>> '\n\n\n\n\n\n\n\n\n\nSave the file. Press \nEnter\n to write the filename. Exit \nnano\n. Finally, clear the terminal window. In the command line, use source to activate the changes in the bash profile for the current shell session. Let\u2019s try out the new command prompt. Type :\n\n\n\n\n1\n$ \necho\n \n'hello'\n\n\n\n\n\n\n\n\n\nType :\n\n\n\n\n1\n$ ls -alt\n\n\n\n\n\n\n\n\nDid you notice that the prompt has changed? What happens when this is stored in \n~/.bash_profile\n?\n\n\n\n\n1\n$ \nexport\n \nPS1\n=\n'>> '\n\n\n\n\n\n\n\n\n\nPS1\n is a variable that defines the makeup and style of the command prompt.\n\n\necho $PS1\n prints the variable.\n\n\nPS1=\"value\"\n changes the variable value.\n\n\nPS1='>> '\n sets the command prompt variable and exports the variable. It makes the variable available in all sub programs of the current shell. Here we change the default command prompt from \n$\n to \n>>\n. After using the source command, the command line displays the new command prompt. Let\u2019s learn about two more environment variables. Type :\n\n\n\n\n1\n$ \necho\n \n$HOME\n\n\n\n\n\n\n\n\n\nThis returns the value of the \nHOME\n variable. What happens when you type this command?\n\n\n\n\n1\n$ \necho\n \n$HOME\n\n\n\n\n\n\n\n\n\nThe \nHOME\n variable is an environment variable that displays the path of the home directory. Here by typing echo \n$HOME\n, the terminal displays the path \n/home/ccuser\n as output. \ncd $HOME\n goes to the home directory. You can customize the \nHOME\n variable if needed, but in most cases this is not necessary. In the command line, type :\n\n\n\n\n1\n$ \necho\n \n$PATH\n\n\n\n\n\n\n\n\n\nType :\n\n\n\n\n1\n/bin/pwd\n\n\n\n\n\n\n\n\nType :\n\n\n\n\n1\n/bin/ls\n\n\n\n\n\n\n\n\nWhat happens when you type this command?\n\n\n\n\n1\n2\n$ \necho\n \n$PATH\n\n/home/ccuser/.gem/ruby/2.0.0/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/sbin:/sbin:/bin\n\n\n\n\n\n\n\n\nPATH\n is an environment variable that stores a list of directories separated by a colon. Looking carefully, echo \n$PATH\n lists the following directories:\n\n\n/home/ccuser/.gem/ruby/2.0.0/bin\n.\n\n\n/usr/local/sbin\n.\n\n\n/usr/local/bin\n.\n\n\n/usr/bin\n.\n\n\n/usr/sbin\n.\n\n\n/sbin\n.\n\n\n/bin\n.\n\n\n\n\n\n\nEach directory contains scripts for the command line to execute. The \nPATH\n variable simply lists which directories contain scripts. For example, many COMMANDS we\u2019ve learned are scripts stored in the \n/bin\n directory.\n\n\n\n\n1\n$ /bin/pwd\n\n\n\n\n\n\n\n\nThis is the script that is executed when you type the \npwd\n command.\n\n\n\n\n1\n$ /bin/ls\n\n\n\n\n\n\n\n\nThis is the script that is executed when you type the ls command. In advanced cases, you can customize the \nPATH\n variable when adding scripts of your own. Type :\n\n\n\n\n1\n$ env\n\n\n\n\n\n\n\n\nType :\n\n\n\n\n1\n$ env \n|\n grep PATH\n\n\n\n\n\n\n\n\nWhat happens when you type this command?\n\n\n\n\n1\n$ env\n\n\n\n\n\n\n\n\nThe \nenv\n command stands for \u2018environment\u2019, and returns a list of the environment variables for the current user. Here, the \nenv\n command returns a number of variables, including \nPATH\n, \nPWD\n, \nPS1\n, and \nHOME\n.\n\n\n\n\n1\n2\n$ env \n|\n grep PATH\n$ env \n|\n grep aliasname\n\n\n\n\n\n\n\n\nenv | grep PATH\n is a command that displays the value of a single environment variable. Here the standard output of env is \u2018piped\u2019 to the \ngrep\n command. \ngrep\n searches for the value of the variable \nPATH\n and outputs it to the terminal. You learned to use the bash profile to configure the environment. What can we generalize so far? The environment refers to the preferences and settings of the current user.\n\n\n\n\n\n\n\n\nLet\u2019s summarize what we\u2019ve done so far.\n\n\nThe \nnano\n editor is a command line text editor used to configure the environment.\n\n\n~/.bash_profile\n is where environment settings are stored. You can edit this file with \nnano\n.\n\n\nEnvironment variables are variables that can be used across commands and programs and hold information about the environment.\n\n\nexport VARIABLE='Value'\n sets and exports an environment variable.\n\n\nUSER\n is the name of the current user.\n\n\nPS1\n is the command prompt.\n\n\nHOME\n is the home directory. It is usually not customized.\n\n\nPATH\n returns a colon separated list of file paths. It is customized in advanced cases.\n\n\necho $PATH\n prints the path.\n\n\nPATH=\"value\"\n changes the path..\n\n\nexport PATH=/home/dir/bin:$PATH\n appends the new path to environment variable PATH.\n\n\nenv\n returns a list of environment variables.\n\n\n\n\nMulti-Users\n\u00b6\n\n\n\n\nLinux is multi-user : multiple users at the same time as opposed to OS X or Windows; just like the UNIX mainframe computers with terminals, users and superuser concepts. \n\n\n\n\nAccess rights\n\n\n\n\nType :\n\n\n\n\n1\n$ ls -l\n\n\n\n\n\n\n\n\n\n\nRead, from left to right :\n\n\n\n\n\u2013\n or \nd\n: file or dir.\n\n\nFile or dir name.\n\n\nOwner access (\nr w x\u2026-\n).\n\n\nGroup access (\nr w x\u2026-\n).\n\n\nAll access (\nr w x\u2026-\n).\n\n\nOwner group, size, date.\n\n\n\n\n\n\n\n\nHow do we change the acces rights? With \nchmod\n.\n\n\n\n\nFirst, there are access right for:\n\n\nu\n ser.\n\n\ng\n roup.\n\n\no\n thers.\n\n\n\n\n\n\nSecond, there levels. Each level has a numeric value.:\n\n\nr\n ead: 4.\n\n\nw\n rite: 2.\n\n\nx\n ecute: 1.\n\nFor example, using levels:\n\n\no+w\n: others can write the file (create).\n\n\nu+x\n: users can execute the file.\n\n\ng-x\n: group can no longer execute the file.\n\n\netc.\n\n\nchmod o+w file1.txt\n for example.\n\n\n\n\n\n\n\n\nWhy numeric value? It an alternative way for \nchmod\n to assign access rights. Levels are ranked with values:\n\n\n\n\n\n\n\n\nLevel\n\n\nBinary\n\n\nDecimal\n\n\n\n\n\n\n\n\n\n\nr w x\n\n\n111\n\n\n7\n\n\n\n\n\n\nr w -\n\n\n110\n\n\n6\n\n\n\n\n\n\nr - -\n\n\n100\n\n\n4\n\n\n\n\n\n\n- - -\n\n\n000\n\n\n0\n\n\n\n\n\n\nr - x\n\n\n101\n\n\n5\n\n\n\n\n\n\n\n\n\n\nA \n7\n grants full rights vs a \n0\n that grants no rights. For example:\n\n\nchmod 600 file\n; change the file access rights to \nrw- --- ---\n.\n\n\nchmod 600 dir\n; change the directory access rights.\n\n\n\n\n\n\nHow are the values calculated?\n\n\n\n\n1\n2\n3\n4\n5\n6\nr+w+x \n=\n \n4\n+2+1 \n=\n \n7\n\n\nr\n     \n=\n \n4\n     \n=\n \n4\n\n    \nx\n \n=\n     \n1\n \n=\n \n1\n\nr+w   \n=\n \n4\n+2   \n=\n \n6\n\n...\n...\n\n\n\n\nOnce you have the values, you can set the access rights:\n\n\n1\n2\n3\n4\n u   g   o  \n--- --- ---\nrwx rw- --x\n \n7\n   \n6\n   \n1\n\n\n\n\n\n\n\n\n\nTherefore, \nchmod 761\n set the access rights \nrwx rw- --x\n to a file, files, a directory or directories.\n\n\n\n\nOwnership\n\n\n\n\nchown thou file\n; assign a new owner, \nthou\n, to \u2018file\u2019.\n\n\nchown thou dir\n; \u2026 to dir.\n\n\nchgrp newgr file\n assign a new group owner, \nnewgr\n, to \u2018file\u2019.\n\n\nchgrp newgr dir\n; \u2026 to dir.\n\n\n\n\nSuperuser\n\n\n\n\nsu\n; superuser login.\n\n\nsu file\n; unlock the file with the superuser password.\n\n\nsudo\n; do it with the privilege of a superuser.\n\n\nsudo apt-get update\n; updates the database.\n\n\nsudo apt-get upgrade\n; upgrades all packages (update before upgrading).\n\n\nsudo apt-get install build-essential\n; install a useful package (or any other package).\n\n\nsudo apt-get install git\n; install \u2018git\u2019.\n\n\nwhich git\n; find where \u2018git\u2019 is located (if it\u2019s instaled).\n\n\nsudo apt-get remove git\n; remove \u2018git\u2019.\n\n\nsudo apt-get purge git\n; remove \u2018git\u2019 and purge any remainings.\n\n\nsudo chnow user file\n; change the owner.\n\n\n\n\nMulti-Tasks\n\u00b6\n\n\n\n\nLinux is multi-task; multiple tasks can run at the same time like OS X or Windows or UNIX; Linux kernel runs processes; they take turns at the processor(s). All programs and processes can be launched/killed from the GUI and the CLI.\n\n\n\n\nShow\n\n\n\n\ntop\n; show process dashboard by PID number; \n?\n for help, \nq\n for quit.\n\n\nps\n; show the process list.\n\n\nps aux\n; show all process.\n\n\nps aux | grep 'top'\n; filter top processes.\n\n\nps aux | grep bash\n; filter processes related to the bash.\n\n\nps aux | grep bash | sort\n; \u2026 and sort them.\n\n\n\n\n\n\n\n\nManage\n\n\n\n\nCtrl\n+\nZ\n; pause a process, put it in the background.\n\n\nfg\n; foreground, bring back the process.\n\n\njobs\n; list paused processes.\n\n\nfg #PID\n; bring back process #PID (if there is more than one process on pause, you must identify the process).\n\n\nCtrl\n+\nC\n; terminate the active process.\n\n\nxload\n; display the system load in a new windows, but jam the current terminal (open another or several terminals then).\n\n\nxload &\n; runs in the background\n\n\nCtrl\n+\nX\n`; suspend the process and unjam the terminal.\n\n\nby\n; resume the process.\n\n\nps\n; show processes and their #PID.\n\n\nps x | grep bad_program\n; find the bad processes.\n\n\nkill\n can send a signal from the OS to a process. For example : when you log off, you send a signal to terminate a word processing program and save the file before closing.\n\n\nkill #PID\n; kill the process.\n\n\nkill -STOP #PID\n; pause the process.\n\n\nkill -TERM #PID\n;  terminate the process.\n\n\nkill -SIGTERM #PID\n;  terminate the process.\n\n\nkill -SIGKILL #PID\n; kill the process.\n\n\nkill -KILL\n; force closing of the current process.\n\n\nkill -9 #PID\n;  force closing the process.\n\n\n\n\n\n\nKilling sequence:\n\n\nkill #PID\n; doesn\u2019t work\u2026\n\n\nkill -9 #PID\n, or..",
            "title": "Codecademy, Learn the Command Line Notes"
        },
        {
            "location": "/Codecademy Learn the Command Line Notes/#manipulation",
            "text": "List file & dir   ls ; list of directories and files.  ls /bin/bash ; list of a remote directory and files.  ls -a ; list all.  ls -t ; list in alphanumeric order, when they were last modified.  ls -l ; list in long format.  ls -la ; list all in long format.  ls -alt ; list all in long format and ordered.  ls /aaa /bbb ; list both directories.  ls -r ; list in reverse.  ls ../paint/ ; list an upper directory.   Copy file & dir   cp aaa.txt bbb.txt ; copy  aaa.txt  file, paste it or create a copy named  bbb.txt .  cp aaa.txt dir/ ; copy  aaa.txt  into directory  dir .  cp aaa.txt bbb.txt dir/ ; copy  aaa.txt  and  bbb.txt into directory  dir .  cp \\*.txt dir/ ; copy all  .txt  files into directory  dir .  cp aaa.txt dir/dir2 ; copy  aaa.txt  into sub-directory  dir2 .  cp -r aaa bbb ; copy  dir1 , create a copy names  dir2  with the exact same content.  cp -i ; interactive, to prompt the user.  cp f\\* ../paint/ ; copy all files beginning with  f  to an upper directory.   Wildcards   \\* ; wildcard for any string.  ? ; wildcard for any character.  \\*.txt ; all files finishing with  .txt .  r\\* ; all files beginning with  r .  ??a.txt ; all files beginning with two characters +  a.txt .  backup[[:digit:]] ; all file beginning with  backup  + any digit.  [abc]\\* ; all files beginning with either  a ,  b  or  c  [[:upper:]]\\* ; all files beginning with an upper case.  \\*[![:lower:]] ; all file not finishing with a lower case.   Move file & dir   mv aaa.txt bbb.txt ; move file or cut  aaa.txt  and paste  bbb.txt .  mv aaa.txt dir/ ; move file  aaa.txt  into directory  dir .  mv aaa.txt bbb.txt dir/ ; move files  aaa.txt  and  bbb.txt  into directory  dir .  mv \\*.txt dir/ ; move all  .txt  files into directory  dir .  mv aaa.txt dir/dir2 ; move file  aaa.txt  into sub-directory  dir2 .  mv -i ; interactive, prompt the user.   Remove file & dir   rm aaa.txt ; remove file  aaa.txt .  rm test1.txt test2.txt ; remove both files.  rm aaa/\\* ; remove all files in the directory  aaa .  rm -r aaa ; remove directory  aaa , must be empty.  rm -rf aaa ; remove directory  aaa  and its files.  rm -i ; interactive, prompt the user.",
            "title": "Manipulation"
        },
        {
            "location": "/Codecademy Learn the Command Line Notes/#redirection",
            "text": "Let\u2019s begin by taking a closer look at input and output. In the terminal, after the shell prompt, type :   1 $  echo   'Hello'     The  echo  command accepts the string \u2018Hello\u2019 as standard input, and echoes the string  'Hello'  back to the terminal as standard output.   standard input, abbreviated as  stdin , is information inputted into the terminal through the keyboard or input device.  standard output, abbreviated as  stdout , is the information outputted after a process is run.  standard error, abbreviated as  stderr , is an error message outputted by a failed process.    Redirection ( > ) reroutes standard input, standard output, and standard error to or from a different location. Type :   1 $  echo   'Hello'  > hello.txt    Type :   1 $ cat hello.txt    The  >  command redirects the standard output to a file. The standard output  'Hello'  is redirected by  >  to the file  hello.txt , and entered as the standard input.   The cat command outputs the contents of a file to the terminal. When you type  cat hello.txt , the contents of  hello.txt  are displayed.    Type :    1 $ ls -l    This is the filesystem we\u2019ll work with. Create a file. Type :   1 $ touch ocean.txt    Fill the file with values (ocean names). Type :   1 $ cat oceans.txt > continents.txt    Use  cat  to view the contents of  continents.txt . Notice that we only see oceans as output:   1 $ cat continents.txt    >  takes the standard output of the command on the left, and redirects it to the file on the right. Here the standard output of cat  oceans.txt  is redirected to  continents.txt . Note that  >  OVERWRITES all original content in  continents.txt . When you view the output data by typing cat on  continents.txt , you will see only the contents of  oceans.txt . Type :   1 $ cat glaciers.txt >> rivers.txt    Use  cat  to view the contents of  rivers.txt :   1 $ cat rivers.txt    Notice that we see both rivers and glaciers as output. Type:   1 $ cat glaciers.txt >> rivers.txt     >>  takes the standard output of the command on the left and APPENDS it to the file on the right. You can view the output data of the file with cat and the filename. Here, the output data of  rivers.txt  will contain the original contents of  rivers.txt  with the content of  glaciers.txt  appended to it.     Type :    1 $ touch lakes.txt    Fill the files with values (lake names). Type :   1 $ cat < lakes.txt    <  takes the standard input from the file on the right and inputs it into the program on the left. Here,  lakes.txt  is the standard input for the  cat  command. The standard output appears in the terminal. Let\u2019s try some more redirection commands. Type :   1 $ touch volcanoes.txt    Fill the files with values (volcano names). Type :   1 $ cat volcanoes.txt  |  wc    You get the count for: lines, words, bytes. Type :   1 $ cat volcanoes.txt  |  wc  |  cat > islands.txt    Use  cat  to output the contents in  islands.txt . The next command should now equals  $ cat volcanoes.txt  |  wc .   1 $ cat volcanoes.txt  |  wc    |  is a \u2018pipe\u2019 or \u2018pipeline\u2019 The  |  takes the standard output of the command on the left, and PIPES it as standard input to the command on the right. You can think of this as \u2018command to command\u2019 redirection. Here, again, the output of cat  volcanoes.txt  is the standard input of  wc . In turn, the  wc  command outputs the number of lines, words, and characters in  volcanoes.txt , respectively:   1\n2 $ cat volcanoes.txt  |  wc  |  cat > islands.txt\n$ less islands.txt    Multiple  | s can be chained together. Here the standard output of cat  volcanoes.txt  is \u2018piped\u2019 to the  wc  command. The standard output of  wc  is then \u2018piped\u2019 to cat. Finally, the standard output of cat is redirected to  islands.txt . You can view the output data of this chain by typing cat  islands.txt .   A few commands are particularly powerful when combined with redirection. Let\u2019s try them out. First, use  cat  to output the contents of  lakes.txt . Then,  sort  it. Type :   1\n2 $ cat lakes.txt\n$ sort lakes.txt    The lakes in  lakes.txt  are listed in alphabetical order. Type:   1 $ cat lakes.txt  |  sort    sort  takes the standard input and orders it alphabetically for the standard output. The lakes in  lakes.txt  are listed in alphabetical order.  Use cat to output the contents of  sorted-lakes.txt .   1 $ cat lakes.txt  |  sort > sorted-lakes.txt    The command takes the standard output from cat  lakes.txt  and \u2018pipes\u2019 it to sort in ascending order. The standard output of  sort  is redirected to  sorted-lakes.txt . You can view the output data by typing:   1 $ cat sorted-lakes.txt    Type :   1 $ touch deserts.txt    Fill the file with values. Type :   1\n2 $ cat deserts.txt\n$ uniq deserts.txt     You get to see all entries and unique entries.    Type :    1 $ sort deserts.txt  |  uniq    You get to see  unique entries sorted. Type :   1 $ sort deserts.txt  |  uniq > uniq-deserts.txt    You save the result in  uniq-deserts.txt .  Use  cat  to output the contents of  uniq-deserts.txt .  uniq  stands for \u2018unique\u2019 and filters out ADJACENT, duplicate lines in a file. Here  uniq deserts.txt  filters out duplicates of \u2018Sahara Desert\u2019, because the duplicate of \u2018Sahara Desert\u2019 directly follows the previous instance. The \u2018Kalahari Desert\u2019 duplicates are not adjacent, and thus remain.  Type :   1 $ grep Mount mountains.txt    grep  stands for \u2018global regular expression print\u2019. It searches files for lines that match a pattern and returns the results. It is also case sensitive. Here,  grep  searches for \u2018Mount\u2019 in  mountains.txt . Type:   1 $ grep -i Mount mountains.txt    grep -i  enables the command to be case insensitive. Here,  grep  searches for capital or lowercase strings that match \u2018Mount\u2019 in  mountains.txt . The above commands are a great way to get started with  grep . If you are familiar with regular expressions, you can use regular expressions to search for patterns in files.  grep  can also be used to search within a directory. Type :   1 $ grep -R Arctic /home/ccuser/workspace/geography    ype :   1 $ grep -R Arctic /home/ccuser/workspace/geography    grep -R  searches files in a directory and outputs filenames and lines containing matched results.  -R  stands for \u2018recursive\u2019. Here  grep -R  searches the  /home/ccuser/workspace/geography  directory for the string \u2018Arctic\u2019 and outputs filenames and lines with matched results. Type:   1 $ grep -R Gambino .    grep -R  searches ALL files recursively and outputs filenames and lines containing matched results.   1 $ grep -Rl Arctic /home/ccuser/workspace/geography    grep -Rl  searches all files in a directory and outputs only filenames with matched results.  -R  stands for \u2018recursive\u2019 and  l  stands for \u2018files with matches\u2019. Here  grep -Rl  searches the  /home/ccuser/workspace/geography  directory for the string \u2018Arctic\u2019 and outputs filenames with matched results. Use  cat  to display the contents of  forests.txt .   grep this file.txt ; search for  this  in  file.txt,  print the output on screen.  grep this < file.txt ; feed  file.txt  to process  grep  to look for  this , print on screen  grep this file.txt > file_this.txt ; write, overwrite the output in  file_this.txt .  grep this file.txt >> file_this.txt ; write, append the output in  file_this.txt .  grep \"is\" file.txt ; search pattern  is , precisely, in words or alone.  grep line file.txt ; search pattern  line .  grep -n line file.txt ; show the line number where it finds pattern  line .  grep -i line file.txt ; case insensitive   grep -v line file.txt ; inverse or when it does not have the pattern line    If you search for one \u201cfile\u201d in the current directory, type:    1 find . _name  \"file.txt\"     find / _name \"file.txt\"  searches \u201cfile\u201d in multiple directories.  find dirA dirB dirC _name \"file.txt\"  search \u201cfile\u201d in one or more directories. Type :   1 $ sed  's/snow/rain/'  forests.txt    sed  stands for \u2018stream editor\u2019. It accepts standard input and modifies it based on an expression, before displaying it as output data. It is similar to \u2018find and replace\u2019.   Let\u2019s look at the expression  's/snow/rain/' :  s : stands for \u2018substitution\u2019. it is always used when using sed for substitution.  snow : the search string, the text to find.  rain : the replacement string, the text to add in place.    In this case,  sed  searches  forests.txt  for the word  'snow'  and replaces it with  'rain' . Importantly, the above command will only replace the FIRST instance of  'snow'  on a line.   1 $ sed  's/snow/rain/g'  forests.txt     The above command uses the  g  expression, meaning \u2018global\u2019. Here  sed  searches  forests.txt  for the word  'snow'  and replaces it with  'rain' , globally. ALL instances of  'snow'  on a line will be turned to  'rain' .    Let\u2019s summarize what we\u2019ve done so far.   The common redirection commands are:  > ; redirects standard output of a command to a file, overwriting previous content.  >> ; redirects standard output of a command to a file, appending new content to old content.  < ; redirects standard input to a command.  | ; redirects standard output of a command to another command.    A number of other commands are powerful when combined with redirection commands:  echo 'a' ; display \u2018a\u2019 in the terminal.  touch ; create a file.  cat ; display the content of a file in the terminal.  less ; edit the content of a file in the terminal; prefer  nano , it\u2019s more user-friendly.  sort ; sorts lines in a file in alphabetical order.  uniq ; filters duplicates in a file.  grep ; searches for a text pattern in files and outputs it.  find ; searches for files, not the content.  sed ; searches for a text pattern in files, modifies it, and outputs it.  head / tail ;  ls , but only the top/bottom results.     Examples   echo 'thisthat' ; show  thisthat .  echo $USER ; show user variable.  echo \\* ,  d\\* ,  s\\* ,  [[:upper]]\\* ,  /usr/\\*/share ; show the content of the current directory according to the specified string (wildcards and characters).  echo ~ ; show  /home/user .  echo .\\* ; show hidden files.  echo 2 + 2 ; show 2 + 2.  echo $((2 + 2)) ; show 4.  echo $(($((5 \\*\\* 2)) \\* 3)) ; show 75.  echo Front-{A,B,C}-Back ; show Front-A-Back Front-B-Back Front-C-Back.  echo Number-{1...5} ; show Number-1 Number-2 Number-3 Number-4 Number-5.  echo {Z...A} ; show Z Y X W\u2026  echo a{A{1,2}, B{3,4}}b ; show aS1b aA2b aB3b aB4b.  mkdir {2007...2009}-0{1...9} {2007...2009}-{10...12} ; show 2007-01 2007-02 2007-03 2007-04 2007-05 2007-06 2007-07 2007-08 2007-09 2007-10 2007-11 2007-12 2008-01\u2026  echo $(ls) ; show, not a list, but a paragraph of files and directories.  echo $USER $((2 + 2)) ; show user 4.  echo $(cal) or echo ' $(cal) ' ; show a calendar.  With  . , special characters become ordinary characters except for  $ ,  \\  or  ' ; and with  . , it doesn\u2019t suppress commands, variables and aliases as with  . . Try :  echo text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))  .  echo 'text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))'  .  echo 'text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))' .  print env | less ; show a list of available variables.  cat test.txt ; showthe content of file  text.txt .  ls > file.txt ; send command and results into a file, if the file exists, it overwrite it, use  cat file.txt  to edit it.  ls >> file.txt ; send command and results into a file, if the file exists, it appends the new content to the existing one, use  cat file.txt  to edit it.  cat test.txt > bbb.txt ; write (overwrite) the (existing) content of  test.txt  into  bbb.txt .  cat test.txt >> bbb.txt ; write the content of  test.txt  into  bbb.txt  or append it to  bbb.txt .  wc test.txt ; count lines, words and bytes of  test.txt .  cat test.txt | wc ; show the wc of  test.txt .  cat test.txt | wc > bbb.txt ; write the  wc  of  test.txt  into  bbb.txt .  sort < file.txt ; push the content into command sort.  cat < aaa.txt ; open file  aaa.txt  to edit.  sort file.txt ; sort the content.  cat aaa.txt | sort > sorted_aaa.txt ; in addition, send the results in a new files.  uniq file.txt ; extract unique values of the content.  ls -l | less ; the list goes into the reader.  ls -l | head ; the list shows the 10 top lines only.  ls -l | tail ; the list shows the 10 bottom lines only.",
            "title": "Redirection"
        },
        {
            "location": "/Codecademy Learn the Command Line Notes/#environment",
            "text": "Each time we launch the terminal application, it creates a new session. The session immediately loads settings and preferences that make up the command line environment.   We can configure this environment to support the commands and programs, customize greetings and command aliases, and create variables to share across commands and programs.    A simple, command line text editor:  nano . It is more powerful than  less . Type :    1 $ nano hello.txt    In  nano , at the top of the window, type :   1 $  'Hello, I am nano.'     Using the menu at the bottom of the terminal for reference, type  Ctrl + O  (the letter, not the number) to save the file. Press  Enter , when prompted about the filename to write. Then type  Ctrl + X  to exit  nano . Finally, type  clear  to clear the terminal window. The command prompt should now be at the top of the window. You just edited a file in the  nano  text editor. Type:   1 $ nano hello.txt    nano  is a command line text editor. It works just like a desktop text editor, except that it is accessible from the command line and only accepts keyboard input. The command  nano hello.txt  opens a new text file named  hello.txt  in the  nano  text editor. \u2018Hello, I am nano\u2019 is a text string entered in  nano  through the cursor.  The menu of keyboard commands at the bottom of the window allow us to save changes to  hello.txt  and exit  nano . The  ^  stands for the  Ctrl  key.  Ctrl + O  saves a file.  o  stands for output.  Ctrl + X  exits the  nano  program.  x  stands for exit.  Ctrl + G  opens a help menu.  Clear  clears the terminal window, moving the command prompt to the top of the screen.    nano editor  Now that you are familiar with editing text in  nano , let\u2019s create a file to store environment settings. Type :   1 $ nano ~/.bash_profile    This opens up a new file in  nano . In  ~/.bash_profile , at the top of the file, type :   1 $  echo   'Welcome, Jane Doe'     You can use your name in place of \u2018Jane Doe\u2019. Type  Ctrl + O  to save the file. Press  Enter  to write the filename. Type  Ctrl + X  to exit. Finally, type  clear  to clear the terminal window. Type :   1 $  source  ~/.bash_profile    You should see the greeting you entered. You created a file in  nano  called  ~/.bash_profile  and added a greeting.   1 $ nano ~/.bash_profile    ~/.bash_profile  is the name of file used to store environment settings. It is commonly called the \u2018bash profile\u2019. When a session starts, it will load the contents of the bash profile before executing commands.  The  ~  represents the user\u2019s home directory.  The  .  indicates a hidden file.    The name  ~/.bash_profile  is important, since this is how the command line recognizes the bash profile.  The command  nano ~/.bash_profile  opens up  ~/.bash_profile  in  nano . The text echoes \u2018Welcome, Jane Doe\u2019 and creates a greeting in the bash profile, which is saved. It tells the command line to echo the string \u2018Welcome, Jane Doe\u2019 when a terminal session begins. The command source  ~/.bash_profile  ACTIVATES the changes in  ~/.bash_profile  for the current session.  Now that we know what bash profile is, let\u2019s continue configuring the environment by adding command aliases. Open  ~/.bash_profile  in  nano . In  ~/.bash_profile , beneath the greeting you created, type :   1 $  alias   pd = 'pwd'     Save the file. Press  Enter  to write the filename. Exit  nano . Clear the terminal window.  In the command line, use the source command to activate the changes in the current session.   1 $  source  ~/.bash_profile    Let\u2019s try out the alias. Type :   1 $ pd    You should see the same output as you would by typing the  pwd  command. What happens when you store this alias in  ~/.bash_profile ?   1 $  alias   pd = 'pwd'     The alias command allows you to create keyboard shortcuts, or aliases, for commonly used commands. Here alias  pd='pwd'  creates the alias  pd  for the  pwd  command, which is then saved in the bash profile. Each time you enter  pd , the output will be the same as the pwd command. The command source  ~/.bash_profile  makes the alias  pd  available in the current session. Each time we open up the terminal, we can use the  pd  alias.  Let\u2019s practice aliases some more. Open  ~/.bash_profile  in  nano . In the bash profile, beneath the previous alias, add :   1 $  alias   hy = 'history'     Save the file. Press  Enter  to write the filename.  Add another alias:   1 $  alias   ll = 'ls -la'     Save the file.  Press  Enter  to write the filename.  Exit  nano .  Clear the terminal window.  In the command line, use source to activate the changes to the bash profile for the current session.  Let\u2019s try out the aliases. Type:   1 $ hy    What happens when you store the following aliases in  ~/.bash_profile ?   1 $  alias   hy = 'history'     hy  is set as alias for the history command in the bash profile. The alias is then made available in the current session through source. By typing  hy , the command line outputs a history of commands that were entered in the current session. Type:   1 $  alias   ll = 'ls -la'     ll  is set as an alias for  ls -la  and made available in the current session through source. By typing  ll , the command line now outputs all contents and directories in long format, including all hidden files.  Now that you are familiar with configuring greetings and aliases, let\u2019s move on to setting environment variables.  Open  ~/.bash_profile  in  nano .  In the bash profile, beneath the aliases, on a new line, type:   1 $  export   USER = 'Jane Doe'     Feel free to use your own name. Save the file. Press  Enter  to write the filename. Exit  nano . Finally, clear the terminal.  In the command line, use source to activate the changes in the bash profile for the current session. Type :   1 $  echo   $USER     This should return the value of the variable that you set. What happens when you store this in  ~/.bash_profile ?   1 $  export   USER = 'Jane Doe'     Environment variables are variables that can be used across commands and programs and hold information about the environment. The line  USER='Jane Doe'  sets the environment variable  USER  to a name  'Jane Doe' . Usually the  USER  variable is set to the name of the computer\u2019s owner. The line export makes the variable to be available to all child sessions initiated from the session you are in. This is a way to make the variable persist across programs. At the command line, the command echo  $USER  returns the value of the variable. Note that  $  is always used when returning a variable\u2019s value. Here, the command echo  $USER  returns the name set for the variable.  Let\u2019s learn a few more environment variables, starting with the variable for the command prompt. Open  ~/.bash_profile  in  nano . On a new line, beneath the last entry, type   1 $  export   PS1 = '>> '     Save the file. Press  Enter  to write the filename. Exit  nano . Finally, clear the terminal window. In the command line, use source to activate the changes in the bash profile for the current shell session. Let\u2019s try out the new command prompt. Type :   1 $  echo   'hello'     Type :   1 $ ls -alt    Did you notice that the prompt has changed? What happens when this is stored in  ~/.bash_profile ?   1 $  export   PS1 = '>> '     PS1  is a variable that defines the makeup and style of the command prompt.  echo $PS1  prints the variable.  PS1=\"value\"  changes the variable value.  PS1='>> '  sets the command prompt variable and exports the variable. It makes the variable available in all sub programs of the current shell. Here we change the default command prompt from  $  to  >> . After using the source command, the command line displays the new command prompt. Let\u2019s learn about two more environment variables. Type :   1 $  echo   $HOME     This returns the value of the  HOME  variable. What happens when you type this command?   1 $  echo   $HOME     The  HOME  variable is an environment variable that displays the path of the home directory. Here by typing echo  $HOME , the terminal displays the path  /home/ccuser  as output.  cd $HOME  goes to the home directory. You can customize the  HOME  variable if needed, but in most cases this is not necessary. In the command line, type :   1 $  echo   $PATH     Type :   1 /bin/pwd    Type :   1 /bin/ls    What happens when you type this command?   1\n2 $  echo   $PATH \n/home/ccuser/.gem/ruby/2.0.0/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/sbin:/sbin:/bin    PATH  is an environment variable that stores a list of directories separated by a colon. Looking carefully, echo  $PATH  lists the following directories:  /home/ccuser/.gem/ruby/2.0.0/bin .  /usr/local/sbin .  /usr/local/bin .  /usr/bin .  /usr/sbin .  /sbin .  /bin .    Each directory contains scripts for the command line to execute. The  PATH  variable simply lists which directories contain scripts. For example, many COMMANDS we\u2019ve learned are scripts stored in the  /bin  directory.   1 $ /bin/pwd    This is the script that is executed when you type the  pwd  command.   1 $ /bin/ls    This is the script that is executed when you type the ls command. In advanced cases, you can customize the  PATH  variable when adding scripts of your own. Type :   1 $ env    Type :   1 $ env  |  grep PATH    What happens when you type this command?   1 $ env    The  env  command stands for \u2018environment\u2019, and returns a list of the environment variables for the current user. Here, the  env  command returns a number of variables, including  PATH ,  PWD ,  PS1 , and  HOME .   1\n2 $ env  |  grep PATH\n$ env  |  grep aliasname    env | grep PATH  is a command that displays the value of a single environment variable. Here the standard output of env is \u2018piped\u2019 to the  grep  command.  grep  searches for the value of the variable  PATH  and outputs it to the terminal. You learned to use the bash profile to configure the environment. What can we generalize so far? The environment refers to the preferences and settings of the current user.     Let\u2019s summarize what we\u2019ve done so far.  The  nano  editor is a command line text editor used to configure the environment.  ~/.bash_profile  is where environment settings are stored. You can edit this file with  nano .  Environment variables are variables that can be used across commands and programs and hold information about the environment.  export VARIABLE='Value'  sets and exports an environment variable.  USER  is the name of the current user.  PS1  is the command prompt.  HOME  is the home directory. It is usually not customized.  PATH  returns a colon separated list of file paths. It is customized in advanced cases.  echo $PATH  prints the path.  PATH=\"value\"  changes the path..  export PATH=/home/dir/bin:$PATH  appends the new path to environment variable PATH.  env  returns a list of environment variables.",
            "title": "Environment"
        },
        {
            "location": "/Codecademy Learn the Command Line Notes/#multi-users",
            "text": "Linux is multi-user : multiple users at the same time as opposed to OS X or Windows; just like the UNIX mainframe computers with terminals, users and superuser concepts.    Access rights   Type :   1 $ ls -l     Read, from left to right :   \u2013  or  d : file or dir.  File or dir name.  Owner access ( r w x\u2026- ).  Group access ( r w x\u2026- ).  All access ( r w x\u2026- ).  Owner group, size, date.     How do we change the acces rights? With  chmod .   First, there are access right for:  u  ser.  g  roup.  o  thers.    Second, there levels. Each level has a numeric value.:  r  ead: 4.  w  rite: 2.  x  ecute: 1. \nFor example, using levels:  o+w : others can write the file (create).  u+x : users can execute the file.  g-x : group can no longer execute the file.  etc.  chmod o+w file1.txt  for example.     Why numeric value? It an alternative way for  chmod  to assign access rights. Levels are ranked with values:     Level  Binary  Decimal      r w x  111  7    r w -  110  6    r - -  100  4    - - -  000  0    r - x  101  5      A  7  grants full rights vs a  0  that grants no rights. For example:  chmod 600 file ; change the file access rights to  rw- --- --- .  chmod 600 dir ; change the directory access rights.    How are the values calculated?   1\n2\n3\n4\n5\n6 r+w+x  =   4 +2+1  =   7  r       =   4       =   4 \n     x   =       1   =   1 \nr+w    =   4 +2    =   6 \n...\n...  \nOnce you have the values, you can set the access rights:  1\n2\n3\n4  u   g   o  \n--- --- ---\nrwx rw- --x\n  7     6     1     Therefore,  chmod 761  set the access rights  rwx rw- --x  to a file, files, a directory or directories.   Ownership   chown thou file ; assign a new owner,  thou , to \u2018file\u2019.  chown thou dir ; \u2026 to dir.  chgrp newgr file  assign a new group owner,  newgr , to \u2018file\u2019.  chgrp newgr dir ; \u2026 to dir.   Superuser   su ; superuser login.  su file ; unlock the file with the superuser password.  sudo ; do it with the privilege of a superuser.  sudo apt-get update ; updates the database.  sudo apt-get upgrade ; upgrades all packages (update before upgrading).  sudo apt-get install build-essential ; install a useful package (or any other package).  sudo apt-get install git ; install \u2018git\u2019.  which git ; find where \u2018git\u2019 is located (if it\u2019s instaled).  sudo apt-get remove git ; remove \u2018git\u2019.  sudo apt-get purge git ; remove \u2018git\u2019 and purge any remainings.  sudo chnow user file ; change the owner.",
            "title": "Multi-Users"
        },
        {
            "location": "/Codecademy Learn the Command Line Notes/#multi-tasks",
            "text": "Linux is multi-task; multiple tasks can run at the same time like OS X or Windows or UNIX; Linux kernel runs processes; they take turns at the processor(s). All programs and processes can be launched/killed from the GUI and the CLI.   Show   top ; show process dashboard by PID number;  ?  for help,  q  for quit.  ps ; show the process list.  ps aux ; show all process.  ps aux | grep 'top' ; filter top processes.  ps aux | grep bash ; filter processes related to the bash.  ps aux | grep bash | sort ; \u2026 and sort them.     Manage   Ctrl + Z ; pause a process, put it in the background.  fg ; foreground, bring back the process.  jobs ; list paused processes.  fg #PID ; bring back process #PID (if there is more than one process on pause, you must identify the process).  Ctrl + C ; terminate the active process.  xload ; display the system load in a new windows, but jam the current terminal (open another or several terminals then).  xload & ; runs in the background  Ctrl + X `; suspend the process and unjam the terminal.  by ; resume the process.  ps ; show processes and their #PID.  ps x | grep bad_program ; find the bad processes.  kill  can send a signal from the OS to a process. For example : when you log off, you send a signal to terminate a word processing program and save the file before closing.  kill #PID ; kill the process.  kill -STOP #PID ; pause the process.  kill -TERM #PID ;  terminate the process.  kill -SIGTERM #PID ;  terminate the process.  kill -SIGKILL #PID ; kill the process.  kill -KILL ; force closing of the current process.  kill -9 #PID ;  force closing the process.    Killing sequence:  kill #PID ; doesn\u2019t work\u2026  kill -9 #PID , or..",
            "title": "Multi-Tasks"
        },
        {
            "location": "/Command Line Crash Course/",
            "text": "Foreword\n\n\nCommands and snippets.\n\n\n\n\nThe Setup\n\u00b6\n\n\nLinux\n\u00b6\n\n\nLook through the menu for your window manager for anything named \u2018shell or \u2018terminal\u2019.\n\n\nMac OS X\n\u00b6\n\n\n\n\nHold down the \nCmd\n key and hit the spacebar.\n\n\nIn the top right the blue \u2018search bar\u2019 will pop up.\n\n\nType: \nterminal\n\n\nClick on the terminal application that looks kind of like a black box.\n\n\nThis will open Terminal.\n\n\nYou can now go to your dock and \nCtrl\n-click to pull up the menu, then select \nOptions->Keep\n In dock.\n\n\n\n\nWindows\n\u00b6\n\n\nOn Windows we\u2019re going to use PowerShell. People used to work with a program called cmd, but it\u2019s not nearly as usable as PowerShell. If you have Windows 7 or later, do this:\n\n\n\n\nClick Start.\n\n\nIn Search programs and files type: \npowershell\n\n\nHit \nEnter\n. \n\n\n\n\nLinux/Mac OS X\n\u00b6\n\n\nList of commands:\n\n\n\n\npwd\n; print working directory.\n\n\nhostname\n; my computer\u2019s network name.\n\n\nmkdir\n; make directory.\n\n\ncd\n; change directory.\n\n\nls\n; list directory.\n\n\nrmdir\n; remove directory.\n\n\npushd\n; push directory.\n\n\npopd\n; pop directory.\n\n\ncp\n; copy a file or directory.\n\n\nmv\n; move a file or directory.\n\n\nless\n; page through a file. :q to quit.\n\n\ncat\n; print the whole file.\n\n\nxargs\n; execute arguments.\n\n\nfind\n; find files.\n\n\ngrep\n; find things inside files.\n\n\nman\n; read a manual page.\n\n\napropos\n; find what man page is appropriate.\n\n\nenv\n; look at your environment.\n\n\necho\n; print some arguments.\n\n\nexport\n; export/set a new environment variable.\n\n\nexit\n; exit the shell.\n\n\nsudo\n; become super user root.\n\n\n\n\nWindows\n\u00b6\n\n\nList of commands:\n\n\n\n\npwd\n; print working directory.\n\n\nhostname\n; my computer\u2019s network name.\n\n\nmkdir\n; make directory.\n\n\ncd\n; change directory.\n\n\nls\n; list directory.\n\n\nrmdir\n; remove directory.\n\n\npushd\n; push directory.\n\n\npopd\n; pop directory.\n\n\ncp\n; copy a file or directory.\n\n\nrobocopy\n; robust copy.\n\n\nmv\n; move a file or directory.\n\n\nmore\n; page through a file.\n\n\ntype\n; print the whole file.\n\n\nforfiles\n; run a command on lots of files.\n\n\ndir -r\n; find files.\n\n\nselect-string\n; find things inside files.\n\n\nhelp\n; read a manual page.\n\n\nhelpctr\n; find what man page is appropriate.\n\n\necho\n; print some arguments.\n\n\nset\n; export/set a new environment variable.\n\n\nexit\n; exit the shell.\n\n\nrunas\n; become super user root.\n\n\n\n\nPaths, Folders, Directories (\npwd\n)\n\u00b6\n\n\n$\n (Unix) or \n>\n (Windows).\n\n\nYou type in the stuff after \n$ or >\n, then hit \nEnter\n.\n\n\nYou can then see what I have for output followed by another \n$\n or \n>\n prompt. That content is the output and you should see the same output.\n\n\nLet\u2019s do a simple first command so you can get the hang of this:\n\n\nLinux/Mac OS X\n\u00b6\n\n\n1\n2\n3\n$ \npwd\n \n/Users/zedshaw\n$\n\n\n\n\n\n\nWindows\n\u00b6\n\n\n1\n2\n3\n4\n5\nPS C:\n\\>\n \npwd\n\n\nC:\n\\U\nsers\n\\z\ned\n\nPS C:\n\\>\n\n\n\n\n\n\n\nView a File (\nless, more\n)\n\u00b6\n\n\nTo do this exercise you\u2019re going to do some work using the commands you know so far. You\u2019ll also need a text editor that can make plain text (.txt) files. Here\u2019s what you do:\n\n\n\n\nOpen your text editor and type some stuff into a new file. On OS X this could be TextWrangler. On Windows this might be Notepad++. On Linux this could be Gedit. Any editor will work.\n\n\nSave that file to your desktop and name it test.txt.\n\n\nIn your shell use the commands you know to copy this file to your temp directory that you\u2019ve been working with.\n\n\n\n\nOnce you\u2019ve done that, complete this exercise:\n\n\nLinux/Mac OS X\n\u00b6\n\n\n1\n2\n3\n$ less test.txt\n\n[\ndisplays file here\n]\n\n$\n\n\n\n\n\n\nThat\u2019s it. To get out of \nless\n just type \n:q\n (as in quit).\n\n\nWindows\n\u00b6\n\n\n1\n2\n3\n> more test.txt\n\n[\ndisplays file here\n]\n\n> \n\n\n\n\n\n\nStream a File (\ncat\n)\n\u00b6\n\n\nYou\u2019re going to do some more setup for this one so you get used to making files in one program and then accessing them from the command line. With the same text editor from the last exercise, create another file named test2.txt but this time save it directly to your temp directory:\n\n\nLinux/Mac OS X\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n$ less test2.txt\n\n[\ndisplays file here\n]\n\n$ cat test2.txt\nI am a fun guy.\nDont you know why?\nBecause I make poems,\nthat make babies cry.\n$ cat test.txt\nHi there this is cool.\n$\n\n\n\n\n\n\nWindows\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n> more test2.txt\n\n[\ndisplays file here\n]\n\n> cat test2.txt\nI am a fun guy.\nDont you know why?\nBecause I make poems,\nthat make babies cry.\n> cat test.txt\nHi there this is cool.\n> \n\n\n\n\n\n\nEdit a file (\ncat, nano, pico, vim\n)\n\u00b6\n\n\n\n\nUnix: try \ncat test.txt test2.txt\n to concatenate the files on screen.\n\n\nWindows: try \ncat test.txt,test2.txt\n.\n\n\ncat test.txt\n will print on screen.\n\n\ncat file1.txt > file2.txt\n to copy.\n\n\ncat file1.txt >> file2.txt\n to append.\n\n\nAlso \nnano test.txt\n, \npico test.txt\n, and \nvim test.txt\n.\n\n\n\n\nExiting Your Terminal (\nexit\n)\n\u00b6\n\n\nLinux/Mac OS X\n\u00b6\n\n\n1\n$ \nexit\n\n\n\n\n\n\n\nWindows\n\u00b6\n\n\n1\n> \nexit\n\n\n\n\n\n\n\nUnix Bash References\n\u00b6\n\n\n\n\nReference Manual\n\n\n\n\nPowerShell References\n\u00b6\n\n\n\n\nOwner\u2019s Manual\n.\n\n\nMaster PowerShell\n.",
            "title": "Command Line Crash Course"
        },
        {
            "location": "/Command Line Crash Course/#linux",
            "text": "Look through the menu for your window manager for anything named \u2018shell or \u2018terminal\u2019.",
            "title": "Linux"
        },
        {
            "location": "/Command Line Crash Course/#mac-os-x",
            "text": "Hold down the  Cmd  key and hit the spacebar.  In the top right the blue \u2018search bar\u2019 will pop up.  Type:  terminal  Click on the terminal application that looks kind of like a black box.  This will open Terminal.  You can now go to your dock and  Ctrl -click to pull up the menu, then select  Options->Keep  In dock.",
            "title": "Mac OS X"
        },
        {
            "location": "/Command Line Crash Course/#windows",
            "text": "On Windows we\u2019re going to use PowerShell. People used to work with a program called cmd, but it\u2019s not nearly as usable as PowerShell. If you have Windows 7 or later, do this:   Click Start.  In Search programs and files type:  powershell  Hit  Enter .",
            "title": "Windows"
        },
        {
            "location": "/Command Line Crash Course/#linuxmac-os-x",
            "text": "List of commands:   pwd ; print working directory.  hostname ; my computer\u2019s network name.  mkdir ; make directory.  cd ; change directory.  ls ; list directory.  rmdir ; remove directory.  pushd ; push directory.  popd ; pop directory.  cp ; copy a file or directory.  mv ; move a file or directory.  less ; page through a file. :q to quit.  cat ; print the whole file.  xargs ; execute arguments.  find ; find files.  grep ; find things inside files.  man ; read a manual page.  apropos ; find what man page is appropriate.  env ; look at your environment.  echo ; print some arguments.  export ; export/set a new environment variable.  exit ; exit the shell.  sudo ; become super user root.",
            "title": "Linux/Mac OS X"
        },
        {
            "location": "/Command Line Crash Course/#windows_1",
            "text": "List of commands:   pwd ; print working directory.  hostname ; my computer\u2019s network name.  mkdir ; make directory.  cd ; change directory.  ls ; list directory.  rmdir ; remove directory.  pushd ; push directory.  popd ; pop directory.  cp ; copy a file or directory.  robocopy ; robust copy.  mv ; move a file or directory.  more ; page through a file.  type ; print the whole file.  forfiles ; run a command on lots of files.  dir -r ; find files.  select-string ; find things inside files.  help ; read a manual page.  helpctr ; find what man page is appropriate.  echo ; print some arguments.  set ; export/set a new environment variable.  exit ; exit the shell.  runas ; become super user root.",
            "title": "Windows"
        },
        {
            "location": "/Command Line Crash Course/#paths-folders-directories-pwd",
            "text": "$  (Unix) or  >  (Windows).  You type in the stuff after  $ or > , then hit  Enter .  You can then see what I have for output followed by another  $  or  >  prompt. That content is the output and you should see the same output.  Let\u2019s do a simple first command so you can get the hang of this:",
            "title": "Paths, Folders, Directories (pwd)"
        },
        {
            "location": "/Command Line Crash Course/#linuxmac-os-x_1",
            "text": "1\n2\n3 $  pwd  \n/Users/zedshaw\n$",
            "title": "Linux/Mac OS X"
        },
        {
            "location": "/Command Line Crash Course/#windows_2",
            "text": "1\n2\n3\n4\n5 PS C: \\>   pwd \n\nC: \\U sers \\z ed\n\nPS C: \\>",
            "title": "Windows"
        },
        {
            "location": "/Command Line Crash Course/#view-a-file-less-more",
            "text": "To do this exercise you\u2019re going to do some work using the commands you know so far. You\u2019ll also need a text editor that can make plain text (.txt) files. Here\u2019s what you do:   Open your text editor and type some stuff into a new file. On OS X this could be TextWrangler. On Windows this might be Notepad++. On Linux this could be Gedit. Any editor will work.  Save that file to your desktop and name it test.txt.  In your shell use the commands you know to copy this file to your temp directory that you\u2019ve been working with.   Once you\u2019ve done that, complete this exercise:",
            "title": "View a File (less, more)"
        },
        {
            "location": "/Command Line Crash Course/#linuxmac-os-x_2",
            "text": "1\n2\n3 $ less test.txt [ displays file here ] \n$   That\u2019s it. To get out of  less  just type  :q  (as in quit).",
            "title": "Linux/Mac OS X"
        },
        {
            "location": "/Command Line Crash Course/#windows_3",
            "text": "1\n2\n3 > more test.txt [ displays file here ] \n>",
            "title": "Windows"
        },
        {
            "location": "/Command Line Crash Course/#stream-a-file-cat",
            "text": "You\u2019re going to do some more setup for this one so you get used to making files in one program and then accessing them from the command line. With the same text editor from the last exercise, create another file named test2.txt but this time save it directly to your temp directory:",
            "title": "Stream a File (cat)"
        },
        {
            "location": "/Command Line Crash Course/#linuxmac-os-x_3",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 $ less test2.txt [ displays file here ] \n$ cat test2.txt\nI am a fun guy.\nDont you know why?\nBecause I make poems,\nthat make babies cry.\n$ cat test.txt\nHi there this is cool.\n$",
            "title": "Linux/Mac OS X"
        },
        {
            "location": "/Command Line Crash Course/#windows_4",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 > more test2.txt [ displays file here ] \n> cat test2.txt\nI am a fun guy.\nDont you know why?\nBecause I make poems,\nthat make babies cry.\n> cat test.txt\nHi there this is cool.\n>",
            "title": "Windows"
        },
        {
            "location": "/Command Line Crash Course/#edit-a-file-cat-nano-pico-vim",
            "text": "Unix: try  cat test.txt test2.txt  to concatenate the files on screen.  Windows: try  cat test.txt,test2.txt .  cat test.txt  will print on screen.  cat file1.txt > file2.txt  to copy.  cat file1.txt >> file2.txt  to append.  Also  nano test.txt ,  pico test.txt , and  vim test.txt .",
            "title": "Edit a file (cat, nano, pico, vim)"
        },
        {
            "location": "/Command Line Crash Course/#exiting-your-terminal-exit",
            "text": "",
            "title": "Exiting Your Terminal (exit)"
        },
        {
            "location": "/Command Line Crash Course/#linuxmac-os-x_4",
            "text": "1 $  exit",
            "title": "Linux/Mac OS X"
        },
        {
            "location": "/Command Line Crash Course/#windows_5",
            "text": "1 >  exit",
            "title": "Windows"
        },
        {
            "location": "/Command Line Crash Course/#unix-bash-references",
            "text": "Reference Manual",
            "title": "Unix Bash References"
        },
        {
            "location": "/Command Line Crash Course/#powershell-references",
            "text": "Owner\u2019s Manual .  Master PowerShell .",
            "title": "PowerShell References"
        },
        {
            "location": "/Command Shell Snippets/",
            "text": "Foreword\n\n\nCode snippets.\n\n\n\n\nSort in alphabetical order:\n\n\n1\n$ sort myfile.txt\n\n\n\n\n\n\nSort in numerical order:\n\n\n1\n$ sort -n myfile.txt\n\n\n\n\n\n\nSort on multiple column (on column 2 in numerical order, then on column 1 in alphabetical order):\n\n\n1\n$ sort myfile.txt -k2n -k1\n\n\n\n\n\n\nSort a comma-separated table:\n\n\n1\n$ sort -k2 -k3 k1 -t \n','\n myfile.txt\n\n\n\n\n\n\nSort in reversed order:\n\n\n1\n$ sort -r myfile.txt\n\n\n\n\n\n\nSort a pip-separated file in reverses order by the second column:\n\n\n1\n$ sort myfile.txt -nrk \n2\n -st \n'|'\n\n\n\n\n\n\n\nSearch text files for lines matching regular expressions (regex) with \ngrep <matching string> <source>\n:\n\n\n1\n$ grep ArticleTitle webpage.html\n\n\n\n\n\n\nAn asterisk (\n*\n) indicates any character (the matching expression starts with \nAr\n and ends with \nle\n):\n\n\n1\n$ grep Ar*le mytext.txt\n\n\n\n\n\n\nSearch for starting metacharacter \n>\n:\n\n\n1\n$ grep ^\n'>'\n mytext.txt\n\n\n\n\n\n\nList the access rights for all files:\n\n\n1\n$ ls -lag\n\n\n\n\n\n\nRun commands in background, kill the job running in the foreground, suspend the job running in the foreground, and background the suspended job:\n\n\n1\n2\n3\n4\n5\n6\n7\n$ \ncommand\n \n&\n\n\n$ ^C\n\n$ ^Z\n\n$ \nbg\n\n\n\n\n\n\n\nList the current jobs, foreground job number 1, and kill job number 1:\n\n\n1\n2\n3\n4\n5\n$ \njobs\n\n\n$ fg%1\n\n$ kill%1\n\n\n\n\n\n\nList current processes, and kill process number 26152:\n\n\n1\n2\n3\n$ ps\n\n$ \nkill\n \n26152\n\n\n\n\n\n\n\nHelp about commands:\n\n\n1\n2\n3\n$ man <\ncommand\n name>\n\n$ whatis <\ncommand\n name>",
            "title": "Command Shell Snippets"
        },
        {
            "location": "/The Linux Command Line/",
            "text": "Foreword\n\n\nNotes. The 537-page volume covers the same material as \nLinuxCommand.org\n, but in much greater detail. In addition to the basics of command line use and shell scripting, The Linux Command Line includes chapters on many common programs used on the command line, as well as more advanced topics.\n\n\n\n\n1.0 Learning the Shell\n\u00b6\n\n\n1.1 What Is \u201cThe Shell\u201d?\n,\n\n\nterminal\n\n\n1.2 Navigation\n,\n\n\nfile, system, organization, working directory\n\n\n1.3 Looking Around\n,\n\n\nlist, file, size, group, owner, permission, create text file, long format, classify, examine\n\n\n1.4 A Guided Tour\n,\n\n\ndirectory, root, boot, etc, bin, usr, local, var, lib, home, root, tmp, dev, proc, media\n\n\n1.5 Manipulating Files\n,\n\n\ncopy, move, remove, create directory\n\n\n1.6 Working With Commands\n,\n\n\ntype, display information, which, locate, help, command, manual\n\n\n1.7 I/O Redirection\n,\n\n\ninput, output, pipeline, pipe, filter, sort, uniq, pattern, read text, grep, fmt, split, page break, header, footer, pr, first line, last line, head, tail, translate, tr, editor, sed, awk\n\n\n1.8 Expansion\n,\n\n\necho, pathname, quote, double-quote, escape, backslash\n\n\n1.9 Permissions\n,\n\n\npermission, chmod, su, sudo, chown, chgrp, directory\n\n\n1.10 Job Control\n,\n\n\njob, process, ps, kill, jobs, background, bg, foreground, fg\n\n\n2.0 Writing Shell Scripts\n\u00b6\n\n\n2.1 Writing Your First Script And Getting It To Work\n,\n\n\neditor, vi, vim, emacs, nano, gedit, kwrite, path\n\n\n2.2 Editing The Scripts You Already Have\n,\n\n\nedit, shell, location, path, export path, alias, environmental variable, today\n\n\n2.3 Here Scripts\n,\n\n\nscript, here, shebang, output, append\n\n\n2.4 Variables\n,\n\n\nscript, variable, create, environment\n\n\n2.5 Command Substitution And Constants\n,\n\n\ncommand substitution, constant, assigning, assign\n\n\n2.6 Shell Functions\n,\n\n\nshell function, open page, head section, right title, close, body section, timestamp, right, time, macro, \n\n\n2.7 Some Real Work\n,\n\n\nuptime, time, show_uptime, drive, space, drive_space, home, space, home_space\n\n\n2.8 Flow Control - Part 1\n,\n\n\nif, test, testing, exit, clear\n\n\n2.9 Stay Out Of Trouble\n,\n\n\nempty variable, missing quote, isolating, isolate, run script, watch\n\n\n2.10 Keyboard Input And Arithmetic\n,\n\n\nread, arithmetic\n\n\n2.11 Flow Control - Part 2\n,\n\n\nbranch, branching, case, loop, while, building an menu\n\n\n2.12 Positional Parameters\n,\n\n\npositional parameters, command line, arguments, options, command line processor into script, interactive\n\n\n2.13 Flow Control - Part 3\n,\n\n\nloop, for, if, else, find, wc, system_info\n\n\n2.14 Errors And Signals And Traps (Oh My!) - Part 1\n,\n\n\ncheck, checking, exit status, error, and, or\n\n\n2.15 Errors And Signals And Traps (Oh My!) - Part 2\n,\n\n\nclean, cleaning, trap, kill, clean_up, temporary, file\n\n\n3.0 Resources (some)\n\u00b6\n\n\nAdventures\n\u00b6\n\n\n3.1 Midnight Commander\n\n\nA directory browser and file manager for command line users.\n\n\ngui, file, directory, manipulation, copy, move, rename, delete, permission, remote system, ftp, ssh, .tar, .zip, local, hotlist, name, content\n\n\n3.2 Terminal Multiplexers\n\n\nGive your terminal some serious muscle.\n\n\nlike gnome, konsole, gnu, desktop, multiple shell session single terminal, display, several computer\n\n\n3.3 Less Typing\n\n\nFingers getting tired? Let\u2019s look at ways to save our digits!\n\n\nalias, shell functions, script, program, alias, cli, editor, control\n\n\n3.4 More Redirection\n\n\nWe take a deeper look at this powerful feature.\n\n\ni/o, input, output, pipeline, pipe, exec\n\n\n3.5 tput\n\n\nOur scripts can have more visual appeal!\n\n\ntput, ncurses, terminal, manipulate, change, color, effect, text\n\n\n3.6 dialog\n\n\nLet\u2019s give our scripts a better user interface.\n\n\nincorporate, script, build list, calendar, checklist, directory, box, form, file select, gauge, pause, gui, simple, simplify\n\n\n3.7 AWK\n\n\nPattern scanning and text processing language. One of the truly classic Unix tools.\n\n\nawk, programming language, coding, code, script\n\n\n3.8 Other Shells\n\n\nWhile most Linux users rely on the bash shell every day, it\u2019s not the only game in town. Let\u2019s look at some of the others.\n\n\ndash, debian almquist shell, tcsh, texex c shell, ksh, korn shell, zsh, z shell\n\n\nShell Scripts\n\u00b6\n\n\n3.9 new_script\n\n\nA bash shell script template generator. Use this script to help write your own shell scripts. Generated templates include useful shell functions, error and signal handling, and command-line option and argument parsing.\n\n\nscript, example, case\n\n\n3.10 my_cloud\n\n\nImplements a primitive cloud storage system using any available remote host running an SSH server.\n\n\nscript, example, case, dropbox\n\n\n3.11 photo2mail\n\n\nRe-sizes large image files (photos) for use as attachments to email messages, blog postings, etc.\n\n\nscript, example, case, read image file, convert, size\n\n\n3.12 program_list\n\n\nCreates an annotated list of programs in a directory. Useful for exploring your system.\n\n\nscript, example, case, program_list, list, whatis, what\n\n\nManuals\n\u00b6\n\n\n\n\nGranneman, Scott, Linux Phrasebook, 2015.\n\n\nParker, Steve, Shell Scripting: Expert Recipes for Linux, Bash and more, 2011.\n\n\nWard, Brian, How Linux Works: What Every Superuser Should Know, 2014.",
            "title": "The Linux Command Line"
        },
        {
            "location": "/The Linux Command Line/#20-writing-shell-scripts",
            "text": "2.1 Writing Your First Script And Getting It To Work ,  editor, vi, vim, emacs, nano, gedit, kwrite, path  2.2 Editing The Scripts You Already Have ,  edit, shell, location, path, export path, alias, environmental variable, today  2.3 Here Scripts ,  script, here, shebang, output, append  2.4 Variables ,  script, variable, create, environment  2.5 Command Substitution And Constants ,  command substitution, constant, assigning, assign  2.6 Shell Functions ,  shell function, open page, head section, right title, close, body section, timestamp, right, time, macro,   2.7 Some Real Work ,  uptime, time, show_uptime, drive, space, drive_space, home, space, home_space  2.8 Flow Control - Part 1 ,  if, test, testing, exit, clear  2.9 Stay Out Of Trouble ,  empty variable, missing quote, isolating, isolate, run script, watch  2.10 Keyboard Input And Arithmetic ,  read, arithmetic  2.11 Flow Control - Part 2 ,  branch, branching, case, loop, while, building an menu  2.12 Positional Parameters ,  positional parameters, command line, arguments, options, command line processor into script, interactive  2.13 Flow Control - Part 3 ,  loop, for, if, else, find, wc, system_info  2.14 Errors And Signals And Traps (Oh My!) - Part 1 ,  check, checking, exit status, error, and, or  2.15 Errors And Signals And Traps (Oh My!) - Part 2 ,  clean, cleaning, trap, kill, clean_up, temporary, file",
            "title": "2.0 Writing Shell Scripts"
        },
        {
            "location": "/The Linux Command Line/#30-resources-some",
            "text": "",
            "title": "3.0 Resources (some)"
        },
        {
            "location": "/The Linux Command Line/#adventures",
            "text": "3.1 Midnight Commander  A directory browser and file manager for command line users.  gui, file, directory, manipulation, copy, move, rename, delete, permission, remote system, ftp, ssh, .tar, .zip, local, hotlist, name, content  3.2 Terminal Multiplexers  Give your terminal some serious muscle.  like gnome, konsole, gnu, desktop, multiple shell session single terminal, display, several computer  3.3 Less Typing  Fingers getting tired? Let\u2019s look at ways to save our digits!  alias, shell functions, script, program, alias, cli, editor, control  3.4 More Redirection  We take a deeper look at this powerful feature.  i/o, input, output, pipeline, pipe, exec  3.5 tput  Our scripts can have more visual appeal!  tput, ncurses, terminal, manipulate, change, color, effect, text  3.6 dialog  Let\u2019s give our scripts a better user interface.  incorporate, script, build list, calendar, checklist, directory, box, form, file select, gauge, pause, gui, simple, simplify  3.7 AWK  Pattern scanning and text processing language. One of the truly classic Unix tools.  awk, programming language, coding, code, script  3.8 Other Shells  While most Linux users rely on the bash shell every day, it\u2019s not the only game in town. Let\u2019s look at some of the others.  dash, debian almquist shell, tcsh, texex c shell, ksh, korn shell, zsh, z shell",
            "title": "Adventures"
        },
        {
            "location": "/The Linux Command Line/#shell-scripts",
            "text": "3.9 new_script  A bash shell script template generator. Use this script to help write your own shell scripts. Generated templates include useful shell functions, error and signal handling, and command-line option and argument parsing.  script, example, case  3.10 my_cloud  Implements a primitive cloud storage system using any available remote host running an SSH server.  script, example, case, dropbox  3.11 photo2mail  Re-sizes large image files (photos) for use as attachments to email messages, blog postings, etc.  script, example, case, read image file, convert, size  3.12 program_list  Creates an annotated list of programs in a directory. Useful for exploring your system.  script, example, case, program_list, list, whatis, what",
            "title": "Shell Scripts"
        },
        {
            "location": "/The Linux Command Line/#manuals",
            "text": "Granneman, Scott, Linux Phrasebook, 2015.  Parker, Steve, Shell Scripting: Expert Recipes for Linux, Bash and more, 2011.  Ward, Brian, How Linux Works: What Every Superuser Should Know, 2014.",
            "title": "Manuals"
        },
        {
            "location": "/Ubuntu/",
            "text": "Foreword\n\n\nNotes.\n\n\n\n\nInstall Ubuntu 16.04\n\u00b6\n\n\n\n\nUbuntu\n.\n\n\n\n\nFollowing the Installation (check list)\n\u00b6\n\n\n\n\nVirtualBox \nguest additions\n. (for a virtual installation).\n\n\nTweaks\n.\n\n\nMove launcher to bottom.\n\n\nRemove guest session.\n\n\nShow/Enable UserName on Unity app panel.\n\n\nStylish your desktop.\n\n\nEnable one-click minimize.\n\n\nInstall Linux graphics drivers.\n\n\nInstall media codecs.\n\n\nInstall punch of apps.\n\n\nInstall more browsers.\n\n\nInstall other desktop environments.\n\n\nInstall Flash player.\n\n\nInstall openJDK.\n\n\nInstall additional softwares.\n\n\nInstall Synaptic Package Manager.\n\n\nInstall Java& openJDK.\n\n\nSet up your online accounts.\n\n\nInstall BleachBit (system cleaner).\n\n\n\u2026\n\n\n\n\n\n\nGet started\n.\n\n\nThings to do after installing\n.\n\n\nTweaks to do after installing\n.\n\n\nInstall software\u2026",
            "title": "Ubuntu 16.04"
        },
        {
            "location": "/Ubuntu/#following-the-installation-check-list",
            "text": "VirtualBox  guest additions . (for a virtual installation).  Tweaks .  Move launcher to bottom.  Remove guest session.  Show/Enable UserName on Unity app panel.  Stylish your desktop.  Enable one-click minimize.  Install Linux graphics drivers.  Install media codecs.  Install punch of apps.  Install more browsers.  Install other desktop environments.  Install Flash player.  Install openJDK.  Install additional softwares.  Install Synaptic Package Manager.  Install Java& openJDK.  Set up your online accounts.  Install BleachBit (system cleaner).  \u2026    Get started .  Things to do after installing .  Tweaks to do after installing .  Install software\u2026",
            "title": "Following the Installation (check list)"
        },
        {
            "location": "/Install_tarball/",
            "text": "Foreword\n\n\nNotes.\n\n\n\n\nInstall a Tarball on Linux\n\u00b6\n\n\nDownload the tarball; it can have different extensions:\n\n\n\n\nfile.tar.gz\n\n\nfile.tgz\n\n\nfile.tar.bz2\n\n\nfile.tbz2\n\n\n\n\nUnzip the file:\n\n\n\n\nCLI: \n$ tar xvfz sqlite-autoconf-3071502.tar.gz\n.\n\n\nDouble-clicking the triggers an software.\n\n\n\n\nInstall the tarball on \n/usr/local\n:\n\n\n\n\nChange directory (where the file is located; however, the installation will be in: \n/usr/local\n).\n\n\n$ \ncd\n sqlite-autoconf-3071502\n.\n\n\n\n\n\n\nOpen INSTALL or README file for more information (choose).\n\n\n$ vi INSTALL\n.\n\n\n$ gedit INSTALL\n.\n\n\nOr another text editor.\n\n\n\n\n\n\nConfigure the software to ensure your system has the necessary functionality and libraries to successfully compile the package.\n\n\n$ ./configure --prefix\n=\n/usr/local\n.\n\n\n\n\n\n\nCompile all the source files into executable binaries.\n\n\n$ make\n.\n\n\n\n\n\n\nInstall the binaries and any supporting files into the appropriate locations.\n\n\n$ sudo make install\n.\n\n\n\n\n\n\n\n\nRun an Executive file on Linux\n\u00b6\n\n\n\n\nRun a .sh file in the bash.\n\n\n$ bash file.sh\n.",
            "title": "Install & Run on Linux"
        },
        {
            "location": "/Install_tarball/#run-an-executive-file-on-linux",
            "text": "Run a .sh file in the bash.  $ bash file.sh .",
            "title": "Run an Executive file on Linux"
        },
        {
            "location": "/Usefull+Shell+Commands+for+Data+Science/",
            "text": "Foreword\n\n\nCode snippets and excerpts from the tutorial. bash. From DataCamp.\n\n\n\n\nQuick notes\n\u00b6\n\n\nUse bash or alternative such as zsh.\n\n\nExamples are based on the \nadult\n dataset from the UCI Machine Learning repository (Census Income dataset). This data set is commonly used to predict whether income exceeds $50K/yr based on census data. With 48842 rows and 14 attributes.\n\n\nMove to the directory with \ncd <dir>\n and print the current working directory with \npwd\n. Move up with \ncd ..\n.\n\n\nCount with \nwc\n\u00b6\n\n\n1\n2\n# count lines\n\nwc -l adult.data\n\n\n\n\n\n\n1\n32562 adult.data\n\n\n\n\n\n\n1\n2\n# count words\n\nwc -w adult.data\n\n\n\n\n\n\n1\n488415 adult.data\n\n\n\n\n\n\n1\nls -l\n\n\n\n\n\n\n1\n2\n3\n4\n5\ntotal 9540\n-rw-rw-r-- 1 ugo ugo 3974305 jan 17 19:47 adult.data\ndrwxrwxr-x 2 ugo ugo    4096 jan 17 19:59 adult\n-rw-rw-r-- 1 ugo ugo 5776165 jan 19 14:37 os\n-rw-rw-r-- 1 ugo ugo    6619 jan 19 14:37 Usefull shell commands for Data Science.ipynb\n\n\n\n\n\n\n1\nls -l folder\n\n\n\n\n\n\n1\n2\n3\ntotal 0\n-rw-rw-r-- 1 ugo ugo 0 jan 17 19:57 Nouveau document 1\n-rw-rw-r-- 1 ugo ugo 0 jan 17 19:57 Nouveau document 2\n\n\n\n\n\n\n1\n2\n# count files\n\nls -l folder \n|\n wc -l\n\n\n\n\n\n\n1\n3\n\n\n\n\n\n\n1\n2\n# print head (10 by default or -n)\n\nhead -n \n2\n adult.data\n\n\n\n\n\n\n1\n2\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n\n\n\n\n\n\nConcatenate with \ncat\n\u00b6\n\n\n\n\nPrint a file content with \ncat adult.data\n.\n\n\nConcatenate files and create (replace) a file with \n>\n. \n>>\n will appends.\n\n\n\n\n1\ncat adult.data adult.data > target_file.csv\n\n\n\n\n\n\nAdd the header to the original file.\n\n\n1\necho\n \n\"age,workclass,fnlwgt,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loss,native-country,class\"\n > header.csv\n\n\n\n\n\n\nAdd the header and rename the file.\n\n\n1\ncat header.csv adult.data > adult.csv\n\n\n\n\n\n\nCheck the first and last row (the default is 10 lines unless specified otherwise).\n\n\n1\nhead -n \n1\n adult.csv\n\n\n\n\n\n\n1\nage,workclass,fnlwgt,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loss,native-country,class\n\n\n\n\n\n\n1\ntail -n \n2\n adult.csv\n\n\n\n\n\n\n1\n52, Self-emp-inc, 287927, HS-grad, 9, Married-civ-spouse, Exec-managerial, Wife, White, Female, 15024, 0, 40, United-States, >50K\n\n\n\n\n\n\nModify with \nsed\n\u00b6\n\n\nWhen a file is corrupted or badly formatted, with no UTF-8 characters or misplaced comma.\n\n\n1\nsed \n\"s/<string to replace>/<string to replace it with>/g\"\n <source_file> > <target_file>.\n\n\n\n\n\n\nReplace \n?\n for missing value with \nNaN\n. \n\n\nFirst, count the instances.\n\n\n1\ngrep \n\", ?,\"\n adult.csv \n|\n wc -l\n\n\n\n\n\n\n1\n2399\n\n\n\n\n\n\nSecond, \n\n\n\n\nreplace all the columns with \n?\n\u2026\n\n\n\"s/<string to replace>/\"\n.\n\n\n\n\n\n\nby an empty string\u2026\n\n\n\"/<string to replace it with>/g\"\n. Use column delimiter \n,\n.\n\n\n\n\n\n\n\n\n1\nsed \n\"s/, ?,/,,/g\"\n adult.csv > adult_v2.csv\n\n\n\n\n\n\nSubset\n\u00b6\n\n\nLarge file (30M rows and more). Sample the head or the tail.\n\n\nExtract the head (120 lines).\n\n\n1\nhead -n \n120\n adult_v2.csv > adult_v3.csv\n\n\n\n\n\n\nExtract the tail (12 lines).\n\n\n1\ntail -n \n12\n adult_v2.csv > adult_v4.csv\n\n\n\n\n\n\nExtract 20 lines starting at line 100.\n\n\n1\nhead -n \n120\n adult_v2.csv \n|\n tail -n \n20\n > adult_sample.csv\n\n\n\n\n\n\nAdd the header to the file without.\n\n\n1\ncat header.csv adult_v4.csv > adult_v4_with_header.csv\n\n\n\n\n\n\n1\ncat header.csv adult_sample.csv > adult_sample_with_header.csv\n\n\n\n\n\n\nFind duplicates with \nuniq\n\u00b6\n\n\nFind adjacent repeated lines in a file.\n\n\n\n\nuniq -c\n adds the repetition count to each line.\n\n\nuniq -d\n only outputs duplicate lines.\n\n\nuniq -u\n only outputs unique lines.\n\n\n\n\nFirst, sort the file to bunch duplicates together. Second, count the duplicates.\n\n\n1\nsort adult_v2.csv \n|\n uniq -d \n|\n wc -l\n\n\n\n\n\n\n1\n23\n\n\n\n\n\n\nThird, sort the file again, find the duplicates, sort the results in reverse, and output the first 3 duplicates. \n\n\n1\nsort adult_v2.csv \n|\n uniq -c \n|\n sort -r \n|\n head -n \n3\n\n\n\n\n\n\n\n1\n2\n3\n      3 25, Private, 195994, 1st-4th, 2, Never-married, Priv-house-serv, Not-in-family, White, Female, 0, 0, 40, Guatemala, <=50K\n      2 90, Private, 52386, Some-college, 10, Never-married, Other-service, Not-in-family, Asian-Pac-Islander, Male, 0, 0, 35, United-States, <=50K\n      2 49, Self-emp-not-inc, 43479, Some-college, 10, Married-civ-spouse, Craft-repair, Husband, White, Male, 0, 0, 40, United-States, <=50K\n\n\n\n\n\n\nSelect columns with \ncut\n\u00b6\n\n\nSelect a particular column. \n-d\n specifies the column delimiter and \n-f\n specifies the columns.\n\n\nFind the number of unique values taken by the categorical variable \nworkclass\n (2\nnd\n column of the file) and print the head of the results.\n\n\n1\ncut -d \n\",\"\n -f \n2\n adult_v2.csv \n|\n head -3\n\n\n\n\n\n\n1\n2\n3\nworkclass\n State-gov\n Self-emp-not-inc\n\n\n\n\n\n\nRepeat, but this time, sort the results and find the duplicates.\n\n\n1\ncut -d \n\",\"\n -f \n2\n adult_v2.csv \n|\n sort \n|\n uniq -c\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n   1837 \n    960  Federal-gov\n   2093  Local-gov\n      7  Never-worked\n  22696  Private\n   1116  Self-emp-inc\n   2541  Self-emp-not-inc\n   1298  State-gov\n     14  Without-pay\n      1 workclass\n\n\n\n\n\n\nLoop\n\u00b6\n\n\nWork of several files with loops.\n\n\nReplace one character with another within all files inside a directory. Here is how to declare variables and call variables.\n\n\n1\n2\n3\nvarname1\n=\n10\n\n\nvarname2\n=\n'hello'\n\n\nvarname3\n=\n123\n.4\n\n\n\n\n\n\n1\necho\n \n$varname1\n\n\n\n\n\n\n\n1\n10\n\n\n\n\n\n\n1\necho\n \n$varname2\n\n\n\n\n\n\n\n1\nhello\n\n\n\n\n\n\n1\necho\n \n$varname3\n\n\n\n\n\n\n\n1\n123.4\n\n\n\n\n\n\nFirst, declare two variables. Second, loop through the folder with \nfor\n. Third, replace the character. Finally, for each file, create a new file.\n\n\n1\n2\n3\n4\n5\n6\nreplace_source\n=\n' '\n\n\nreplace_target\n=\n'_'\n\n\nfor\n filename in ./*.csv\n;\n \ndo\n\n    \nnew_filename\n=\n${\nfilename\n//\n$replace_source\n/\n$replace_target\n}\n\n    mv \n\"\n$filename\n\"\n \n\"\n$new_filename\n\"\n\n\ndone\n\n\n\n\n\n\n\nWith the \nwhile\n loop. However, \nfor\n loops are faster.\n\n\n1\n2\n3\n4\n5\n6\nreplace_source\n=\n' '\n\n\nreplace_target\n=\n'_'\n\n\nwhile\n true\n;\n \ndo\n\n    \nnew_filename\n=\n${\nfilename\n//\n$replace_source\n/\n$replace_target\n}\n\n    mv \n\"\n$filename\n\"\n \n\"\n$new_filename\n\"\n\n\ndone",
            "title": "Useful Shell Commands for Data Science"
        },
        {
            "location": "/Usefull+Shell+Commands+for+Data+Science/#count-with-wc",
            "text": "1\n2 # count lines \nwc -l adult.data   1 32562 adult.data   1\n2 # count words \nwc -w adult.data   1 488415 adult.data   1 ls -l   1\n2\n3\n4\n5 total 9540\n-rw-rw-r-- 1 ugo ugo 3974305 jan 17 19:47 adult.data\ndrwxrwxr-x 2 ugo ugo    4096 jan 17 19:59 adult\n-rw-rw-r-- 1 ugo ugo 5776165 jan 19 14:37 os\n-rw-rw-r-- 1 ugo ugo    6619 jan 19 14:37 Usefull shell commands for Data Science.ipynb   1 ls -l folder   1\n2\n3 total 0\n-rw-rw-r-- 1 ugo ugo 0 jan 17 19:57 Nouveau document 1\n-rw-rw-r-- 1 ugo ugo 0 jan 17 19:57 Nouveau document 2   1\n2 # count files \nls -l folder  |  wc -l   1 3   1\n2 # print head (10 by default or -n) \nhead -n  2  adult.data   1\n2 39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K",
            "title": "Count with wc"
        },
        {
            "location": "/Usefull+Shell+Commands+for+Data+Science/#concatenate-with-cat",
            "text": "Print a file content with  cat adult.data .  Concatenate files and create (replace) a file with  > .  >>  will appends.   1 cat adult.data adult.data > target_file.csv   Add the header to the original file.  1 echo   \"age,workclass,fnlwgt,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loss,native-country,class\"  > header.csv   Add the header and rename the file.  1 cat header.csv adult.data > adult.csv   Check the first and last row (the default is 10 lines unless specified otherwise).  1 head -n  1  adult.csv   1 age,workclass,fnlwgt,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loss,native-country,class   1 tail -n  2  adult.csv   1 52, Self-emp-inc, 287927, HS-grad, 9, Married-civ-spouse, Exec-managerial, Wife, White, Female, 15024, 0, 40, United-States, >50K",
            "title": "Concatenate with cat"
        },
        {
            "location": "/Usefull+Shell+Commands+for+Data+Science/#modify-with-sed",
            "text": "When a file is corrupted or badly formatted, with no UTF-8 characters or misplaced comma.  1 sed  \"s/<string to replace>/<string to replace it with>/g\"  <source_file> > <target_file>.   Replace  ?  for missing value with  NaN .   First, count the instances.  1 grep  \", ?,\"  adult.csv  |  wc -l   1 2399   Second,    replace all the columns with  ? \u2026  \"s/<string to replace>/\" .    by an empty string\u2026  \"/<string to replace it with>/g\" . Use column delimiter  , .     1 sed  \"s/, ?,/,,/g\"  adult.csv > adult_v2.csv",
            "title": "Modify with sed"
        },
        {
            "location": "/Usefull+Shell+Commands+for+Data+Science/#subset",
            "text": "Large file (30M rows and more). Sample the head or the tail.  Extract the head (120 lines).  1 head -n  120  adult_v2.csv > adult_v3.csv   Extract the tail (12 lines).  1 tail -n  12  adult_v2.csv > adult_v4.csv   Extract 20 lines starting at line 100.  1 head -n  120  adult_v2.csv  |  tail -n  20  > adult_sample.csv   Add the header to the file without.  1 cat header.csv adult_v4.csv > adult_v4_with_header.csv   1 cat header.csv adult_sample.csv > adult_sample_with_header.csv",
            "title": "Subset"
        },
        {
            "location": "/Usefull+Shell+Commands+for+Data+Science/#find-duplicates-with-uniq",
            "text": "Find adjacent repeated lines in a file.   uniq -c  adds the repetition count to each line.  uniq -d  only outputs duplicate lines.  uniq -u  only outputs unique lines.   First, sort the file to bunch duplicates together. Second, count the duplicates.  1 sort adult_v2.csv  |  uniq -d  |  wc -l   1 23   Third, sort the file again, find the duplicates, sort the results in reverse, and output the first 3 duplicates.   1 sort adult_v2.csv  |  uniq -c  |  sort -r  |  head -n  3    1\n2\n3       3 25, Private, 195994, 1st-4th, 2, Never-married, Priv-house-serv, Not-in-family, White, Female, 0, 0, 40, Guatemala, <=50K\n      2 90, Private, 52386, Some-college, 10, Never-married, Other-service, Not-in-family, Asian-Pac-Islander, Male, 0, 0, 35, United-States, <=50K\n      2 49, Self-emp-not-inc, 43479, Some-college, 10, Married-civ-spouse, Craft-repair, Husband, White, Male, 0, 0, 40, United-States, <=50K",
            "title": "Find duplicates with uniq"
        },
        {
            "location": "/Usefull+Shell+Commands+for+Data+Science/#select-columns-with-cut",
            "text": "Select a particular column.  -d  specifies the column delimiter and  -f  specifies the columns.  Find the number of unique values taken by the categorical variable  workclass  (2 nd  column of the file) and print the head of the results.  1 cut -d  \",\"  -f  2  adult_v2.csv  |  head -3   1\n2\n3 workclass\n State-gov\n Self-emp-not-inc   Repeat, but this time, sort the results and find the duplicates.  1 cut -d  \",\"  -f  2  adult_v2.csv  |  sort  |  uniq -c    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10    1837 \n    960  Federal-gov\n   2093  Local-gov\n      7  Never-worked\n  22696  Private\n   1116  Self-emp-inc\n   2541  Self-emp-not-inc\n   1298  State-gov\n     14  Without-pay\n      1 workclass",
            "title": "Select columns with cut"
        },
        {
            "location": "/Usefull+Shell+Commands+for+Data+Science/#loop",
            "text": "Work of several files with loops.  Replace one character with another within all files inside a directory. Here is how to declare variables and call variables.  1\n2\n3 varname1 = 10  varname2 = 'hello'  varname3 = 123 .4   1 echo   $varname1    1 10   1 echo   $varname2    1 hello   1 echo   $varname3    1 123.4   First, declare two variables. Second, loop through the folder with  for . Third, replace the character. Finally, for each file, create a new file.  1\n2\n3\n4\n5\n6 replace_source = ' '  replace_target = '_'  for  filename in ./*.csv ;   do \n     new_filename = ${ filename // $replace_source / $replace_target } \n    mv  \" $filename \"   \" $new_filename \"  done    With the  while  loop. However,  for  loops are faster.  1\n2\n3\n4\n5\n6 replace_source = ' '  replace_target = '_'  while  true ;   do \n     new_filename = ${ filename // $replace_source / $replace_target } \n    mv  \" $filename \"   \" $new_filename \"  done",
            "title": "Loop"
        },
        {
            "location": "/SQL-NoSQL_CS/",
            "text": "Foreword\n\n\nCheat sheets.\n\n\n\n\nSQL\n\u00b6\n\n\n\n\nSQL 1\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSQL 2\n. PDF.\n\n\n\n\n\n\n\n\n\n\nSQLite\n\u00b6\n\n\n\n\nSQLite\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\nMySQL\n\u00b6\n\n\n\n\nMySQL\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEssential MySQL\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEssential Admin for MySQL\n. PDF only.\n\n\n\n\nPostgreSQL\n\u00b6\n\n\n\n\nPostgreSQL\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPostgreSQL Interactive Terminal Commands\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEssential PostgreSQL\n. PDF only.\n\n\n\n\nNoSQL\n\u00b6\n\n\n\n\nNoSQL and Data Scalability\n. PDF only.\n\n\nMongoDB\n. PDF only.",
            "title": "SQL-NoSQL Cheat Sheets"
        },
        {
            "location": "/SQL-NoSQL_CS/#sqlite",
            "text": "SQLite . PDF.",
            "title": "SQLite"
        },
        {
            "location": "/SQL-NoSQL_CS/#mysql",
            "text": "MySQL . PDF.         Essential MySQL . PDF.         Essential Admin for MySQL . PDF only.",
            "title": "MySQL"
        },
        {
            "location": "/SQL-NoSQL_CS/#postgresql",
            "text": "PostgreSQL . PDF.          PostgreSQL Interactive Terminal Commands . PDF.        Essential PostgreSQL . PDF only.",
            "title": "PostgreSQL"
        },
        {
            "location": "/SQL-NoSQL_CS/#nosql",
            "text": "NoSQL and Data Scalability . PDF only.  MongoDB . PDF only.",
            "title": "NoSQL"
        },
        {
            "location": "/Codecademy Learn SQL/",
            "text": "Foreword\n\n\nCode Snippets. From Codecademy.\n\n\n\n\nSpecifying Comments\n\u00b6\n\n\n\n\nLine comment. This is indicated by two negative signs (eg. \n--\n). The remainder of the text on the line is the comment.\n\n\nBlock comment. The start of the block comment is indicated by \n/*\n, the end of the comment by the same string. A block comment can cover text in part of a line, or can span multiple lines.\n\n\nRem\n or \n@\n. For Oracle, a line starting with either \nREM\n or \n@\n is a comment line.\n\n\n\n\nManipulation\n\u00b6\n\n\nSQL, for Structured Query Language, is a programming language designed to manage data stored in relational databases. SQL operates through simple, declarative statements. This keeps data accurate and secure, and helps maintain the integrity of databases, regardless of size.\n\n\nThe SQL language is widely used today across web frameworks and database applications. Knowing SQL gives you the freedom to explore your data, and the power to make better decisions. By learning SQL, you will also learn concepts that apply to nearly every data storage system.\n\n\nThe statements covered in this course, use SQLite Relational Database Management System (RDBMS). You can learn more about RDBMS\u2019s here. You can also access a glossary of all the SQL commands taught in this course here.\n\n\n1\n2\n3\n4\n5\n6\n--Show, create, use database\n\n\n\nSHOW\n \ndatabases\n;\n\n\nCREATE\n \nDATABASE\n \ndbname\n;\n\n\nSHOW\n \ndatabases\n;\n\n\nUSE\n \ndbname\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Show tables\n\n\n\nSHOW\n \ntables\n;\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n--Create the table:\n\n\n\nCREATE\n \nTABLE\n \ntable_name\n \n(\n\n    \ncolumn_1\n \ndata_type\n,\n \n    \ncolumn_2\n \ndata_type\n,\n \n    \ncolumn_3\n \ndata_type\n\n  \n);\n\n\n\n\n\n\n\n1\nCREATE\n \nTABLE\n \ncelebs\n \n(\nid\n \nINTEGER\n,\n \nname\n \nTEXT\n,\n \nage\n \nINTEGER\n);\n\n\n\n\n\n\n\n1\n2\n3\n--Add a row:\n\n\n\nINSERT\n \nINTO\n \ncelebs\n \n(\nid\n,\nname\n,\nage\n)\n \nVALUES\n \n(\n1\n,\n\"Justin Bieber\"\n,\n21\n);\n\n\n\n\n\n\n\n1\n2\n3\n--View:\n\n\n\nSELECT\n \n*\n \nFrom\n \ncelebs\n;\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n--Insert more rows:\n\n\n\nINSERT\n \nINTO\n \ncelebs\n \n(\nid\n,\nname\n,\nage\n)\n \nVALUES\n \n(\n2\n,\n\"Beyonce Knowles\"\n,\n33\n);\n\n\nINSERT\n \nINTO\n \ncelebs\n \n(\nid\n,\nname\n,\nage\n)\n \nVALUES\n \n(\n3\n,\n\"Jeremy Lin\"\n,\n26\n);\n\n\nINSERT\n \nINTO\n \ncelebs\n \n(\nid\n,\nname\n,\nage\n)\n \nVALUES\n \n(\n4\n,\n\"Taylor Swift\"\n,\n26\n);\n\n\n\n\n\n\n\n1\n2\n3\n--View:\n\n\n\nSELECT\n \nname\n \nFROM\n \ncelebs\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Edit a row:\n\n\n\nUPDATE\n \ncelebs\n \nSET\n \nage\n \n=\n \n22\n \nWHERE\n \nid\n \n=\n \n1\n;\n\n\n\n\n\n\n\n1\n2\n3\n--View:\n\n\n\nSELECT\n \n*\n \nFROM\n \ncelebs\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Add a new column:\n\n\n\nALTER\n \nTABLE\n \ncelebs\n \nADD\n \nCOLUMN\n \ntwitter_handle\n \nTEXT\n;\n\n\n\n\n\n\n\n1\n2\n3\n--View:\n\n\n\nSELECT\n \n*\n \nFROM\n \ncelebs\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Update the table:\n\n\n\nUPDATE\n \ncelebs\n \nSET\n \ntwitter_handle\n \n=\n \n'@taylorswift13'\n \nWHERE\n \nid\n \n=\n \n4\n;\n\n\n\n\n\n\n\n1\n2\n3\n--View:\n\n\n\nSELECT\n \n*\n \nFROM\n \ncelebs\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Delete rows. NULL for missing or unknown data:\n\n\n\nDELETE\n \nFROM\n \ncelebs\n \nWHERE\n \ntwitter_handle\n \nIS\n \nNULL\n;\n\n\n\n\n\n\n\n1\n2\n3\n--View:\n\n\n\nSELECT\n \n*\n \nFROM\n \ncelebs\n;\n\n\n\n\n\n\n\nProject Create Table\n\u00b6\n\n\nIn this project you will create your own friends table and add and delete data from it.\n\n\nThe instructions provided are a general guideline, but feel free to add more columns, insert more data, or create more tables.\n\n\nNotes: plan a project by drawing an ULM schema.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nCREATE\n \nTABLE\n \nfriends\n \n(\nid\n \nINTEGER\n,\n \nname\n \nTEXT\n,\n \nbirthday\n \nDATE\n);\n\n\n\nSELECT\n \n*\n \nFROM\n \nfriends\n;\n\n\n\nDROP\n \ntables\n \nfriends\n;\n\n\n\nCREATE\n \nTABLE\n \nfriends\n \n(\nid\n \nINTEGER\n,\n \nname\n \nTEXT\n,\n \nbirthday\n \nDATE\n);\n\n\n\nSHOW\n \ntables\n;\n\n\n\nINSERT\n \nINTO\n \nfriends\n \n(\nid\n,\n \nname\n,\n \nbirthday\n)\n \nVALUES\n \n(\n1\n,\n\"Jane Doe\"\n,\n'1993-05-19'\n);\n\n\nINSERT\n \nINTO\n \nfriends\n \n(\nid\n,\n \nname\n,\n \nbirthday\n)\n \nVALUES\n \n(\n2\n,\n\"Jade Donot\"\n,\n'1995-06-12'\n);\n\n\nINSERT\n \nINTO\n \nfriends\n \n(\nid\n,\n \nname\n,\n \nbirthday\n)\n \nVALUES\n \n(\n3\n,\n\"Jack Doom\"\n,\n'1990-10-01'\n);\n\n\nINSERT\n \nINTO\n \nfriends\n \n(\nid\n,\n \nname\n,\n \nbirthday\n)\n \nVALUES\n \n(\n4\n,\n\"John Doe\"\n,\n'1988-12-09'\n);\n\n\n\nUPDATE\n \nfriends\n \nSET\n \nname\n \n=\n \n\"Jane Smith\"\n \nWHERE\n \nid\n \n=\n \n1\n;\n \n\n\nALTER\n \nTABLE\n \nfriends\n \nADD\n \nCOLUMN\n \nemail\n \nTEXT\n;\n\n\n\nUPDATE\n \nfriends\n \nSET\n \nemail\n \n=\n \n\"jdoe@example.com\"\n \nWHERE\n \nid\n \n=\n \n1\n;\n\n\nUPDATE\n \nfriends\n \nSET\n \nemail\n \n=\n \n\"jade@example.com\"\n \nWHERE\n \nid\n \n=\n \n2\n;\n\n\nUPDATE\n \nfriends\n \nSET\n \nemail\n \n=\n \n\"doom@example.com\"\n \nWHERE\n \nid\n \n=\n \n3\n;\n\n\nUPDATE\n \nfriends\n \nSET\n \nemail\n \n=\n \n\"johndoe@example.com\"\n \nWHERE\n \nid\n \n=\n \n4\n;\n\n\n\nDELETE\n \nFROM\n \nfriends\n \nWHERE\n \nid\n \n=\n \n1\n;\n\n\n\n\n\n\n\nQueries\n\u00b6\n\n\nOne of the core purposes of the SQL language is to retrieve information stored in a database. This is commonly referred to as querying. Queries allow us to communicate with the database by asking questions and having the result set return data relevant to the question. In this lesson, you will be querying a database with one table named movies. Let\u2019s get started.\n\n\n1\n2\n3\n--Find rows:\n\n\n\nSELECT\n \nname\n,\n \nimdb_rating\n \nFROM\n \nmovies\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Find rows. SELECT DISTINCT for unique values in the result set. It filters out all duplicate values:\n\n\n\nSELECT\n \nDISTINCT\n \ngenre\n \nFROM\n \nmovies\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Find rows with WHERE:\n\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nimdb_rating\n \n>\n \n8\n;\n\n\n\n\n\n\n\nClauses:\n\n\n\n\n=\n, equals.\n\n\n!=\n, not equals.\n\n\n>\n, greater than.\n\n\n<\n, less than.\n\n\n>=\n, greater than or equal to.\n\n\n<=\n, less than or equal to.\n\n\n\n\n1\n2\n3\n--Find rows with LIKE. Se_en represents a pattern with a wildcard character. The _ means you can substitute any individual character here without breaking the pattern. The names Seven and Se7en both match this pattern:\n\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nname\n \nLIKE\n \n'Se_en'\n;\n\n\n\n\n\n\n\n1\n2\n3\n4\n--Find rows with LIKE. % is another wildcard character that can be used with LIKE. % is a wildcard character that matches zero or more missing letters in the pattern. A% matches all movies with names that begin with \"A\". %a matches all movies that end with \"a\":\n\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nname\n \nLIKE\n \n'a%'\n;\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nname\n \nLIKE\n \n'%man%'\n;\n\n\n\n\n\n\n\n1\n2\n3\n4\n--Find rows with BETWEEN. The values can be numbers, text or dates. Names that begin with letters \"A\" up to but not including \"J\". Years between 1990 up to and including 2000:\n\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nname\n \nBETWEEN\n \n'A'\n \nAND\n \n'J'\n;\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nyear\n \nBETWEEN\n \n1990\n \nAND\n \n2000\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Find rows with BETWEEN, AND:\n\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nyear\n \nBETWEEN\n \n1990\n \nAND\n \n2000\n \nAND\n \ngenre\n \n=\n \n'comedy'\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Find rows with BETWEEN, OR:\n\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \ngenre\n \n=\n \n'comedy'\n \nOR\n \nyear\n \n<\n \n1980\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Find rows with BETWEEN, ORDERED BY, DESC, ASC:\n\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nORDER\n \nBY\n \nimdb_rating\n \nDESC\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Find rows, BETWEEN, ORDERED BY, ASC. LIMIT is a clause that lets you specify the maximum number of rows the result set will have:\n\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nORDER\n \nBY\n \nimdb_rating\n \nASC\n \nLIMIT\n \n3\n;\n\n\n\n\n\n\n\nProject Writing Queries\n\u00b6\n\n\nIn this project you will write queries to retrieve information from the movies table.\n\n\nThe instructions provided are a general guideline, but feel free to experiment writing your own queries.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n--Return all of the unique years in the movies table.\n\n\n\nSELECT\n \nDISTINCT\n \nyear\n \nFROM\n \nmovies\n;\n\n\n\n--Return all of the unique years in the movies table sorted from oldest to newest.\n\n\n\nSELECT\n \nDISTINCT\n \nyear\n \nFROM\n \nmovies\n \nORDER\n \nBY\n \nyear\n \nASC\n;\n\n\n\n--Return all movies that are dramas.\n\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \ngenre\n \n=\n \n\"drama\"\n;\n\n\n\n--Return all of the movies with names that contain \"bride\".\n\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nname\n \nLIKE\n \n'%Br%'\n;\n\n\n\n--Return all of the movies that were made between 2000 and 2015.\n\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nyear\n \n>=\n \n2000\n \nAND\n \nyear\n \n<=\n \n2015\n;\n\n\n\n--Return all of the movies that were made in 1995 or have an IMDb rating of 9.\n\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nyear\n \n=\n \n1995\n \nOR\n \nimdb_rating\n \n=\n \n9\n;\n\n\n\n--Return the name and IMDb rating of every movie made after 2009 in alphabetical order.\n\n\n\nSELECT\n \nname\n,\n \nimdb_rating\n \nFROM\n \nmovies\n \nWHERE\n \nyear\n \n>\n \n2009\n \nORDER\n \nBY\n \nname\n \nASC\n;\n\n\n\n--Return 3 movies with an IMDb rating of 7.\n\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nimdb_rating\n \n=\n \n7\n \nLIMIT\n \n3\n;\n\n\n\n--Return 10 movies with an IMDb rating greater than 6, are comedies, were made after 1995, and sorted by IMDb rating from highest to lowest. (Hint: you can use AND more than once in a statement).\n\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nimdb_rating\n \n>\n \n6\n \nAND\n \ngenre\n \n=\n \n'comedy'\n \nAND\n \nyear\n \n>\n \n1995\n \nORDER\n \nBY\n \nimdb_rating\n \nASC\n \nLIMIT\n \n10\n;\n\n\n\n--Return all movies named 'Cast Away'.\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nname\n \n=\n \n'Cast Away'\n;\n\n\n\n--Return all movies with an IMDb rating not equal to 7.\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nimdb_rating\n \n!=\n \n7\n;\n\n\n\n--Return all movies with a horror genre and an IMDb rating less than 6.\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \ngenre\n \n=\n \n'horror'\n \nAND\n \nimdb_rating\n \n<\n \n6\n;\n\n\n\n--Return 10 movies with an IMDb rating greater than 8 sorted by their genre.\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nimdb_rating\n \n>\n \n8\n \nORDER\n \nBY\n \ngenre\n \nASC\n \nLIMIT\n \n10\n;\n\n\n\n--Return all movies that include 'King' in the name.\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nname\n \nLIKE\n \n'%King%'\n;\n\n\n\n--Return all movies with names that end with the word 'Out'\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nname\n \nLIKE\n \n'%Out'\n;\n\n\n\n--Return all movies with names that begin with the word \"The\" sorted by IMDb rating from highest to lowest\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nname\n \nLIKE\n \n'The%'\n \nORDER\n \nBY\n \nimdb_rating\n \nDESC\n;\n\n\n\n--Return all of the movies.\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n;\n\n\n\n--Return the name and id of each movie with an id greater than 125.\n\n\nSELECT\n \nname\n,\n \nid\n \nFROM\n \nmovies\n \nWHERE\n \nid\n \n>\n \n125\n;\n\n\n\n--Return all movies with names that begin with 'X-Men'\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nname\n \nLIKE\n \n'X-Men%'\n;\n\n\n\n--Return the first 10 movies sorted in reverse alphabetical order.\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nDESC\n \nLIMIT\n \n10\n;\n\n\n\n--Return the id, name, and genre of all movies that are romances.\n\n\nSELECT\n \nid\n,\n \nname\n,\n \ngenre\n \nFROM\n \nmovies\n \nWHERE\n \ngenre\n \n=\n \n'romance'\n;\n\n\n\n--Return all of the Twilight movies in order from the year they were released from oldest to newest.\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nname\n \nLIKE\n \n'%Twilight%'\n \nORDER\n \nBY\n \nyear\n \nASC\n;\n\n\n\n--Return all of the movies that were released in 2012 that are comedies.\n\n\nSELECT\n \n*\n \nFROM\n \nmovies\n \nWHERE\n \nyear\n \n=\n \n2012\n \nAND\n \ngenre\n \n=\n \n'comedy'\n;\n\n\n\n\n\n\n\nAggregate Functions\n\u00b6\n\n\nAggregate functions compute a single result from a set of input values. For instance, when we need the sum or average of a particular column, we can use aggregate functions to quickly compute it for us. We will be learning about different aggregate functions in this lesson.\n\n\nFor this lesson we have given you a table named fake_apps which is made up of data for fake mobile applications.\n\n\n1\n2\n3\n--View:\n\n\n\nSELECT\n \n*\n \nFROM\n \nfake_apps\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Calculate the number of rows in a table. where the column is not NULL. Here, we want to count every row so we pass * as an argument:\n\n\n\nSELECT\n \nCOUNT\n(\n*\n)\n \nFROM\n \nfake_apps\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Count * or columns):\n\n\n\nSELECT\n \nCOUNT\n(\n*\n)\n \nFROM\n \nfake_apps\n \nWHERE\n \nprice\n \n=\n \n0\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Aggregate. GROUP BY is a clause in SQL that is only used with aggregate functions. It is used in collaboration with the SELECT statement to arrange identical data into groups. Here, our aggregate function is COUNT() and we are passing price as an argument to GROUP BY. SQL will count the total number of apps for each price in the table:\n\n\n\nSELECT\n \nprice\n,\n \nCOUNT\n(\n*\n)\n \nFROM\n \nfake_apps\n \nGROUP\n \nBY\n \nprice\n;\n\n\n\n\n\n\n\n1\n2\n3\n--It calculates the total number of apartments in each neighborhood in the cities table and groups them by neighborhood.\n\n\n\nSELECT\n \nneighborhood\n,\n \nSUM\n(\napartments\n)\n \nFROM\n \ncities\n \nGROUP\n \nBY\n \nneighborhood\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Sum up, aggregate by category:\n\n\n\nSELECT\n \ncategory\n,\n \nSUM\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n \nGROUP\n \nBY\n \ncategory\n;\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n--Find the maximum, minimum, global, categorical:\n\n\nSELECT\n \nMAX\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n;\n\n\nSELECT\n \nname\n,\n \ncategory\n,\n \nMAX\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n \nGROUP\n \nBY\n \ncategory\n;\n\n\n\nSELECT\n \nMIN\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n;\n\n\nSELECT\n \nname\n,\n \ncategory\n,\n \nMIN\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n \nGROUP\n \nBY\n \ncategory\n;\n\n\n\n\n\n\n\n1\n2\n3\n4\n--It returns the title, genre and checkout count for the book with the most checkouts in the library_books table.\n\n\n\nSELECT\n \ntitle\n,\n \ngenre\n,\n \nMAX\n(\ncheckouts\n)\n \nFROM\n \nlibrary_books\n\n\nGROUP\n \nBY\n \ngenre\n;\n\n\n\n\n\n\n\n1\n2\n3\n4\n--Find the average, or category average:\n\n\n\nSELECT\n \nAVG\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n;\n\n\nSELECT\n \nprice\n,\n \nAVG\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n \nGROUP\n \nBY\n \nprice\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Round up to x decimal(s):\n\n\n\nSELECT\n \nprice\n,\n \nROUND\n(\nAVG\n(\ndownloads\n),\n \n2\n)\n \nFROM\n \nfake_apps\n \nGROUP\n \nBY\n \nprice\n;\n\n\n\n\n\n\n\nProject Fake Apps\n\u00b6\n\n\nIn this project you will write queries with aggregate functions to retrieve information from the fake_apps table.\n\n\nThe instructions provided are a general guideline, but feel free to experiment writing your own queries, creating your own tables, or adding your own apps to the existing table!\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n--Return the total number of apps in the table fake_apps.\n\n\n\nSELECT\n \nCOUNT\n(\n*\n)\n \nFROM\n \nfake_apps\n;\n\n\n\n--Return the name, category, and price of the app that has been downloaded the least amount of times.\n\n\n\nSELECT\n \nname\n,\n \ncategory\n,\n \nprice\n,\n \nMIN\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n;\n\n\n\n--Return the total number of apps for each category.\n\n\n\nSELECT\n \nCOUNT\n(\n*\n)\n \nFROM\n \nfake_apps\n \nGROUP\n \nBY\n \ncategory\n;\n\n\n\n--Return the name and category of the app that has been downloaded the most amount of times.\n\n\n\nSELECT\n \nname\n,\n \ncategory\n,\n \nMAX\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n;\n\n\n\n--Return the name and category of the app that has been downloaded the least amount of times.\n\n\n\nSELECT\n \nname\n,\n \ncategory\n,\n \nMIN\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n;\n\n\n\n--Return the average price for an app in each category.\n\n\n\nSELECT\n \nAVG\n(\nprice\n)\n \nFROM\n \nfake_apps\n \nGROUP\n \nBY\n \ncategory\n;\n\n\n\n--Return the average price for an app in each category. Round the averages to two decimal places.\n\n\n\nSELECT\n \nROUND\n(\nAVG\n(\nprice\n),\n2\n)\n \nFROM\n \nfake_apps\n \nGROUP\n \nBY\n \ncategory\n;\n\n\n\n--Return the maximum price for an app.\n\n\n\nSELECT\n \nMAX\n(\nprice\n)\n \nFROM\n \nfake_apps\n;\n\n\n\n--Return the minimum number of downloads for an app.\n\n\n\nSELECT\n \nMIN\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n;\n\n\n\n--Return the total number of downloads for apps that belong to the Games category.\n\n\n\nSELECT\n \nMIN\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n \nWHERE\n \ncategory\n \n=\n \n'Games'\n;\n\n\n\n--Return the total number of apps that are free.\n\n\n\nSELECT\n \nCOUNT\n(\n*\n)\n \nFROM\n \nfake_apps\n \nWHERE\n \nprice\n \n=\n \n0\n;\n\n\n\n--Return the total number of apps that cost 14.99.\n\n\n\nSELECT\n \nCOUNT\n(\n*\n)\n \nFROM\n \nfake_apps\n \nWHERE\n \nprice\n \n=\n \n14\n.\n99\n;\n\n\n\n--Return the sum of the total number of downloads for apps that belong to the Music category.\n\n\n\nSELECT\n \nSUM\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n \nWHERE\n \ncategory\n \n=\n \n'Music'\n;\n\n\n\n--Return the sum of the total number of downloads for apps that belong to the Business category.\n\n\n\nSELECT\n \nSUM\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n \nWHERE\n \ncategory\n \n=\n \n'Business'\n;\n\n\n\n--Return the name of each category with the total number of apps that belong to it.\n\n\n\nSELECT\n \nname\n,\n \nCOUNT\n(\n*\n)\n \nFROM\n \nfake_apps\n \nGROUP\n \nBY\n \ncategory\n;\n\n\n\n--Return the price and average number of downloads grouped by price.\n\n\n\nSELECT\n \nprice\n,\n \nAVG\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n \nGROUP\n \nBY\n \nprice\n;\n\n\n\n--Return the price and average number of downloads grouped by price. Round the averages to the nearest integer.\n\n\n\nSELECT\n \nROUND\n(\nprice\n,\n0\n),\n \nAVG\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n \nGROUP\n \nBY\n \nprice\n;\n\n\n\n--Return the name and category and price of the most expensive app for each category.\n\n\n\nSELECT\n \nname\n,\n \ncategory\n,\n \nprice\n,\n \nMAX\n(\nprice\n)\n \nFROM\n \nfake_apps\n \nGROUP\n \nBY\n \ncategory\n;\n\n\n\n--Return the total number of apps whose name begin with the letter 'A'.\n\n\n\nSELECT\n \nCOUNT\n(\n*\n)\n \nFROM\n \nfake_apps\n \nWHERE\n \nname\n \nLIKE\n \n'A%'\n;\n\n\n\n--Return the total number of downloads for apps belonging to the Sports or Health & Fitness category.\n\n\n\nSELECT\n \nSUM\n(\ndownloads\n)\n \nFROM\n \nfake_apps\n \nWHERE\n \ncategory\n \n=\n \n'Sports'\n \nOR\n \ncategory\n \n=\n \n'Health & Fitness'\n;\n\n\n\n\n\n\n\nMultiple Tables\n\u00b6\n\n\nMost of the time, data is distributed across multiple tables in the database. Imagine a database with two tables, artists and albums. An artist can produce many different albums, and an album is produced by an artist.\n\n\nThe data in these tables are related to each other. Through SQL, we can write queries that combine data from multiple tables that are related to one another. This is one of the most powerful features of relational databases.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n--We have created a table named albums for you. Create a second table named artists. Using the CREATE TABLE statement we added a PRIMARY KEY to the id column. A primary key serves as a unique identifier for each row or record in a given table. The primary key is literally an id value for a record. We're going to use this value to connect artists to the albums they have produced. By specifying that the id column is the PRIMARY KEY, SQL makes sure that none of the values in this column are NULL and each value in this column is unique:\n\n\n\nCREATE\n \nTABLE\n \nartists\n(\nid\n \nINTEGER\n \nPRIMARY\n \nKEY\n,\n \nname\n \nTEXT\n);\n\n\n\n--View both tables:\n\n\n\nSELECT\n \n*\n \nFROM\n \nalbums\n;\n\n\nSELECT\n \n*\n \nFROM\n \nartists\n;\n\n\n\n\n\n\n\n1\n2\n3\n--Query:\n\n\n\nSELECT\n \n*\n \nFROM\n \nartists\n \nWHERE\n \nid\n \n=\n \n3\n;\n\n\n\n\n\n\n\n1\n2\n3\n--A foreign key is a column that contains the primary key of another table in the database. We use foreign keys and primary keys to connect rows in two different tables. One table's foreign key holds the value of another table's primary key. Unlike primary keys, foreign keys do not need to be unique and can be NULL. Here, artist_id is a foreign key in the albums table. We can see that Michael Jackson has an id of 3 in the artists table. All of the albums by Michael Jackson also have a 3 in the artist_id column in the albums table. This is how SQL is linking data between the two tables. The relationship between the artists table and the albums table is the id value of the artists.\n\n\n\nSELECT\n \n*\n \nFROM\n \nalbums\n \nWHERE\n \nartist_id\n \n=\n \n3\n;\n\n\n\n\n\n\n\n1\n2\n3\n--One way to query multiple tables is to write a SELECT statement with multiple table names separated by a comma. This is also known as a CROSS JOIN. Here, albums and artists are the different tables we are querying. When querying more than one table, column names need to be specified by table_name.column_name. Here, the result set includes the name and year columns from the albums table and the name column from the artists table. Unfortunately the result of this cross join is not very useful. It combines every row of the artists table with every row of the albums table. It would be more useful to only combine the rows where the album was created by the artist.\n\n\n\nSELECT\n \nalbums\n.\nname\n,\n \nalbums\n.\nyear\n,\n \nartists\n.\nname\n \nFROM\n \nalbums\n,\n \nartists\n;\n\n\n\n\n\n\n\n1\n2\n3\n--In SQL, joins are used to combine rows from two or more tables. The most common type of join in SQL is an INNER JOIN. An inner join will combine rows from different tables if the join condition is true. Let's look at the syntax to see how it works. SELECT * specifies the columns our result set will have. Here, we want to include every column in both tables. FROM albums specifies the first table we are querying. JOIN artists ON specifies the type of join we are going to use as well as the name of the second table. Here, we want to do an inner join and the second table we want to query is artists. albums.artist_id = artists.id is the join condition that describes how the two tables are related to each other. Here, SQL uses the foreign key column artist_id in the albums table to match it with exactly one row in the artists table with the same value in the id column. We know it will only match one row in the artists table because id is the PRIMARY KEY of artists.\n\n\n\nSELECT\n \n*\n \nFROM\n \nalbums\n \nJOIN\n \nartists\n \nON\n \nalbums\n.\nartist_id\n \n=\n \nartists\n.\nid\n;\n\n\n\n\n\n\n\n1\n2\n3\n--OUTER JOINS (LEFT or RIGHT) also combine rows from two or more tables, but unlike inner joins, they do not require the join condition to be met. Instead, every row in the left table is returned in the result set, and if the join condition is not met, then NULL values are used to fill in the columns from the right table. The left table is simply the first table that appears in the statement. Here, the left table is albums. Likewise, the right table is the second table that appears. Here, artists is the right table.\n\n\n\nSELECT\n \n*\n \nFROM\n \nalbums\n \nLEFT\n \nJOIN\n \nartists\n \nON\n \nalbums\n.\nartist_id\n \n=\n \nartists\n.\nid\n;\n\n\n\n\n\n\n\n1\n2\n3\n--AS is a keyword in SQL that allows you to rename a column or table using an alias. The new name can be anything you want as long as you put it inside of single quotes. Here we want to rename the albums.name column as 'Album', and the artists.name column as 'Artist'. It is important to note that the columns have not been renamed in either table. The aliases only appear in the result set.\n\n\n\nSELECT\n \nalbums\n.\nname\n,\n \nalbums\n.\nyear\n,\n \nartists\n.\nname\n \nFROM\n \nalbums\n \nJOIN\n \nartists\n \nON\n \nalbums\n.\nartist_id\n \n=\n \nartists\n.\nid\n \nWHERE\n \nalbums\n.\nyear\n \n>\n \n1980\n;\n\n\n\n\n\n\n\n1\nSELECT\n \nalbums\n.\nname\n \nAS\n \n'Album'\n,\n \nalbums\n.\nyear\n,\n \nartists\n.\nname\n \nAS\n \n'Artist'\n \nFROM\n \nalbums\n \nJOIN\n \nartists\n \nON\n \nalbums\n.\nartist_id\n \n=\n \nartists\n.\nid\n \nWHERE\n \nalbums\n.\nyear\n \n>\n \n1980\n;\n\n\n\n\n\n\n\nRecap\n\u00b6\n\n\n\n\nPrimary Key is a column that serves a unique identifier for row in the table. Values in this column must be unique and cannot be \nNULL\n.\n\n\nForeign Key is a column that contains the primary key to another table in the database. It is used to identify a particular row in the referenced table.\n\n\nJoins are used in SQL to combine data from multiple tables.\n\n\nINNER JOIN\n will combine rows from different tables if the join condition is true.\n\n\nLEFT OUTER JOIN\n will return every row in the left table, and if the join condition is not met, \nNULL\n values are used to fill in the columns from the right table.\n\n\nAS\n is a keyword in SQL that allows you to rename a column or table in the result set using an alias.\n\n\n\n\nProject Querying Tables\n\u00b6\n\n\nIn this project you will practice querying multiple tables using joins.\n\n\nThe instructions provided are a general guideline, but feel free to experiment creating your own tables, adding data, and writing more queries.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n--Create a table named tracks with an id, title, and album_id column. The id column should be the PRIMARY KEY.\n\n\n\nCREATE\n \nTABLE\n \ntracks\n \n(\nid\n \nINTEGER\n \nPRIMARY\n \nKEY\n,\n \ntitle\n \nTEXT\n,\n \nalbums_id\n \nINTEGER\n);\n\n\n\n--\"Smooth Criminal\" is a track from Michael Jackson's \"Bad\" album. Add this track to the database.\n\n\n\nINSERT\n \nINTO\n \ntracks\n \n(\nid\n,\ntitle\n,\nalbums_id\n)\n \nVALUES\n \n(\n1\n,\n\"Smooth Criminal\"\n,\n \n8\n);\n\n\n\n--Add more tracks to the database.\n\n\nINSERT\n \nINTO\n \ntracks\n \n(\nid\n,\ntitle\n,\nalbums_id\n)\n \nVALUES\n \n(\n2\n,\n\"aaa\"\n,\n \n1\n);\n\n\nINSERT\n \nINTO\n \ntracks\n \n(\nid\n,\ntitle\n,\nalbums_id\n)\n \nVALUES\n \n(\n3\n,\n\"bbb\"\n,\n \n1\n);\n\n\nINSERT\n \nINTO\n \ntracks\n \n(\nid\n,\ntitle\n,\nalbums_id\n)\n \nVALUES\n \n(\n4\n,\n\"ccc\"\n,\n \n8\n);\n\n\nINSERT\n \nINTO\n \ntracks\n \n(\nid\n,\ntitle\n,\nalbums_id\n)\n \nVALUES\n \n(\n5\n,\n\"ddd\"\n,\n \n8\n);\n\n\nINSERT\n \nINTO\n \ntracks\n \n(\nid\n,\ntitle\n,\nalbums_id\n)\n \nVALUES\n \n(\n6\n,\n\"eee\"\n,\n \n8\n);\n\n\n\n--Combine the albums and tracks tables using an INNER JOIN. Order the query by album_id.\n\n\n\nSELECT\n \n*\n \nFROM\n \nalbums\n \nJOIN\n \ntracks\n \nON\n \nalbums\n.\nartist_id\n \n=\n \ntracks\n.\nid\n;\n\n\n\n--Combine the albums and artists table using a LEFT OUTER JOIN. Let albums be the left table.\n\n\n\nSELECT\n \n*\n \nFROM\n \nalbums\n \nLEFT\n \nJOIN\n \nartists\n \nON\n \nalbums\n.\nartist_id\n \n=\n \nartists\n.\nid\n;\n\n\n\n--Combine the albums and artists table using a LEFT OUTER JOIN. Let artists be the left table.\n\n\n\nSELECT\n \n*\n \nFROM\n \nartists\n \nLEFT\n \nJOIN\n \nalbums\n \nON\n \nalbums\n.\nartist_id\n \n=\n \nartists\n.\nid\n;\n\n\n\n--Use any join you like to combine the albums and tracks table. Rename the album_id column to Albums.\n\n\n\nSELECT\n \n*\n \nFROM\n \nalbums\n \nLEFT\n \nJOIN\n \ntracks\n \nON\n \nalbums\n.\nartist_id\n \n=\n \ntracks\n.\nid\n;",
            "title": "Codecademy, Learn SQL"
        },
        {
            "location": "/Codecademy Learn SQL/#manipulation",
            "text": "SQL, for Structured Query Language, is a programming language designed to manage data stored in relational databases. SQL operates through simple, declarative statements. This keeps data accurate and secure, and helps maintain the integrity of databases, regardless of size.  The SQL language is widely used today across web frameworks and database applications. Knowing SQL gives you the freedom to explore your data, and the power to make better decisions. By learning SQL, you will also learn concepts that apply to nearly every data storage system.  The statements covered in this course, use SQLite Relational Database Management System (RDBMS). You can learn more about RDBMS\u2019s here. You can also access a glossary of all the SQL commands taught in this course here.  1\n2\n3\n4\n5\n6 --Show, create, use database  SHOW   databases ;  CREATE   DATABASE   dbname ;  SHOW   databases ;  USE   dbname ;    1\n2\n3 --Show tables  SHOW   tables ;    1\n2\n3\n4\n5\n6\n7 --Create the table:  CREATE   TABLE   table_name   ( \n     column_1   data_type ,  \n     column_2   data_type ,  \n     column_3   data_type \n   );    1 CREATE   TABLE   celebs   ( id   INTEGER ,   name   TEXT ,   age   INTEGER );    1\n2\n3 --Add a row:  INSERT   INTO   celebs   ( id , name , age )   VALUES   ( 1 , \"Justin Bieber\" , 21 );    1\n2\n3 --View:  SELECT   *   From   celebs ;    1\n2\n3\n4\n5 --Insert more rows:  INSERT   INTO   celebs   ( id , name , age )   VALUES   ( 2 , \"Beyonce Knowles\" , 33 );  INSERT   INTO   celebs   ( id , name , age )   VALUES   ( 3 , \"Jeremy Lin\" , 26 );  INSERT   INTO   celebs   ( id , name , age )   VALUES   ( 4 , \"Taylor Swift\" , 26 );    1\n2\n3 --View:  SELECT   name   FROM   celebs ;    1\n2\n3 --Edit a row:  UPDATE   celebs   SET   age   =   22   WHERE   id   =   1 ;    1\n2\n3 --View:  SELECT   *   FROM   celebs ;    1\n2\n3 --Add a new column:  ALTER   TABLE   celebs   ADD   COLUMN   twitter_handle   TEXT ;    1\n2\n3 --View:  SELECT   *   FROM   celebs ;    1\n2\n3 --Update the table:  UPDATE   celebs   SET   twitter_handle   =   '@taylorswift13'   WHERE   id   =   4 ;    1\n2\n3 --View:  SELECT   *   FROM   celebs ;    1\n2\n3 --Delete rows. NULL for missing or unknown data:  DELETE   FROM   celebs   WHERE   twitter_handle   IS   NULL ;    1\n2\n3 --View:  SELECT   *   FROM   celebs ;",
            "title": "Manipulation"
        },
        {
            "location": "/Codecademy Learn SQL/#project-create-table",
            "text": "In this project you will create your own friends table and add and delete data from it.  The instructions provided are a general guideline, but feel free to add more columns, insert more data, or create more tables.  Notes: plan a project by drawing an ULM schema.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25 CREATE   TABLE   friends   ( id   INTEGER ,   name   TEXT ,   birthday   DATE );  SELECT   *   FROM   friends ;  DROP   tables   friends ;  CREATE   TABLE   friends   ( id   INTEGER ,   name   TEXT ,   birthday   DATE );  SHOW   tables ;  INSERT   INTO   friends   ( id ,   name ,   birthday )   VALUES   ( 1 , \"Jane Doe\" , '1993-05-19' );  INSERT   INTO   friends   ( id ,   name ,   birthday )   VALUES   ( 2 , \"Jade Donot\" , '1995-06-12' );  INSERT   INTO   friends   ( id ,   name ,   birthday )   VALUES   ( 3 , \"Jack Doom\" , '1990-10-01' );  INSERT   INTO   friends   ( id ,   name ,   birthday )   VALUES   ( 4 , \"John Doe\" , '1988-12-09' );  UPDATE   friends   SET   name   =   \"Jane Smith\"   WHERE   id   =   1 ;   ALTER   TABLE   friends   ADD   COLUMN   email   TEXT ;  UPDATE   friends   SET   email   =   \"jdoe@example.com\"   WHERE   id   =   1 ;  UPDATE   friends   SET   email   =   \"jade@example.com\"   WHERE   id   =   2 ;  UPDATE   friends   SET   email   =   \"doom@example.com\"   WHERE   id   =   3 ;  UPDATE   friends   SET   email   =   \"johndoe@example.com\"   WHERE   id   =   4 ;  DELETE   FROM   friends   WHERE   id   =   1 ;",
            "title": "Project Create Table"
        },
        {
            "location": "/Codecademy Learn SQL/#queries",
            "text": "One of the core purposes of the SQL language is to retrieve information stored in a database. This is commonly referred to as querying. Queries allow us to communicate with the database by asking questions and having the result set return data relevant to the question. In this lesson, you will be querying a database with one table named movies. Let\u2019s get started.  1\n2\n3 --Find rows:  SELECT   name ,   imdb_rating   FROM   movies ;    1\n2\n3 --Find rows. SELECT DISTINCT for unique values in the result set. It filters out all duplicate values:  SELECT   DISTINCT   genre   FROM   movies ;    1\n2\n3 --Find rows with WHERE:  SELECT   *   FROM   movies   WHERE   imdb_rating   >   8 ;    Clauses:   = , equals.  != , not equals.  > , greater than.  < , less than.  >= , greater than or equal to.  <= , less than or equal to.   1\n2\n3 --Find rows with LIKE. Se_en represents a pattern with a wildcard character. The _ means you can substitute any individual character here without breaking the pattern. The names Seven and Se7en both match this pattern:  SELECT   *   FROM   movies   WHERE   name   LIKE   'Se_en' ;    1\n2\n3\n4 --Find rows with LIKE. % is another wildcard character that can be used with LIKE. % is a wildcard character that matches zero or more missing letters in the pattern. A% matches all movies with names that begin with \"A\". %a matches all movies that end with \"a\":  SELECT   *   FROM   movies   WHERE   name   LIKE   'a%' ;  SELECT   *   FROM   movies   WHERE   name   LIKE   '%man%' ;    1\n2\n3\n4 --Find rows with BETWEEN. The values can be numbers, text or dates. Names that begin with letters \"A\" up to but not including \"J\". Years between 1990 up to and including 2000:  SELECT   *   FROM   movies   WHERE   name   BETWEEN   'A'   AND   'J' ;  SELECT   *   FROM   movies   WHERE   year   BETWEEN   1990   AND   2000 ;    1\n2\n3 --Find rows with BETWEEN, AND:  SELECT   *   FROM   movies   WHERE   year   BETWEEN   1990   AND   2000   AND   genre   =   'comedy' ;    1\n2\n3 --Find rows with BETWEEN, OR:  SELECT   *   FROM   movies   WHERE   genre   =   'comedy'   OR   year   <   1980 ;    1\n2\n3 --Find rows with BETWEEN, ORDERED BY, DESC, ASC:  SELECT   *   FROM   movies   ORDER   BY   imdb_rating   DESC ;    1\n2\n3 --Find rows, BETWEEN, ORDERED BY, ASC. LIMIT is a clause that lets you specify the maximum number of rows the result set will have:  SELECT   *   FROM   movies   ORDER   BY   imdb_rating   ASC   LIMIT   3 ;",
            "title": "Queries"
        },
        {
            "location": "/Codecademy Learn SQL/#project-writing-queries",
            "text": "In this project you will write queries to retrieve information from the movies table.  The instructions provided are a general guideline, but feel free to experiment writing your own queries.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77 --Return all of the unique years in the movies table.  SELECT   DISTINCT   year   FROM   movies ;  --Return all of the unique years in the movies table sorted from oldest to newest.  SELECT   DISTINCT   year   FROM   movies   ORDER   BY   year   ASC ;  --Return all movies that are dramas.  SELECT   *   FROM   movies   WHERE   genre   =   \"drama\" ;  --Return all of the movies with names that contain \"bride\".  SELECT   *   FROM   movies   WHERE   name   LIKE   '%Br%' ;  --Return all of the movies that were made between 2000 and 2015.  SELECT   *   FROM   movies   WHERE   year   >=   2000   AND   year   <=   2015 ;  --Return all of the movies that were made in 1995 or have an IMDb rating of 9.  SELECT   *   FROM   movies   WHERE   year   =   1995   OR   imdb_rating   =   9 ;  --Return the name and IMDb rating of every movie made after 2009 in alphabetical order.  SELECT   name ,   imdb_rating   FROM   movies   WHERE   year   >   2009   ORDER   BY   name   ASC ;  --Return 3 movies with an IMDb rating of 7.  SELECT   *   FROM   movies   WHERE   imdb_rating   =   7   LIMIT   3 ;  --Return 10 movies with an IMDb rating greater than 6, are comedies, were made after 1995, and sorted by IMDb rating from highest to lowest. (Hint: you can use AND more than once in a statement).  SELECT   *   FROM   movies   WHERE   imdb_rating   >   6   AND   genre   =   'comedy'   AND   year   >   1995   ORDER   BY   imdb_rating   ASC   LIMIT   10 ;  --Return all movies named 'Cast Away'.  SELECT   *   FROM   movies   WHERE   name   =   'Cast Away' ;  --Return all movies with an IMDb rating not equal to 7.  SELECT   *   FROM   movies   WHERE   imdb_rating   !=   7 ;  --Return all movies with a horror genre and an IMDb rating less than 6.  SELECT   *   FROM   movies   WHERE   genre   =   'horror'   AND   imdb_rating   <   6 ;  --Return 10 movies with an IMDb rating greater than 8 sorted by their genre.  SELECT   *   FROM   movies   WHERE   imdb_rating   >   8   ORDER   BY   genre   ASC   LIMIT   10 ;  --Return all movies that include 'King' in the name.  SELECT   *   FROM   movies   WHERE   name   LIKE   '%King%' ;  --Return all movies with names that end with the word 'Out'  SELECT   *   FROM   movies   WHERE   name   LIKE   '%Out' ;  --Return all movies with names that begin with the word \"The\" sorted by IMDb rating from highest to lowest  SELECT   *   FROM   movies   WHERE   name   LIKE   'The%'   ORDER   BY   imdb_rating   DESC ;  --Return all of the movies.  SELECT   *   FROM   movies ;  --Return the name and id of each movie with an id greater than 125.  SELECT   name ,   id   FROM   movies   WHERE   id   >   125 ;  --Return all movies with names that begin with 'X-Men'  SELECT   *   FROM   movies   WHERE   name   LIKE   'X-Men%' ;  --Return the first 10 movies sorted in reverse alphabetical order.  SELECT   *   FROM   movies   DESC   LIMIT   10 ;  --Return the id, name, and genre of all movies that are romances.  SELECT   id ,   name ,   genre   FROM   movies   WHERE   genre   =   'romance' ;  --Return all of the Twilight movies in order from the year they were released from oldest to newest.  SELECT   *   FROM   movies   WHERE   name   LIKE   '%Twilight%'   ORDER   BY   year   ASC ;  --Return all of the movies that were released in 2012 that are comedies.  SELECT   *   FROM   movies   WHERE   year   =   2012   AND   genre   =   'comedy' ;",
            "title": "Project Writing Queries"
        },
        {
            "location": "/Codecademy Learn SQL/#aggregate-functions",
            "text": "Aggregate functions compute a single result from a set of input values. For instance, when we need the sum or average of a particular column, we can use aggregate functions to quickly compute it for us. We will be learning about different aggregate functions in this lesson.  For this lesson we have given you a table named fake_apps which is made up of data for fake mobile applications.  1\n2\n3 --View:  SELECT   *   FROM   fake_apps ;    1\n2\n3 --Calculate the number of rows in a table. where the column is not NULL. Here, we want to count every row so we pass * as an argument:  SELECT   COUNT ( * )   FROM   fake_apps ;    1\n2\n3 --Count * or columns):  SELECT   COUNT ( * )   FROM   fake_apps   WHERE   price   =   0 ;    1\n2\n3 --Aggregate. GROUP BY is a clause in SQL that is only used with aggregate functions. It is used in collaboration with the SELECT statement to arrange identical data into groups. Here, our aggregate function is COUNT() and we are passing price as an argument to GROUP BY. SQL will count the total number of apps for each price in the table:  SELECT   price ,   COUNT ( * )   FROM   fake_apps   GROUP   BY   price ;    1\n2\n3 --It calculates the total number of apartments in each neighborhood in the cities table and groups them by neighborhood.  SELECT   neighborhood ,   SUM ( apartments )   FROM   cities   GROUP   BY   neighborhood ;    1\n2\n3 --Sum up, aggregate by category:  SELECT   category ,   SUM ( downloads )   FROM   fake_apps   GROUP   BY   category ;    1\n2\n3\n4\n5\n6 --Find the maximum, minimum, global, categorical:  SELECT   MAX ( downloads )   FROM   fake_apps ;  SELECT   name ,   category ,   MAX ( downloads )   FROM   fake_apps   GROUP   BY   category ;  SELECT   MIN ( downloads )   FROM   fake_apps ;  SELECT   name ,   category ,   MIN ( downloads )   FROM   fake_apps   GROUP   BY   category ;    1\n2\n3\n4 --It returns the title, genre and checkout count for the book with the most checkouts in the library_books table.  SELECT   title ,   genre ,   MAX ( checkouts )   FROM   library_books  GROUP   BY   genre ;    1\n2\n3\n4 --Find the average, or category average:  SELECT   AVG ( downloads )   FROM   fake_apps ;  SELECT   price ,   AVG ( downloads )   FROM   fake_apps   GROUP   BY   price ;    1\n2\n3 --Round up to x decimal(s):  SELECT   price ,   ROUND ( AVG ( downloads ),   2 )   FROM   fake_apps   GROUP   BY   price ;",
            "title": "Aggregate Functions"
        },
        {
            "location": "/Codecademy Learn SQL/#project-fake-apps",
            "text": "In this project you will write queries with aggregate functions to retrieve information from the fake_apps table.  The instructions provided are a general guideline, but feel free to experiment writing your own queries, creating your own tables, or adding your own apps to the existing table!   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79 --Return the total number of apps in the table fake_apps.  SELECT   COUNT ( * )   FROM   fake_apps ;  --Return the name, category, and price of the app that has been downloaded the least amount of times.  SELECT   name ,   category ,   price ,   MIN ( downloads )   FROM   fake_apps ;  --Return the total number of apps for each category.  SELECT   COUNT ( * )   FROM   fake_apps   GROUP   BY   category ;  --Return the name and category of the app that has been downloaded the most amount of times.  SELECT   name ,   category ,   MAX ( downloads )   FROM   fake_apps ;  --Return the name and category of the app that has been downloaded the least amount of times.  SELECT   name ,   category ,   MIN ( downloads )   FROM   fake_apps ;  --Return the average price for an app in each category.  SELECT   AVG ( price )   FROM   fake_apps   GROUP   BY   category ;  --Return the average price for an app in each category. Round the averages to two decimal places.  SELECT   ROUND ( AVG ( price ), 2 )   FROM   fake_apps   GROUP   BY   category ;  --Return the maximum price for an app.  SELECT   MAX ( price )   FROM   fake_apps ;  --Return the minimum number of downloads for an app.  SELECT   MIN ( downloads )   FROM   fake_apps ;  --Return the total number of downloads for apps that belong to the Games category.  SELECT   MIN ( downloads )   FROM   fake_apps   WHERE   category   =   'Games' ;  --Return the total number of apps that are free.  SELECT   COUNT ( * )   FROM   fake_apps   WHERE   price   =   0 ;  --Return the total number of apps that cost 14.99.  SELECT   COUNT ( * )   FROM   fake_apps   WHERE   price   =   14 . 99 ;  --Return the sum of the total number of downloads for apps that belong to the Music category.  SELECT   SUM ( downloads )   FROM   fake_apps   WHERE   category   =   'Music' ;  --Return the sum of the total number of downloads for apps that belong to the Business category.  SELECT   SUM ( downloads )   FROM   fake_apps   WHERE   category   =   'Business' ;  --Return the name of each category with the total number of apps that belong to it.  SELECT   name ,   COUNT ( * )   FROM   fake_apps   GROUP   BY   category ;  --Return the price and average number of downloads grouped by price.  SELECT   price ,   AVG ( downloads )   FROM   fake_apps   GROUP   BY   price ;  --Return the price and average number of downloads grouped by price. Round the averages to the nearest integer.  SELECT   ROUND ( price , 0 ),   AVG ( downloads )   FROM   fake_apps   GROUP   BY   price ;  --Return the name and category and price of the most expensive app for each category.  SELECT   name ,   category ,   price ,   MAX ( price )   FROM   fake_apps   GROUP   BY   category ;  --Return the total number of apps whose name begin with the letter 'A'.  SELECT   COUNT ( * )   FROM   fake_apps   WHERE   name   LIKE   'A%' ;  --Return the total number of downloads for apps belonging to the Sports or Health & Fitness category.  SELECT   SUM ( downloads )   FROM   fake_apps   WHERE   category   =   'Sports'   OR   category   =   'Health & Fitness' ;",
            "title": "Project Fake Apps"
        },
        {
            "location": "/Codecademy Learn SQL/#multiple-tables",
            "text": "Most of the time, data is distributed across multiple tables in the database. Imagine a database with two tables, artists and albums. An artist can produce many different albums, and an album is produced by an artist.  The data in these tables are related to each other. Through SQL, we can write queries that combine data from multiple tables that are related to one another. This is one of the most powerful features of relational databases.  1\n2\n3\n4\n5\n6\n7\n8 --We have created a table named albums for you. Create a second table named artists. Using the CREATE TABLE statement we added a PRIMARY KEY to the id column. A primary key serves as a unique identifier for each row or record in a given table. The primary key is literally an id value for a record. We're going to use this value to connect artists to the albums they have produced. By specifying that the id column is the PRIMARY KEY, SQL makes sure that none of the values in this column are NULL and each value in this column is unique:  CREATE   TABLE   artists ( id   INTEGER   PRIMARY   KEY ,   name   TEXT );  --View both tables:  SELECT   *   FROM   albums ;  SELECT   *   FROM   artists ;    1\n2\n3 --Query:  SELECT   *   FROM   artists   WHERE   id   =   3 ;    1\n2\n3 --A foreign key is a column that contains the primary key of another table in the database. We use foreign keys and primary keys to connect rows in two different tables. One table's foreign key holds the value of another table's primary key. Unlike primary keys, foreign keys do not need to be unique and can be NULL. Here, artist_id is a foreign key in the albums table. We can see that Michael Jackson has an id of 3 in the artists table. All of the albums by Michael Jackson also have a 3 in the artist_id column in the albums table. This is how SQL is linking data between the two tables. The relationship between the artists table and the albums table is the id value of the artists.  SELECT   *   FROM   albums   WHERE   artist_id   =   3 ;    1\n2\n3 --One way to query multiple tables is to write a SELECT statement with multiple table names separated by a comma. This is also known as a CROSS JOIN. Here, albums and artists are the different tables we are querying. When querying more than one table, column names need to be specified by table_name.column_name. Here, the result set includes the name and year columns from the albums table and the name column from the artists table. Unfortunately the result of this cross join is not very useful. It combines every row of the artists table with every row of the albums table. It would be more useful to only combine the rows where the album was created by the artist.  SELECT   albums . name ,   albums . year ,   artists . name   FROM   albums ,   artists ;    1\n2\n3 --In SQL, joins are used to combine rows from two or more tables. The most common type of join in SQL is an INNER JOIN. An inner join will combine rows from different tables if the join condition is true. Let's look at the syntax to see how it works. SELECT * specifies the columns our result set will have. Here, we want to include every column in both tables. FROM albums specifies the first table we are querying. JOIN artists ON specifies the type of join we are going to use as well as the name of the second table. Here, we want to do an inner join and the second table we want to query is artists. albums.artist_id = artists.id is the join condition that describes how the two tables are related to each other. Here, SQL uses the foreign key column artist_id in the albums table to match it with exactly one row in the artists table with the same value in the id column. We know it will only match one row in the artists table because id is the PRIMARY KEY of artists.  SELECT   *   FROM   albums   JOIN   artists   ON   albums . artist_id   =   artists . id ;    1\n2\n3 --OUTER JOINS (LEFT or RIGHT) also combine rows from two or more tables, but unlike inner joins, they do not require the join condition to be met. Instead, every row in the left table is returned in the result set, and if the join condition is not met, then NULL values are used to fill in the columns from the right table. The left table is simply the first table that appears in the statement. Here, the left table is albums. Likewise, the right table is the second table that appears. Here, artists is the right table.  SELECT   *   FROM   albums   LEFT   JOIN   artists   ON   albums . artist_id   =   artists . id ;    1\n2\n3 --AS is a keyword in SQL that allows you to rename a column or table using an alias. The new name can be anything you want as long as you put it inside of single quotes. Here we want to rename the albums.name column as 'Album', and the artists.name column as 'Artist'. It is important to note that the columns have not been renamed in either table. The aliases only appear in the result set.  SELECT   albums . name ,   albums . year ,   artists . name   FROM   albums   JOIN   artists   ON   albums . artist_id   =   artists . id   WHERE   albums . year   >   1980 ;    1 SELECT   albums . name   AS   'Album' ,   albums . year ,   artists . name   AS   'Artist'   FROM   albums   JOIN   artists   ON   albums . artist_id   =   artists . id   WHERE   albums . year   >   1980 ;",
            "title": "Multiple Tables"
        },
        {
            "location": "/Codecademy Learn SQL/#recap",
            "text": "Primary Key is a column that serves a unique identifier for row in the table. Values in this column must be unique and cannot be  NULL .  Foreign Key is a column that contains the primary key to another table in the database. It is used to identify a particular row in the referenced table.  Joins are used in SQL to combine data from multiple tables.  INNER JOIN  will combine rows from different tables if the join condition is true.  LEFT OUTER JOIN  will return every row in the left table, and if the join condition is not met,  NULL  values are used to fill in the columns from the right table.  AS  is a keyword in SQL that allows you to rename a column or table in the result set using an alias.",
            "title": "Recap"
        },
        {
            "location": "/Codecademy Learn SQL/#project-querying-tables",
            "text": "In this project you will practice querying multiple tables using joins.  The instructions provided are a general guideline, but feel free to experiment creating your own tables, adding data, and writing more queries.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30 --Create a table named tracks with an id, title, and album_id column. The id column should be the PRIMARY KEY.  CREATE   TABLE   tracks   ( id   INTEGER   PRIMARY   KEY ,   title   TEXT ,   albums_id   INTEGER );  --\"Smooth Criminal\" is a track from Michael Jackson's \"Bad\" album. Add this track to the database.  INSERT   INTO   tracks   ( id , title , albums_id )   VALUES   ( 1 , \"Smooth Criminal\" ,   8 );  --Add more tracks to the database.  INSERT   INTO   tracks   ( id , title , albums_id )   VALUES   ( 2 , \"aaa\" ,   1 );  INSERT   INTO   tracks   ( id , title , albums_id )   VALUES   ( 3 , \"bbb\" ,   1 );  INSERT   INTO   tracks   ( id , title , albums_id )   VALUES   ( 4 , \"ccc\" ,   8 );  INSERT   INTO   tracks   ( id , title , albums_id )   VALUES   ( 5 , \"ddd\" ,   8 );  INSERT   INTO   tracks   ( id , title , albums_id )   VALUES   ( 6 , \"eee\" ,   8 );  --Combine the albums and tracks tables using an INNER JOIN. Order the query by album_id.  SELECT   *   FROM   albums   JOIN   tracks   ON   albums . artist_id   =   tracks . id ;  --Combine the albums and artists table using a LEFT OUTER JOIN. Let albums be the left table.  SELECT   *   FROM   albums   LEFT   JOIN   artists   ON   albums . artist_id   =   artists . id ;  --Combine the albums and artists table using a LEFT OUTER JOIN. Let artists be the left table.  SELECT   *   FROM   artists   LEFT   JOIN   albums   ON   albums . artist_id   =   artists . id ;  --Use any join you like to combine the albums and tracks table. Rename the album_id column to Albums.  SELECT   *   FROM   albums   LEFT   JOIN   tracks   ON   albums . artist_id   =   tracks . id ;",
            "title": "Project Querying Tables"
        },
        {
            "location": "/Codecademy SQL, Analyzing Business Metrics/",
            "text": "Foreword\n\n\nCode snippets. With SQLite. From Codecademy in collaboration with \nPeriscope Data\n.\n\n\n\n\nSpecifying Comments\n\u00b6\n\n\n\n\nLine comment. This is indicated by two negative signs. The remainder of the text on the line is the comment.\n\n\nBlock comment. The start of the block comment is indicated by \n/*\n, the end of the comment by the same string. A block comment can cover text in part of a line, or can span multiple lines.\n\n\nRem\n or \n@\n. For Oracle, a line starting with either REM or @ is a comment line.\n\n\n\n\nAvanced Aggregates\n\u00b6\n\n\nAt the heart of every great business decision is data. Since most businesses store critical data in SQL databases, a deep understanding of SQL is a necessary skill for every data analyst.\n\n\nChief among data analysis tasks is data aggregation, the grouping of data to express in summary form. We\u2019ll be working with SpeedySpoon, a meal delivery app. The folks at SpeedySpoon have asked us to look into their deliveries and help them optimize their process.\n\n\nThis course was developed in partnership with our good friends at Periscope Data. If you\u2019re new to SQL, we recommend you do this course first.\n\n\nComplete each query by replacing the comments \n/**/\n with SQL code.\n\n\nWe\u2019ll start by looking at SpeedySpoon\u2019s data. The orders table has a row for each order of a SpeedySpoon delivery. It says when the order took place, and who ordered it.\n\n\nAdd code to the select statement to select all columns in the orders table.\n\n\n1\n2\n3\n4\nselect\n \n*\n\n\nfrom\n \n/**/\n\n\norder\n \nby\n \nid\n\n\nlimit\n \n100\n;\n\n\n\n\n\n\n\nNote that the order and limit clauses keep the data organized.\n\n\n1\nSELECT\n \n*\n \nFROM\n \norders\n \nORDER\n \nBY\n \nid\n \nLIMIT\n \n100\n;\n\n\n\n\n\n\n\nThe order_items table lists the individual foods and their prices in each order. Complete the query to select all columns in the order_items table.\n\n\n1\nSELECT\n \n*\n \nFROM\n \norder_items\n \nORDER\n \nBY\n \nid\n \nLIMIT\n \n100\n;\n\n\n\n\n\n\n\nNow that we have a good handle on our data, let\u2019s dive into some common business queries. We\u2019ll begin with the Daily Count of orders placed. To make our Daily Count metric, we\u2019ll focus on the date function and the ordered_at field.\n\n\nTo get the Daily Metrics we need the date. Most dates in databases are timestamps, which have hours and minutes, as well as the year, month, and day. Timestamps look like \n2015-01-05 14:43:31\n, while dates are the first part: \n2015-01-05\n.\n\n\nWe can easily select the date an item was ordered at with the date function and the ordered_at field:\n\n\n1\n2\nSELECT\n \ndate\n(\nordered_at\n)\n\n\nFROM\n \norders\n;\n\n\n\n\n\n\n\nLet\u2019s get a Daily Count of orders from the orders table. Complete the query using the date function to cast the timestamps in ordered_at to dates.\n\n\n1\n2\n3\n4\n\u200b\nSELECT\n \ndate\n(\nordered_at\n)\n\n\nFROM\n \norders\n\n\nORDER\n \nBY\n \n1\n\n\nLIMIT\n \n100\n;\n\n\n\n\n\n\n\nThe order by 1 statement is a shortcut for order by date(ordered_at). The 1 refers to the first column.\n\n\nNow that we can convert timestamps to dates, we can count the Orders Per Date. Generally, when we count all records in a table we run a query with the count function, as follows:\n\n\n1\n2\nSELECT\n \ncount\n(\n1\n)\n\n\nFROM\n \nusers\n;\n\n\n\n\n\n\n\nThis will treat all rows as a single group, and return one row in the result set - the total count.\n\n\nTo count orders by their dates, we\u2019ll use the date and count functions and pair them with the group by clause. Together this will count the records in the table, grouped by date.\n\n\nFor example, to see the user records counted by the date created, we use the date and count functions and group by clause as follows:\n\n\n1\n2\n3\nSELECT\n \ndate\n(\ncreated_at\n),\n \ncount\n(\n1\n)\n\n\nFROM\n \nusers\n\n\nGROUP\n \nBY\n \ndate\n(\ncreated_at\n)\n\n\n\n\n\n\n\nUse the date and count functions and group by clause to count and group the orders by the dates they were ordered_at.\n\n\n1\n2\n3\n4\nSELECT\n \ndate\n(\nordered_at\n),\n \ncount\n(\n1\n)\n\n\nFROM\n \norders\n\n\nGROUP\n \nBY\n \n1\n\n\nORDER\n \nBY\n \n1\n;\n\n\n\n\n\n\n\nGroup before ordering\n\n\nWe have the Daily Count of orders, but what we really want to know is revenue. How much money has SpeedySpoon made from orders each day?\n\n\nWe can make a few changes to our Daily Count query to get the revenue.\n\n\nFirst, instead of using count(1) to count the rows per date, we\u2019ll use round(sum(amount_paid), 2) to add up the revenue per date. Complete the query by adding revenue per date.\n\n\nSecond, we need to join in the order_items table because that table has an amount_paid column representing revenue. Complete the query by adding a join clause where orders.id = order_items.order_id.\n\n\nNote that the round function rounds decimals to digits, based on the number passed in. Here round(\u2026, 2) rounds the sum paid to two digits.\n\n\n1\n2\n3\n4\n5\nSELECT\n \ndate\n(\nordered_at\n),\n \nround\n(\nsum\n(\namount_paid\n),\n \n2\n)\n\n\nFROM\n \norders\n \nJOIN\n \norder_items\n \nON\n \n    \norders\n.\nid\n \n=\n \norder_items\n.\norder_id\n\n\nGROUP\n \nBY\n \n1\n\n\nORDER\n \nBY\n \n1\n;\n\n\n\n\n\n\n\nNow with a small change, we can find out how much we\u2019re making per day for any single dish. What\u2019s the daily revenue from customers ordering kale smoothies? Complete the query by using a where clause to filter the daily sums down to orders where the name = \u2018kale-smoothie\u2019.\n\n\n1\n2\n3\n4\n5\n6\nSELECT\n \ndate\n(\nordered_at\n),\n \nround\n(\nsum\n(\namount_paid\n),\n \n2\n)\n\n\nFROM\n \norders\n \nJOIN\n \norder_items\n \nON\n \n    \norders\n.\nid\n \n=\n \norder_items\n.\norder_id\n\n\nWHERE\n \nname\n \n=\n \n'kale-smoothie'\n\n\nGROUP\n \nBY\n \n1\n\n\nORDER\n \nBY\n \n1\n;\n\n\n\n\n\n\n\nIt looks like the smoothies might not be performing well, but to be sure we need to see how they\u2019re doing in the context of the other order items. We\u2019ll look at the data several different ways, the first of which is determining what percent of revenue these smoothies represent.\n\n\nTo get the percent of revenue that each item represents, we need to know the total revenue of each item. We will later divide the per-item total with the overall total revenue.\n\n\nThe following query groups and sum the products by price to get the total revenue for each item. Complete the query by passing in sum(amount_paid) into the round function and rounding to two decimal places.\n\n\n1\n2\n3\n4\nSELECT\n \nname\n,\n \nround\n(\nsum\n(\namount_paid\n),\n \n2\n)\n\n\nFROM\n \norder_items\n\n\nGROUP\n \nBY\n \nname\n\n\nORDER\n \nBY\n \n2\n \nDESC\n;\n\n\n\n\n\n\n\nWe have the sum of the the products by revenue, but we still need the percent. For that, we\u2019ll need to get the total using a subquery. A subquery can perform complicated calculations and create filtered or aggregate tables on the fly.\n\n\nSubqueries are useful when you want to perform an aggregation outside the context of the current query. This will let us calculate the overall total and per-item total at the same time.\n\n\nComplete the denominator in the subquery, which is the total revenue from order_items. Use the sum function to query the amount_paid from the order_items table.\n\n\nWe now have the percent or revenue each product represents!\n\n\n1\n2\n3\n4\n5\nSELECT\n \nname\n,\n \nround\n(\nsum\n(\namount_paid\n)\n \n/\n \n    \n(\nSELECT\n \nsum\n(\namount_paid\n)\n \nFROM\n \norder_items\n)\n \n*\n \n100\n.\n0\n,\n \n2\n)\n\n\nFROM\n \norder_items\n\n\nGROUP\n \nBY\n \n1\n\n\nORDER\n \nBY\n \n2\n \nDESC\n;\n\n\n\n\n\n\n\nHere order by 2 desc sorts by the second column (the percent) to show the products in order of their contribution to revenue.\n\n\nAs we suspected, kale smoothies are not bringing in the money. And thanks to this analysis, we found what might be a trend - several of the other low performing products are also smoothies. Let\u2019s keep digging to find out what\u2019s going on with these smoothies.\n\n\nTo see if our smoothie suspicion has merit, let\u2019s look at purchases by category. We can group the order items by what type of food they are, and go from there. Since our order_items table does not include categories already, we\u2019ll need to make some!\n\n\nPreviously we\u2019ve been using group by with a column (like order_items.name) or a function (like date(orders.ordered_at)).\n\n\nWe can also use group by with expressions. In this case a case statement is just what we need to build our own categories. case statements are similar to if/else in other languages. Here\u2019s the basic structure of a case statement:\n\n\n1\n2\n3\n4\n5\nCASE\n \n{\ncondition\n}\n\n    \nWHEN\n \n{\nvalue1\n}\n \nTHEN\n \n{\nresult1\n}\n\n    \nWHEN\n \n{\nvalue2\n}\n \nTHEN\n \n{\nresult2\n}\n\n    \nELSE\n \n{\nresult3\n}\n\n\nEND\n\n\n\n\n\n\n\nWe\u2019ll build our own categories using a case statement. Complete the query below with a case condition of name that lists out each product, and decides its group.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nSELECT\n \n*\n,\n\n    \nCASE\n \nname\n\n        \nWHEN\n \n'kale-smoothie'\n    \nTHEN\n \n'smoothie'\n\n        \nWHEN\n \n'banana-smoothie'\n  \nTHEN\n \n'smoothie'\n\n        \nWHEN\n \n'orange-juice'\n     \nTHEN\n \n'drink'\n\n        \nWHEN\n \n'soda'\n             \nTHEN\n \n'drink'\n\n        \nWHEN\n \n'blt'\n              \nTHEN\n \n'sandwich'\n\n        \nWHEN\n \n'grilled-cheese'\n   \nTHEN\n \n'sandwich'\n\n        \nWHEN\n \n'tikka-masala'\n     \nTHEN\n \n'dinner'\n\n        \nWHEN\n \n'chicken-parm'\n     \nTHEN\n \n'dinner'\n\n        \nELSE\n \n'other'\n\n    \nEND\n \nAS\n \ncategory\n\n\nFROM\n \norder_items\n\n\nORDER\n \nBY\n \nid\n\n\nLIMIT\n \n100\n;\n\n\n\n\n\n\n\nComplete the query by using the category column created by the case statement in our previous revenue percent calculation. Add the denominator that will sum the amount_paid.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nSELECT\n\n    \nCASE\n \nname\n\n        \nWHEN\n \n'kale-smoothie'\n    \nTHEN\n \n'smoothie'\n\n        \nWHEN\n \n'banana-smoothie'\n  \nTHEN\n \n'smoothie'\n\n        \nWHEN\n \n'orange-juice'\n     \nTHEN\n \n'drink'\n\n        \nWHEN\n \n'soda'\n             \nTHEN\n \n'drink'\n\n        \nWHEN\n \n'blt'\n              \nTHEN\n \n'sandwich'\n\n        \nWHEN\n \n'grilled-cheese'\n   \nTHEN\n \n'sandwich'\n\n        \nWHEN\n \n'tikka-masala'\n     \nTHEN\n \n'dinner'\n\n        \nWHEN\n \n'chicken-parm'\n     \nTHEN\n \n'dinner'\n\n        \nELSE\n \n'other'\n\n    \nEND\n \nAS\n \ncategory\n,\n \nround\n(\n1\n.\n0\n \n*\n \nsum\n(\namount_paid\n)\n \n/\n \n(\nSELECT\n \nsum\n(\namount_paid\n)\n \nFROM\n \norder_items\n)\n \n*\n \n100\n,\n \n2\n)\n \nAS\n \npct\n\n\nFROM\n \norder_items\n\n\nGROUP\n \nBY\n \n1\n\n\nORDER\n \nBY\n \n2\n \nDESC\n;\n\n\n\n\n\n\n\nHere 1.0 * is a shortcut to ensure the database represents the percent as a decimal.\n\n\nIt\u2019s true that the whole smoothie category is performing poorly compared to the others. We\u2019ll certainly take this discovery to SpeedySpoon. Before we do, let\u2019s go one level deeper and figure out why.\n\n\nWhile we do know that kale smoothies (and drinks overall) are not driving a lot of revenue, we don\u2019t know why. A big part of data analysis is implementing your own metrics to get information out of the piles of data in your database.\n\n\nIn our case, the reason could be that no one likes kale, but it could be something else entirely. To find out, we\u2019ll create a metric called reorder rate and see how that compares to the other products at SpeedySpoon.\n\n\nWe\u2019ll define reorder rate as the ratio of the total number of orders to the number of people making those orders. A lower ratio means most of the orders are reorders. A higher ratio means more of the orders are first purchases.\n\n\nLet\u2019s calculate the reorder ratio for all of SpeedySpoon\u2019s products and take a look at the results. Counting the total orders per product is straightforward. We count the distinct order_ids (different) in the order_items table.\n\n\nComplete the query by passing in the distinct keyword and the order_id column name into the count function\n\n\nHere\u2019s a hint on how to use the count function to count distinct columns in a table.\n\n\n1\n2\n3\n4\n5\n6\n7\nSELECT\n \nDISTINCT\n \ncolumn_name\n \nFROM\n \ntable_name\n;\n\n\nSELECT\n \ncolumn_name\n \nFROM\n \ntable_name\n \nGROUP\n \nBY\n \ncolumn_name\n;\n\n\n\nSELECT\n \nname\n,\n \ncount\n(\nDISTINCT\n \norder_id\n)\n\n\nFROM\n \norder_items\n\n\nGROUP\n \nBY\n \n1\n\n\nORDER\n \nBY\n \n1\n;\n\n\n\n\n\n\n\nNow we need the number of people making these orders.\n\n\nTo get that information, we need to join in the orders table and count unique values in the delivered_to field, and sort by the reorder_rate.\n\n\nComplete the query below. The numerator should count the distinct order_ids. The denominator should count the distinct values of the orders table\u2019s delivered_to field (orders.delivered_to).\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nSELECT\n \nname\n,\n \nround\n(\n1\n.\n0\n \n*\n \n\ncount\n(\nDISTINCT\n \norder_id\n)\n \n/\n\n    \ncount\n(\nDISTINCT\n \ndelivered_to\n),\n \n2\n)\n \nAS\n \nreorder_rate\n\n\nFROM\n \norder_items\n\n    \nJOIN\n \norders\n \nON\n\n        \norders\n.\nid\n \n=\n \norder_items\n.\norder_id\n\n\nGROUP\n \nBY\n \n1\n\n\nORDER\n \nBY\n \n2\n \nDESC\n;\n\n\n\n\n\n\n\nThat\u2019s unexpected. While smoothies aren\u2019t making a lot of money for SpeedySpoon, they have a very high reorder rate. That means these smoothie customers are strong repeat customers.\n\n\nInstead of recommending smoothies be taken off the menu, we should talk to the smoothie customers and see what they like so much about these smoothies. There could be an opportunity here to expand the product line, or get other customers as excited as these kale fanatics. Nice work!\n\n\nLet\u2019s generalize what we\u2019ve learned so far:\n\n\n\n\n\n\nData aggregation is the grouping of data in summary form.\n\n\n\n\n\n\nDaily Count is the count of orders in a day.\n\n\n\n\n\n\nDaily Revenue Count is the revenue on orders per day.\n\n\n\n\n\n\nProduct Sum is the total revenue of a product.\n\n\n\n\n\n\nSubqueries can be used to perform complicated calculations and create filtered or aggregate tables on the fly.\n\n\n\n\n\n\nReorder Rate is the ratio of the total number of orders to the number of people making orders.\n\n\n\n\n\n\nCommon Metrics\n\u00b6\n\n\nAs a data scientist, when you\u2019re not investigating spikes or dips in your data, you might be building dashboards of KPIs, or key performance indicators for a company.\n\n\nKPIs are often displayed on TVs on the walls of the office, and serve as high level health metrics for the business. While every company\u2019s metrics are defined slightly differently, the basics are usually very similar.\n\n\nIn this lesson we\u2019ll take a look at basic KPIs like Daily Revenue, Daily Active Users, ARPU, and Retention for a video game, Mineblocks.\n\n\nThis company has two tables, gameplays and purchases. The purchases table lists all purchases made by players while they\u2019re playing Mineblocks. Complete the query to select from purchases.\n\n\n1\nSELECT\n \n*\n \nFROM\n \npurchases\n \nORDER\n \nBY\n \nid\n \nLIMIT\n \n10\n;\n\n\n\n\n\n\n\nThe gameplays table lists the date and platform for each session a user plays. Select from gameplays.\n\n\n1\nSELECT\n \n*\n \nFROM\n \ngameplays\n \nORDER\n \nBY\n \nid\n \nLIMIT\n \n10\n;\n\n\n\n\n\n\n\nAt the heart of every company is revenue, and Mineblocks is no exception. For our first KPI we\u2019ll calculate daily revenue.\n\n\nDaily Revenue is simply the sum of money made per day.\n\n\nTo get close to Daily Revenue, we calculate the daily sum of the prices in the purchases table. Complete the query by using the sum function and passing the price column from the purchases table. \n\n\n1\n2\n3\n4\n5\n6\nSELECT\n \n    \ndate\n(\ncreated_at\n),\n\n    \nround\n(\nsum\n(\nprice\n),\n2\n)\n\n\nFROM\n \npurchases\n\n\nGROUP\n \nBY\n \n1\n\n\nORDER\n \nBY\n \n1\n;\n\n\n\n\n\n\n\nUpdate our daily revenue query to exclude refunds. Complete the query by filtering for refunded_at is not null. Update our daily revenue query to exclude refunds. Complete the query by filtering for refunded_at is not null.\n\n\nMineblocks is a game, and one of the core metrics to any game is the number people who play each day. That KPI is called Daily Active Users, or DAU.\n\n\nDAU is defined as the number of unique players seen in-game each day. It\u2019s important not to double count users who played multiple times, so we\u2019ll use distinct in our count function.\n\n\nLikewise, Weekly Active Users (WAU) and Monthly Active Users (MAU) are in the same family.\n\n\nFor Mineblocks, we\u2019ll use the gameplays table to calculate DAU. Each time a user plays the game, their session is recorded in gameplays. Thus, a distinct count of users per day from gameplays will give us DAU.\n\n\nCalculate Daily Active Users for Mineblocks. Complete the query\u2019s count function by passing in the distinct keyword and the user_id column name.\n\n\n1\n2\n3\n4\n5\n6\nSELECT\n\n    \ndate\n(\ncreated_at\n),\n \n    \nCOUNT\n(\nDISTINCT\n \nuser_id\n)\n \nAS\n \ndau\n\n\nFROM\n \ngameplays\n\n\nGROUP\n \nBY\n \n1\n\n\nORDER\n \nBY\n \n1\n;\n\n\n\n\n\n\n\nSince Mineblocks is on multiple platforms, we can calculate DAU per-platform.\n\n\nPreviously we calculated DAU only per day, so the output we wanted was [date, dau_count]. Now we want DAU per platform, making the desired output [date, platform, dau_count].\n\n\nCalculate DAU for Mineblocks per-platform. Complete the query below. You will need to select the platform column and add a count function by passing in the distinct keyword and the user_id column name.\n\n\n1\n2\n3\n4\n5\n6\n7\nSELECT\n\n    \ndate\n(\ncreated_at\n),\n \n    \nplatform\n,\n\n    \nCOUNT\n(\nDISTINCT\n \nuser_id\n)\n \nAS\n \ndau\n\n\nFROM\n \ngameplays\n\n\nGROUP\n \nBY\n \n1\n,\n \n2\n\n\nORDER\n \nBY\n \n1\n,\n \n2\n;\n\n\n\n\n\n\n\ngroup by 1 (date), 2 (platform)\n\n order by 1 (date), 2 (platform)\n\n\nWe\u2019ve looked at DAU and Daily Revenue in Mineblocks. Now we must understand the purchasing habits of our users.\n\n\nMineblocks, like every freemium game, has two types of users:\n\n\n1\n2\npurchasers\n:\n \nusers\n \nwho\n \nhave\n \nbought\n \nthings\n \nin\n \nthe\n \ngame\n\n\nplayers\n:\n \nusers\n \nwho\n \nplay\n \nthe\n \ngame\n \nbut\n \nhave\n \nnot\n \nyet\n \npurchased\n\n\n\n\n\n\n\nThe next KPI we\u2019ll look at Daily ARPPU - Average Revenue Per Purchasing User. This metric shows if the average amount of money spent by purchasers is going up over time.\n\n\nDaily ARPPU is defined as the sum of revenue divided by the number of purchasers per day.\n\n\nTo get Daily ARPPU, modify the daily revenue query from earlier to divide by the number of purchasers.\n\n\nComplete the query by adding a numerator and a denominator. The numerator will display daily revenue, or sum the price columns. The denominator will display the number of purchasers by passing the distinct keyword and the user_id column name into the count function.\n\n\n1\n2\n3\n4\n5\n6\n7\nSELECT\n\n    \ndate\n(\ncreated_at\n),\n \n    \nround\n(\nsum\n(\nprice\n)\n \n/\n \nCOUNT\n(\nDISTINCT\n \nuser_id\n),\n \n2\n)\n \nAS\n \narppu\n\n\nFROM\n \npurchases\n\n\nWHERE\n \nrefunded_at\n \nIS\n \nNULL\n\n\nGROUP\n \nBY\n \n1\n\n\nORDER\n \nBY\n \n1\n;\n\n\n\n\n\n\n\nThe more popular (and difficult) cousin to Daily ARPPU is Daily ARPU, Average Revenue Per User. ARPU measures the average amount of money we\u2019re getting across all players, whether or not they\u2019ve purchased.\n\n\nARPPU increases if purchasers are spending more money. ARPU increases if more players are choosing to purchase, even if the purchase size stays consistent.\n\n\nNo one metric can tell the whole story. That\u2019s why it\u2019s so helpful to have many KPIs on the same dashboard.\n\n\nDaily ARPU is defined as revenue divided by the number of players, per-day. To get that, we\u2019ll need to calculate the daily revenue and daily active users separately, and then join them on their dates.\n\n\nOne way to easily create and organize temporary results in a query is with CTEs, Common Table Expressions, also known as with clauses. The with clauses make it easy to define and use results in a more organized way than subqueries. These clauses usually look like this:\n\n\n1\n2\n3\n4\n5\n6\nWITH\n \n{\nsubquery_name\n}\n \nAS\n \n(\n\n    \n{\nsubquery_body\n}\n\n\n)\n\n\nSELECT\n \n...\n\n\nFROM\n \n{\nsubquery_name\n}\n\n\nWHERE\n \n...\n\n\n\n\n\n\n\nUse a with clause to define daily_revenue and then select from it.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\ndaily_revenue\n \nAS\n \n(\n\n    \nSELECT\n\n        \ndate\n(\ncreated_at\n)\n \nAS\n \ndt\n,\n\n        \nround\n(\nsum\n(\nprice\n),\n \n2\n)\n \nAS\n \nrev\n\n    \nFROM\n \npurchases\n\n    \nWHERE\n \nrefunded_at\n \nIS\n \nNULL\n\n    \nGROUP\n \nBY\n \n1\n\n\n)\n\n\nSELECT\n \n*\n \nFROM\n \ndaily_revenue\n \nORDER\n \nBY\n \ndt\n;\n\n\n\n\n\n\n\nUse a with clause to define daily_revenue and then select from it.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nWITH\n \ndaily_revenue\n \nAS\n \n(\n\n    \nSELECT\n\n        \ndate\n(\ncreated_at\n)\n \nAS\n \ndt\n,\n\n        \nround\n(\nsum\n(\nprice\n),\n \n2\n)\n \nAS\n \nrev\n\n    \nFROM\n \npurchases\n\n    \nWHERE\n \nrefunded_at\n \nIS\n \nNULL\n\n    \nGROUP\n \nBY\n \n1\n\n\n)\n\n\nSELECT\n \n*\n \nFROM\n \ndaily_revenue\n \nORDER\n \nBY\n \ndt\n;\n\n\n\n\n\n\n\nNow you\u2019re familiar with using the with clause to create temporary result sets.\n\n\nYou just built the first part of ARPU, daily_revenue. From here we can build the second half of ARPU in our with clause, daily_players, and use both together to create ARPU.\n\n\nBuilding on this CTE, we can add in DAU from earlier. Complete the query by calling the DAU query we created earlier, now aliased as daily_players:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nWITH\n \ndaily_revenue\n \nAS\n \n(\n\n    \nSELECT\n\n        \ndate\n(\ncreated_at\n)\n \nAS\n \ndt\n,\n\n        \nround\n(\nsum\n(\nprice\n),\n \n2\n)\n \nAS\n \nrev\n\n    \nFROM\n \npurchases\n\n    \nWHERE\n \nrefunded_at\n \nIS\n \nNULL\n\n    \nGROUP\n \nBY\n \n1\n\n\n),\n\n\ndaily_players\n \nAS\n \n(\n\n    \nSELECT\n\n        \ndate\n(\ncreated_at\n)\n \nAS\n \ndt\n,\n\n        \nCOUNT\n(\nDISTINCT\n \nuser_id\n)\n \nAS\n \nplayers\n\n    \nFROM\n \ngameplays\n\n    \nGROUP\n \nBY\n \n1\n\n\n)\n\n\nSELECT\n \n*\n \nFROM\n \ndaily_players\n\n\nORDER\n \nBY\n \ndt\n;\n\n\n\n\n\n\n\nNow that we have the revenue and DAU, join them on their dates and calculate daily ARPU. Complete the query by adding the keyword using in the join clause.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nWITH\n \ndaily_revenue\n \nAS\n \n(\n\n    \nSELECT\n\n        \ndate\n(\ncreated_at\n)\n \nAS\n \ndt\n,\n\n        \nround\n(\nsum\n(\nprice\n),\n \n2\n)\n \nAS\n \nrev\n\n    \nFROM\n \npurchases\n\n    \nWHERE\n \nrefunded_at\n \nIS\n \nNULL\n\n    \nGROUP\n \nBY\n \n1\n\n\n),\n\n\ndaily_players\n \nAS\n \n(\n\n    \nSELECT\n\n        \ndate\n(\ncreated_at\n)\n \nAS\n \ndt\n,\n\n        \nCOUNT\n(\nDISTINCT\n \nuser_id\n)\n \nAS\n \nplayers\n\n    \nFROM\n \ngameplays\n\n    \nGROUP\n \nBY\n \n1\n\n\n)\n\n\nSELECT\n \n    \ndaily_revenue\n.\ndt\n,\n\n    \ndaily_revenue\n.\nrev\n \n/\n\n    \ndaily_players\n.\nplayers\n\n\nFROM\n \ndaily_players\n\n\nJOIN\n \ndaily_players\n \nUSING\n \n(\ndt\n);\n\n\n\n\n\n\n\nIn the final select statement, daily_revenue.dt represents the date, while daily_revenue.rev / daily_players.players is the daily revenue divided by the number of players that day. In full, it represents how much the company is making per player, per day.\n\n\nIn our ARPU query, we used using instead of on in the join clause. This is a special case join.\n\n\n1\nFROM\n \ndaily_revenue\n \nJOIN\n \ndaily_players\n \nUSING\n \n(\ndt\n);\n\n\n\n\n\n\n\nWhen the columns to join have the same name in both tables you can use USING instead of on. Our use of the using keyword is in this case equivalent to this clause:\n\n\n1\n2\n3\n4\nFROM\n \ndaily_revenue\n \nJOIN\n \ndaily_players\n \nON\n \ndaily_revenue\n.\ndt\n \n=\n \ndaily_players\n.\ndt\n;\n\n\n\nJOIN\n \ndaily_players\n \nUSING\n \n(\ndt\n);\n\n\nJOIN\n \ndaily_players\n \nON\n \ndaily_revenue\n.\ndt\n \n=\n \ndaily_players\n.\ndt\n;\n\n\n\n\n\n\n\nNow let\u2019s find out what percent of Mineblock players are returning to play the next day. This KPI is called 1 Day Retention.\n\n\nRetention can be defined many different ways, but we\u2019ll stick to the most basic definition. For all players on Day N, we\u2019ll consider them retained if they came back to play again on Day N+1.\n\n\nThis will let us track whether or not Mineblocks is getting \u201cstickier\u201d over time. The stickier our game, the more days players will spend in-game.\n\n\nAnd more time in-game means more opportunities to monetize and grow our business.\n\n\nBefore we can calculate retention we need to get our data formatted in a way where we can determine if a user returned.\n\n\nCurrently the gameplays table is a list of when the user played, and it\u2019s not easy to see if any user came back.\n\n\nBy using a self-join, we can make multiple gameplays available on the same row of results. This will enable us to calculate retention.\n\n\nThe power of self-join comes from joining every row to every other row. This makes it possible to compare values from two different rows in the new result set. In our case, we\u2019ll compare rows that are one date apart from each user.\n\n\nTo calculate retention, start from a query that selects the date(created_at) as dt and user_id columns from the gameplays table.\n\n\n1\n2\n3\n4\n5\n6\nSELECT\n\n    \ndate\n(\ncreated_at\n)\n \nAS\n \ndt\n,\n\n    \nuser_id\n\n\nFROM\n \ngameplays\n \nAS\n \ng1\n\n\nORDER\n \nBY\n \ndt\n\n\nLIMIT\n \n100\n;\n\n\n\n\n\n\n\nNow we\u2019ll join gameplays on itself so that we can have access to all gameplays for each player, for each of their gameplays.\n\n\nThis is known as a self-join and will let us connect the players on Day N to the players on Day N+1. In order to join a table to itself, it must be aliased so we can access each copy separately.\n\n\nWe aliased gameplays in the query above because in the next step, we need to join gameplays to itself so we can get a result selecting [date, user_id, user_id_if_retained].\n\n\nComplete the query by using a join statement to join gameplays to itself on user_id using the aliases g1 and g2.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nSELECT\n\n    \ndate\n(\ng1\n.\ncreated_at\n)\n \nas\n \ndt\n,\n\n    \ng1\n.\nuser_id\n\n\nFROM\n \ngameplays\n \nAS\n \ng1\n\n\nJOIN\n \ngameplays\n \nAS\n \ng2\n \nON\n\n    \ng1\n.\nuser_id\n \n=\n \ng2\n.\nuser_id\n\n\nORDER\n \nBY\n \n1\n\n\nLIMIT\n \n100\n;\n\n\n\n\n\n\n\nWe don\u2019t use the using clause here because the join is about to get more complicated.\n\n\nNow that we have our gameplays table joined to itself, we can start to calculate retention.\n\n\n1 Day Retention is defined as the number of players who returned the next day divided by the number of original players, per day. Suppose 10 players played Mineblocks on Dec 10\nth\n. If 4 of them play on Dec 11\nth\n, the 1 day retention for Dec 10\nth\n is 40%.\n\n\nThe previous query joined all rows in gameplays against all other rows for each user, making a massive result set that we don\u2019t need.\n\n\nWe\u2019ll need to modify this query.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nSELECT\n\n  \ndate\n(\ng1\n.\ncreated_at\n)\n \nAS\n \ndt\n,\n\n  \ng1\n.\nuser_id\n,\n\n  \ng2\n.\nuser_id\n\n\nFROM\n \ngameplays\n \nAS\n \ng1\n\n  \nJOIN\n \ngameplays\n \nAS\n \ng2\n \nON\n\n    \ng1\n.\nuser_id\n \n=\n \ng2\n.\nuser_id\n\n    \nAND\n \n/**/\n\n\nORDER\n \nBY\n \n1\n\n\nLIMIT\n \n100\n;\n\n\n\n\n\n\n\nComplete the query above such that the join clause includes a date join:\n\n\n1\ndate\n(\ng1\n.\ncreated_at\n)\n \n=\n \ndate\n(\ndatetime\n(\ng2\n.\ncreated_at\n,\n \n'-1 day'\n))\n\n\n\n\n\n\n\nThis means \u201conly join rows where the date in g1 is one less than the date in g2\u201d, which makes it possible to see if users have returned!\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nSELECT\n\n    \ndate\n(\ng1\n.\ncreated_at\n)\n \nas\n \ndt\n,\n\n    \ng1\n.\nuser_id\n,\n\n    \ng2\n.\nuser_id\n\n\nFROM\n \ngameplays\n \nAS\n \ng1\n\n\nJOIN\n \ngameplays\n \nAS\n \ng2\n \nON\n\n    \ng1\n.\nuser_id\n \n=\n \ng2\n.\nuser_id\n\n    \nAND\n \ndate\n(\ng1\n.\ncreated_at\n)\n \n=\n \ndate\n(\ndatetime\n(\ng2\n.\ncreated_at\n,\n \n'-1 day'\n))\n\n\nORDER\n \nBY\n \n1\n\n\nLIMIT\n \n100\n;\n\n\n\n\n\n\n\nThe query above won\u2019t return meaningful results because we\u2019re using an inner join. This type of join requires that the condition be met for all rows, effectively limiting our selection to only the users that have returned.\n\n\nInstead, we want to use a left join, this way all rows in g1 are preserved, leaving nulls in the rows from g2 where users did not return to play the next day.\n\n\nChange the join clause to use left join and count the distinct number of users from g1 and g2 per date.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nSELECT\n\n    \ndate\n(\ng1\n.\ncreated_at\n)\n \nas\n \ndt\n,\n\n    \ncount\n(\nDISTINCT\n \ng1\n.\nuser_id\n)\n \nas\n \ntotal_users\n,\n\n    \ncount\n(\nDISTINCT\n \ng2\n.\nuser_id\n)\n \nas\n \nretained_users\n\n\nFROM\n \ngameplays\n \nAS\n \ng1\n\n\nLEFT\n \nJOIN\n \ngameplays\n \nAS\n \ng2\n \nON\n\n    \ng1\n.\nuser_id\n \n=\n \ng2\n.\nuser_id\n\n    \nAND\n \ndate\n(\ng1\n.\ncreated_at\n)\n \n=\n \ndate\n(\ndatetime\n(\ng2\n.\ncreated_at\n,\n \n'-1 day'\n))\n\n\nGROUP\n \nBY\n \n1\n\n\nORDER\n \nBY\n \n1\n\n\nLIMIT\n \n100\n;\n\n\n\n\n\n\n\nNow that we have retained users as count(distinct g2.user_id) and total users as count(distinct g1.user_id), divide retained users by total users to calculate 1 day retention!\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nSELECT\n\n    \ndate\n(\ng1\n.\ncreated_at\n)\n \nas\n \ndt\n,\n\n    \nround\n(\n100\n \n*\n \ncount\n(\nDISTINCT\n \ng2\n.\nuser_id\n)\n \n/\n \ncount\n(\nDISTINCT\n \ng1\n.\nuser_id\n))\n \nAS\n \nretention\n\n\nFROM\n \ngameplays\n \nAS\n \ng1\n\n\nLEFT\n \nJOIN\n \ngameplays\n \nAS\n \ng2\n \nON\n\n    \ng1\n.\nuser_id\n \n=\n \ng2\n.\nuser_id\n\n    \nAND\n \ndate\n(\ng1\n.\ncreated_at\n)\n \n=\n \ndate\n(\ndatetime\n(\ng2\n.\ncreated_at\n,\n \n'-1 day'\n))\n\n\nGROUP\n \nBY\n \n1\n\n\nORDER\n \nBY\n \n1\n\n\nLIMIT\n \n100\n;\n\n\n\n\n\n\n\nWhile every business has different metrics to track their success, most are based on revenue and usage.\n\n\nThe metrics in this lesson are merely a starting point, and from here you\u2019ll be able to create and customize metrics to track whatever is most important to your company.\n\n\nAnd remember, data science is exploratory! The current set of metrics can always be improved and there\u2019s usually more to any spike or dip than what immediately meets the eye.\n\n\nLet\u2019s generalize what we\u2019ve learned so far:\n\n\n\n\nKey Performance Indicators are high level health metrics for a business.\n\n\nDaily Revenue is the sum of money made per day.\n\n\nDaily Active Users are the number of unique users seen each day.\n\n\nDaily Average Revenue Per Purchasing User (ARPPU) is the average amount of money spent by purchasers each day.\n\n\nDaily Average Revenue Per User (ARPU) is the average amount of money across all users.\n\n\n1 Day Retention is defined as the number of players from Day N who came back to play again on Day N+1.",
            "title": "Codecademy, SQL, Analyzing Business Metrics"
        },
        {
            "location": "/Codecademy SQL, Analyzing Business Metrics/#avanced-aggregates",
            "text": "At the heart of every great business decision is data. Since most businesses store critical data in SQL databases, a deep understanding of SQL is a necessary skill for every data analyst.  Chief among data analysis tasks is data aggregation, the grouping of data to express in summary form. We\u2019ll be working with SpeedySpoon, a meal delivery app. The folks at SpeedySpoon have asked us to look into their deliveries and help them optimize their process.  This course was developed in partnership with our good friends at Periscope Data. If you\u2019re new to SQL, we recommend you do this course first.  Complete each query by replacing the comments  /**/  with SQL code.  We\u2019ll start by looking at SpeedySpoon\u2019s data. The orders table has a row for each order of a SpeedySpoon delivery. It says when the order took place, and who ordered it.  Add code to the select statement to select all columns in the orders table.  1\n2\n3\n4 select   *  from   /**/  order   by   id  limit   100 ;    Note that the order and limit clauses keep the data organized.  1 SELECT   *   FROM   orders   ORDER   BY   id   LIMIT   100 ;    The order_items table lists the individual foods and their prices in each order. Complete the query to select all columns in the order_items table.  1 SELECT   *   FROM   order_items   ORDER   BY   id   LIMIT   100 ;    Now that we have a good handle on our data, let\u2019s dive into some common business queries. We\u2019ll begin with the Daily Count of orders placed. To make our Daily Count metric, we\u2019ll focus on the date function and the ordered_at field.  To get the Daily Metrics we need the date. Most dates in databases are timestamps, which have hours and minutes, as well as the year, month, and day. Timestamps look like  2015-01-05 14:43:31 , while dates are the first part:  2015-01-05 .  We can easily select the date an item was ordered at with the date function and the ordered_at field:  1\n2 SELECT   date ( ordered_at )  FROM   orders ;    Let\u2019s get a Daily Count of orders from the orders table. Complete the query using the date function to cast the timestamps in ordered_at to dates.  1\n2\n3\n4 \u200b SELECT   date ( ordered_at )  FROM   orders  ORDER   BY   1  LIMIT   100 ;    The order by 1 statement is a shortcut for order by date(ordered_at). The 1 refers to the first column.  Now that we can convert timestamps to dates, we can count the Orders Per Date. Generally, when we count all records in a table we run a query with the count function, as follows:  1\n2 SELECT   count ( 1 )  FROM   users ;    This will treat all rows as a single group, and return one row in the result set - the total count.  To count orders by their dates, we\u2019ll use the date and count functions and pair them with the group by clause. Together this will count the records in the table, grouped by date.  For example, to see the user records counted by the date created, we use the date and count functions and group by clause as follows:  1\n2\n3 SELECT   date ( created_at ),   count ( 1 )  FROM   users  GROUP   BY   date ( created_at )    Use the date and count functions and group by clause to count and group the orders by the dates they were ordered_at.  1\n2\n3\n4 SELECT   date ( ordered_at ),   count ( 1 )  FROM   orders  GROUP   BY   1  ORDER   BY   1 ;    Group before ordering  We have the Daily Count of orders, but what we really want to know is revenue. How much money has SpeedySpoon made from orders each day?  We can make a few changes to our Daily Count query to get the revenue.  First, instead of using count(1) to count the rows per date, we\u2019ll use round(sum(amount_paid), 2) to add up the revenue per date. Complete the query by adding revenue per date.  Second, we need to join in the order_items table because that table has an amount_paid column representing revenue. Complete the query by adding a join clause where orders.id = order_items.order_id.  Note that the round function rounds decimals to digits, based on the number passed in. Here round(\u2026, 2) rounds the sum paid to two digits.  1\n2\n3\n4\n5 SELECT   date ( ordered_at ),   round ( sum ( amount_paid ),   2 )  FROM   orders   JOIN   order_items   ON  \n     orders . id   =   order_items . order_id  GROUP   BY   1  ORDER   BY   1 ;    Now with a small change, we can find out how much we\u2019re making per day for any single dish. What\u2019s the daily revenue from customers ordering kale smoothies? Complete the query by using a where clause to filter the daily sums down to orders where the name = \u2018kale-smoothie\u2019.  1\n2\n3\n4\n5\n6 SELECT   date ( ordered_at ),   round ( sum ( amount_paid ),   2 )  FROM   orders   JOIN   order_items   ON  \n     orders . id   =   order_items . order_id  WHERE   name   =   'kale-smoothie'  GROUP   BY   1  ORDER   BY   1 ;    It looks like the smoothies might not be performing well, but to be sure we need to see how they\u2019re doing in the context of the other order items. We\u2019ll look at the data several different ways, the first of which is determining what percent of revenue these smoothies represent.  To get the percent of revenue that each item represents, we need to know the total revenue of each item. We will later divide the per-item total with the overall total revenue.  The following query groups and sum the products by price to get the total revenue for each item. Complete the query by passing in sum(amount_paid) into the round function and rounding to two decimal places.  1\n2\n3\n4 SELECT   name ,   round ( sum ( amount_paid ),   2 )  FROM   order_items  GROUP   BY   name  ORDER   BY   2   DESC ;    We have the sum of the the products by revenue, but we still need the percent. For that, we\u2019ll need to get the total using a subquery. A subquery can perform complicated calculations and create filtered or aggregate tables on the fly.  Subqueries are useful when you want to perform an aggregation outside the context of the current query. This will let us calculate the overall total and per-item total at the same time.  Complete the denominator in the subquery, which is the total revenue from order_items. Use the sum function to query the amount_paid from the order_items table.  We now have the percent or revenue each product represents!  1\n2\n3\n4\n5 SELECT   name ,   round ( sum ( amount_paid )   /  \n     ( SELECT   sum ( amount_paid )   FROM   order_items )   *   100 . 0 ,   2 )  FROM   order_items  GROUP   BY   1  ORDER   BY   2   DESC ;    Here order by 2 desc sorts by the second column (the percent) to show the products in order of their contribution to revenue.  As we suspected, kale smoothies are not bringing in the money. And thanks to this analysis, we found what might be a trend - several of the other low performing products are also smoothies. Let\u2019s keep digging to find out what\u2019s going on with these smoothies.  To see if our smoothie suspicion has merit, let\u2019s look at purchases by category. We can group the order items by what type of food they are, and go from there. Since our order_items table does not include categories already, we\u2019ll need to make some!  Previously we\u2019ve been using group by with a column (like order_items.name) or a function (like date(orders.ordered_at)).  We can also use group by with expressions. In this case a case statement is just what we need to build our own categories. case statements are similar to if/else in other languages. Here\u2019s the basic structure of a case statement:  1\n2\n3\n4\n5 CASE   { condition } \n     WHEN   { value1 }   THEN   { result1 } \n     WHEN   { value2 }   THEN   { result2 } \n     ELSE   { result3 }  END    We\u2019ll build our own categories using a case statement. Complete the query below with a case condition of name that lists out each product, and decides its group.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 SELECT   * , \n     CASE   name \n         WHEN   'kale-smoothie'      THEN   'smoothie' \n         WHEN   'banana-smoothie'    THEN   'smoothie' \n         WHEN   'orange-juice'       THEN   'drink' \n         WHEN   'soda'               THEN   'drink' \n         WHEN   'blt'                THEN   'sandwich' \n         WHEN   'grilled-cheese'     THEN   'sandwich' \n         WHEN   'tikka-masala'       THEN   'dinner' \n         WHEN   'chicken-parm'       THEN   'dinner' \n         ELSE   'other' \n     END   AS   category  FROM   order_items  ORDER   BY   id  LIMIT   100 ;    Complete the query by using the category column created by the case statement in our previous revenue percent calculation. Add the denominator that will sum the amount_paid.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 SELECT \n     CASE   name \n         WHEN   'kale-smoothie'      THEN   'smoothie' \n         WHEN   'banana-smoothie'    THEN   'smoothie' \n         WHEN   'orange-juice'       THEN   'drink' \n         WHEN   'soda'               THEN   'drink' \n         WHEN   'blt'                THEN   'sandwich' \n         WHEN   'grilled-cheese'     THEN   'sandwich' \n         WHEN   'tikka-masala'       THEN   'dinner' \n         WHEN   'chicken-parm'       THEN   'dinner' \n         ELSE   'other' \n     END   AS   category ,   round ( 1 . 0   *   sum ( amount_paid )   /   ( SELECT   sum ( amount_paid )   FROM   order_items )   *   100 ,   2 )   AS   pct  FROM   order_items  GROUP   BY   1  ORDER   BY   2   DESC ;    Here 1.0 * is a shortcut to ensure the database represents the percent as a decimal.  It\u2019s true that the whole smoothie category is performing poorly compared to the others. We\u2019ll certainly take this discovery to SpeedySpoon. Before we do, let\u2019s go one level deeper and figure out why.  While we do know that kale smoothies (and drinks overall) are not driving a lot of revenue, we don\u2019t know why. A big part of data analysis is implementing your own metrics to get information out of the piles of data in your database.  In our case, the reason could be that no one likes kale, but it could be something else entirely. To find out, we\u2019ll create a metric called reorder rate and see how that compares to the other products at SpeedySpoon.  We\u2019ll define reorder rate as the ratio of the total number of orders to the number of people making those orders. A lower ratio means most of the orders are reorders. A higher ratio means more of the orders are first purchases.  Let\u2019s calculate the reorder ratio for all of SpeedySpoon\u2019s products and take a look at the results. Counting the total orders per product is straightforward. We count the distinct order_ids (different) in the order_items table.  Complete the query by passing in the distinct keyword and the order_id column name into the count function  Here\u2019s a hint on how to use the count function to count distinct columns in a table.  1\n2\n3\n4\n5\n6\n7 SELECT   DISTINCT   column_name   FROM   table_name ;  SELECT   column_name   FROM   table_name   GROUP   BY   column_name ;  SELECT   name ,   count ( DISTINCT   order_id )  FROM   order_items  GROUP   BY   1  ORDER   BY   1 ;    Now we need the number of people making these orders.  To get that information, we need to join in the orders table and count unique values in the delivered_to field, and sort by the reorder_rate.  Complete the query below. The numerator should count the distinct order_ids. The denominator should count the distinct values of the orders table\u2019s delivered_to field (orders.delivered_to).  1\n2\n3\n4\n5\n6\n7\n8 SELECT   name ,   round ( 1 . 0   *   count ( DISTINCT   order_id )   / \n     count ( DISTINCT   delivered_to ),   2 )   AS   reorder_rate  FROM   order_items \n     JOIN   orders   ON \n         orders . id   =   order_items . order_id  GROUP   BY   1  ORDER   BY   2   DESC ;    That\u2019s unexpected. While smoothies aren\u2019t making a lot of money for SpeedySpoon, they have a very high reorder rate. That means these smoothie customers are strong repeat customers.  Instead of recommending smoothies be taken off the menu, we should talk to the smoothie customers and see what they like so much about these smoothies. There could be an opportunity here to expand the product line, or get other customers as excited as these kale fanatics. Nice work!  Let\u2019s generalize what we\u2019ve learned so far:    Data aggregation is the grouping of data in summary form.    Daily Count is the count of orders in a day.    Daily Revenue Count is the revenue on orders per day.    Product Sum is the total revenue of a product.    Subqueries can be used to perform complicated calculations and create filtered or aggregate tables on the fly.    Reorder Rate is the ratio of the total number of orders to the number of people making orders.",
            "title": "Avanced Aggregates"
        },
        {
            "location": "/Codecademy SQL, Analyzing Business Metrics/#common-metrics",
            "text": "As a data scientist, when you\u2019re not investigating spikes or dips in your data, you might be building dashboards of KPIs, or key performance indicators for a company.  KPIs are often displayed on TVs on the walls of the office, and serve as high level health metrics for the business. While every company\u2019s metrics are defined slightly differently, the basics are usually very similar.  In this lesson we\u2019ll take a look at basic KPIs like Daily Revenue, Daily Active Users, ARPU, and Retention for a video game, Mineblocks.  This company has two tables, gameplays and purchases. The purchases table lists all purchases made by players while they\u2019re playing Mineblocks. Complete the query to select from purchases.  1 SELECT   *   FROM   purchases   ORDER   BY   id   LIMIT   10 ;    The gameplays table lists the date and platform for each session a user plays. Select from gameplays.  1 SELECT   *   FROM   gameplays   ORDER   BY   id   LIMIT   10 ;    At the heart of every company is revenue, and Mineblocks is no exception. For our first KPI we\u2019ll calculate daily revenue.  Daily Revenue is simply the sum of money made per day.  To get close to Daily Revenue, we calculate the daily sum of the prices in the purchases table. Complete the query by using the sum function and passing the price column from the purchases table.   1\n2\n3\n4\n5\n6 SELECT  \n     date ( created_at ), \n     round ( sum ( price ), 2 )  FROM   purchases  GROUP   BY   1  ORDER   BY   1 ;    Update our daily revenue query to exclude refunds. Complete the query by filtering for refunded_at is not null. Update our daily revenue query to exclude refunds. Complete the query by filtering for refunded_at is not null.  Mineblocks is a game, and one of the core metrics to any game is the number people who play each day. That KPI is called Daily Active Users, or DAU.  DAU is defined as the number of unique players seen in-game each day. It\u2019s important not to double count users who played multiple times, so we\u2019ll use distinct in our count function.  Likewise, Weekly Active Users (WAU) and Monthly Active Users (MAU) are in the same family.  For Mineblocks, we\u2019ll use the gameplays table to calculate DAU. Each time a user plays the game, their session is recorded in gameplays. Thus, a distinct count of users per day from gameplays will give us DAU.  Calculate Daily Active Users for Mineblocks. Complete the query\u2019s count function by passing in the distinct keyword and the user_id column name.  1\n2\n3\n4\n5\n6 SELECT \n     date ( created_at ),  \n     COUNT ( DISTINCT   user_id )   AS   dau  FROM   gameplays  GROUP   BY   1  ORDER   BY   1 ;    Since Mineblocks is on multiple platforms, we can calculate DAU per-platform.  Previously we calculated DAU only per day, so the output we wanted was [date, dau_count]. Now we want DAU per platform, making the desired output [date, platform, dau_count].  Calculate DAU for Mineblocks per-platform. Complete the query below. You will need to select the platform column and add a count function by passing in the distinct keyword and the user_id column name.  1\n2\n3\n4\n5\n6\n7 SELECT \n     date ( created_at ),  \n     platform , \n     COUNT ( DISTINCT   user_id )   AS   dau  FROM   gameplays  GROUP   BY   1 ,   2  ORDER   BY   1 ,   2 ;    group by 1 (date), 2 (platform) \n order by 1 (date), 2 (platform)  We\u2019ve looked at DAU and Daily Revenue in Mineblocks. Now we must understand the purchasing habits of our users.  Mineblocks, like every freemium game, has two types of users:  1\n2 purchasers :   users   who   have   bought   things   in   the   game  players :   users   who   play   the   game   but   have   not   yet   purchased    The next KPI we\u2019ll look at Daily ARPPU - Average Revenue Per Purchasing User. This metric shows if the average amount of money spent by purchasers is going up over time.  Daily ARPPU is defined as the sum of revenue divided by the number of purchasers per day.  To get Daily ARPPU, modify the daily revenue query from earlier to divide by the number of purchasers.  Complete the query by adding a numerator and a denominator. The numerator will display daily revenue, or sum the price columns. The denominator will display the number of purchasers by passing the distinct keyword and the user_id column name into the count function.  1\n2\n3\n4\n5\n6\n7 SELECT \n     date ( created_at ),  \n     round ( sum ( price )   /   COUNT ( DISTINCT   user_id ),   2 )   AS   arppu  FROM   purchases  WHERE   refunded_at   IS   NULL  GROUP   BY   1  ORDER   BY   1 ;    The more popular (and difficult) cousin to Daily ARPPU is Daily ARPU, Average Revenue Per User. ARPU measures the average amount of money we\u2019re getting across all players, whether or not they\u2019ve purchased.  ARPPU increases if purchasers are spending more money. ARPU increases if more players are choosing to purchase, even if the purchase size stays consistent.  No one metric can tell the whole story. That\u2019s why it\u2019s so helpful to have many KPIs on the same dashboard.  Daily ARPU is defined as revenue divided by the number of players, per-day. To get that, we\u2019ll need to calculate the daily revenue and daily active users separately, and then join them on their dates.  One way to easily create and organize temporary results in a query is with CTEs, Common Table Expressions, also known as with clauses. The with clauses make it easy to define and use results in a more organized way than subqueries. These clauses usually look like this:  1\n2\n3\n4\n5\n6 WITH   { subquery_name }   AS   ( \n     { subquery_body }  )  SELECT   ...  FROM   { subquery_name }  WHERE   ...    Use a with clause to define daily_revenue and then select from it.  1\n2\n3\n4\n5\n6\n7\n8\n9 daily_revenue   AS   ( \n     SELECT \n         date ( created_at )   AS   dt , \n         round ( sum ( price ),   2 )   AS   rev \n     FROM   purchases \n     WHERE   refunded_at   IS   NULL \n     GROUP   BY   1  )  SELECT   *   FROM   daily_revenue   ORDER   BY   dt ;    Use a with clause to define daily_revenue and then select from it.  1\n2\n3\n4\n5\n6\n7\n8\n9 WITH   daily_revenue   AS   ( \n     SELECT \n         date ( created_at )   AS   dt , \n         round ( sum ( price ),   2 )   AS   rev \n     FROM   purchases \n     WHERE   refunded_at   IS   NULL \n     GROUP   BY   1  )  SELECT   *   FROM   daily_revenue   ORDER   BY   dt ;    Now you\u2019re familiar with using the with clause to create temporary result sets.  You just built the first part of ARPU, daily_revenue. From here we can build the second half of ARPU in our with clause, daily_players, and use both together to create ARPU.  Building on this CTE, we can add in DAU from earlier. Complete the query by calling the DAU query we created earlier, now aliased as daily_players:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 WITH   daily_revenue   AS   ( \n     SELECT \n         date ( created_at )   AS   dt , \n         round ( sum ( price ),   2 )   AS   rev \n     FROM   purchases \n     WHERE   refunded_at   IS   NULL \n     GROUP   BY   1  ),  daily_players   AS   ( \n     SELECT \n         date ( created_at )   AS   dt , \n         COUNT ( DISTINCT   user_id )   AS   players \n     FROM   gameplays \n     GROUP   BY   1  )  SELECT   *   FROM   daily_players  ORDER   BY   dt ;    Now that we have the revenue and DAU, join them on their dates and calculate daily ARPU. Complete the query by adding the keyword using in the join clause.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 WITH   daily_revenue   AS   ( \n     SELECT \n         date ( created_at )   AS   dt , \n         round ( sum ( price ),   2 )   AS   rev \n     FROM   purchases \n     WHERE   refunded_at   IS   NULL \n     GROUP   BY   1  ),  daily_players   AS   ( \n     SELECT \n         date ( created_at )   AS   dt , \n         COUNT ( DISTINCT   user_id )   AS   players \n     FROM   gameplays \n     GROUP   BY   1  )  SELECT  \n     daily_revenue . dt , \n     daily_revenue . rev   / \n     daily_players . players  FROM   daily_players  JOIN   daily_players   USING   ( dt );    In the final select statement, daily_revenue.dt represents the date, while daily_revenue.rev / daily_players.players is the daily revenue divided by the number of players that day. In full, it represents how much the company is making per player, per day.  In our ARPU query, we used using instead of on in the join clause. This is a special case join.  1 FROM   daily_revenue   JOIN   daily_players   USING   ( dt );    When the columns to join have the same name in both tables you can use USING instead of on. Our use of the using keyword is in this case equivalent to this clause:  1\n2\n3\n4 FROM   daily_revenue   JOIN   daily_players   ON   daily_revenue . dt   =   daily_players . dt ;  JOIN   daily_players   USING   ( dt );  JOIN   daily_players   ON   daily_revenue . dt   =   daily_players . dt ;    Now let\u2019s find out what percent of Mineblock players are returning to play the next day. This KPI is called 1 Day Retention.  Retention can be defined many different ways, but we\u2019ll stick to the most basic definition. For all players on Day N, we\u2019ll consider them retained if they came back to play again on Day N+1.  This will let us track whether or not Mineblocks is getting \u201cstickier\u201d over time. The stickier our game, the more days players will spend in-game.  And more time in-game means more opportunities to monetize and grow our business.  Before we can calculate retention we need to get our data formatted in a way where we can determine if a user returned.  Currently the gameplays table is a list of when the user played, and it\u2019s not easy to see if any user came back.  By using a self-join, we can make multiple gameplays available on the same row of results. This will enable us to calculate retention.  The power of self-join comes from joining every row to every other row. This makes it possible to compare values from two different rows in the new result set. In our case, we\u2019ll compare rows that are one date apart from each user.  To calculate retention, start from a query that selects the date(created_at) as dt and user_id columns from the gameplays table.  1\n2\n3\n4\n5\n6 SELECT \n     date ( created_at )   AS   dt , \n     user_id  FROM   gameplays   AS   g1  ORDER   BY   dt  LIMIT   100 ;    Now we\u2019ll join gameplays on itself so that we can have access to all gameplays for each player, for each of their gameplays.  This is known as a self-join and will let us connect the players on Day N to the players on Day N+1. In order to join a table to itself, it must be aliased so we can access each copy separately.  We aliased gameplays in the query above because in the next step, we need to join gameplays to itself so we can get a result selecting [date, user_id, user_id_if_retained].  Complete the query by using a join statement to join gameplays to itself on user_id using the aliases g1 and g2.  1\n2\n3\n4\n5\n6\n7\n8 SELECT \n     date ( g1 . created_at )   as   dt , \n     g1 . user_id  FROM   gameplays   AS   g1  JOIN   gameplays   AS   g2   ON \n     g1 . user_id   =   g2 . user_id  ORDER   BY   1  LIMIT   100 ;    We don\u2019t use the using clause here because the join is about to get more complicated.  Now that we have our gameplays table joined to itself, we can start to calculate retention.  1 Day Retention is defined as the number of players who returned the next day divided by the number of original players, per day. Suppose 10 players played Mineblocks on Dec 10 th . If 4 of them play on Dec 11 th , the 1 day retention for Dec 10 th  is 40%.  The previous query joined all rows in gameplays against all other rows for each user, making a massive result set that we don\u2019t need.  We\u2019ll need to modify this query.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 SELECT \n   date ( g1 . created_at )   AS   dt , \n   g1 . user_id , \n   g2 . user_id  FROM   gameplays   AS   g1 \n   JOIN   gameplays   AS   g2   ON \n     g1 . user_id   =   g2 . user_id \n     AND   /**/  ORDER   BY   1  LIMIT   100 ;    Complete the query above such that the join clause includes a date join:  1 date ( g1 . created_at )   =   date ( datetime ( g2 . created_at ,   '-1 day' ))    This means \u201conly join rows where the date in g1 is one less than the date in g2\u201d, which makes it possible to see if users have returned!   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 SELECT \n     date ( g1 . created_at )   as   dt , \n     g1 . user_id , \n     g2 . user_id  FROM   gameplays   AS   g1  JOIN   gameplays   AS   g2   ON \n     g1 . user_id   =   g2 . user_id \n     AND   date ( g1 . created_at )   =   date ( datetime ( g2 . created_at ,   '-1 day' ))  ORDER   BY   1  LIMIT   100 ;    The query above won\u2019t return meaningful results because we\u2019re using an inner join. This type of join requires that the condition be met for all rows, effectively limiting our selection to only the users that have returned.  Instead, we want to use a left join, this way all rows in g1 are preserved, leaving nulls in the rows from g2 where users did not return to play the next day.  Change the join clause to use left join and count the distinct number of users from g1 and g2 per date.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 SELECT \n     date ( g1 . created_at )   as   dt , \n     count ( DISTINCT   g1 . user_id )   as   total_users , \n     count ( DISTINCT   g2 . user_id )   as   retained_users  FROM   gameplays   AS   g1  LEFT   JOIN   gameplays   AS   g2   ON \n     g1 . user_id   =   g2 . user_id \n     AND   date ( g1 . created_at )   =   date ( datetime ( g2 . created_at ,   '-1 day' ))  GROUP   BY   1  ORDER   BY   1  LIMIT   100 ;    Now that we have retained users as count(distinct g2.user_id) and total users as count(distinct g1.user_id), divide retained users by total users to calculate 1 day retention!   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 SELECT \n     date ( g1 . created_at )   as   dt , \n     round ( 100   *   count ( DISTINCT   g2 . user_id )   /   count ( DISTINCT   g1 . user_id ))   AS   retention  FROM   gameplays   AS   g1  LEFT   JOIN   gameplays   AS   g2   ON \n     g1 . user_id   =   g2 . user_id \n     AND   date ( g1 . created_at )   =   date ( datetime ( g2 . created_at ,   '-1 day' ))  GROUP   BY   1  ORDER   BY   1  LIMIT   100 ;    While every business has different metrics to track their success, most are based on revenue and usage.  The metrics in this lesson are merely a starting point, and from here you\u2019ll be able to create and customize metrics to track whatever is most important to your company.  And remember, data science is exploratory! The current set of metrics can always be improved and there\u2019s usually more to any spike or dip than what immediately meets the eye.  Let\u2019s generalize what we\u2019ve learned so far:   Key Performance Indicators are high level health metrics for a business.  Daily Revenue is the sum of money made per day.  Daily Active Users are the number of unique users seen each day.  Daily Average Revenue Per Purchasing User (ARPPU) is the average amount of money spent by purchasers each day.  Daily Average Revenue Per User (ARPU) is the average amount of money across all users.  1 Day Retention is defined as the number of players from Day N who came back to play again on Day N+1.",
            "title": "Common Metrics"
        },
        {
            "location": "/Codecademy SQL, Table Transformation/",
            "text": "Foreword\n\n\nCode snippets. From Codecademy.\n\n\n\n\nWhile working with databases, we often need to transform data from one format to achieve a desired result. In SQL, this is often called data transformation or table transformation.\n\n\nSubqueries\n\u00b6\n\n\nSubqueries, sometimes referred to as inner queries or nested queries, are used to transform table data by nesting one query within another query. \n\n\nTwo tables: \nairports\n and \nflights\n.\n\n\nSelect ten rows from the \nflights\n table.\n\n\n1\n2\n3\n4\n5\n6\n7\nSELECT\n \n*\n \nFROM\n \nflights\n \nLIMIT\n \n10\n;\n\n\n```\nsql\n\n\n\nWe\n \nfirst\n \ncreate\n \nan\n \ninner\n \nquery\n,\n \nor\n \nsubquery\n,\n \nthat\n \nfinds\n \nthe\n \nairports\n \nwith\n \nelevation\n \ngreater\n \nthan\n \n2000\n \nfrom\n \nthe\n \n`\nairports\n`\n \ntable\n.\n \n\n\n```\nsql\n\n\nSELECT\n \ncode\n \nFROM\n \nairports\n \nWHERE\n \nelevation\n \n>\n \n2000\n;\n\n\n\n\n\n\n\nNext, we take the result set of the inner query and use it to filter on the \nflights\n table, to find the flight detail that meets the elevation criteria.\n\n\n1\n2\nSELECT\n \n*\n \nFROM\n \nflights\n \nWHERE\n \norigin\n \nin\n \n(\n\n    \nSELECT\n \ncode\n \nFROM\n \nairports\n \nWHERE\n \nelevation\n \n>\n \n2000\n);\n\n\n\n\n\n\n\nFind flight information about \nflights\n where the origin elevation is less than 2000 feet.\n\n\n1\n2\nSELECT\n \n*\n \nFROM\n \nflights\n \nWHERE\n \norigin\n \nin\n \n(\n\n    \nSELECT\n \ncode\n \nFROM\n \nairports\n \nWHERE\n \nelevation\n \n<\n \n2000\n);\n\n\n\n\n\n\n\nNon-Correlated Subqueries\n\u00b6\n\n\nA non-correlated subquery is a subquery that can be run independently of the outer query and as we saw, can be used to complete a multi-step transformation.\n\n\nPerhaps we\u2019d like to look at a selection of \nflights\n whose origin airport is a seaplane base, designated by \n'SEAPLANE_BASE'\n. The facility type of an airport is located in the \nfac_type\n field of the \nairports\n table.\n\n\n1\n2\nSELECT\n \n*\n \nFROM\n \nflights\n \nWHERE\n \norigin\n \nin\n \n(\n\n    \nSELECT\n \ncode\n \nFROM\n \nairports\n \nWHERE\n \nfac_type\n \n=\n \n'SEAPLANE_BASE'\n);\n\n\n\n\n\n\n\nUsing the same pattern, find flight information about \nflights\n where the Federal Aviation Administration region (\nfaa_region\n) is the Southern region (\n'ASO'\n).\n\n\n1\n2\nSELECT\n \n*\n \nFROM\n \nflights\n \nWHERE\n \norigin\n \nin\n \n(\n\n    \nSELECT\n \ncode\n \nFROM\n \nairports\n \nWHERE\n \nfaa_region\n \n=\n \n'ASO'\n);\n\n\n\n\n\n\n\nPerform transformations on a single table. For instance, sometimes we need to aggregate in multiple steps \u2013 like taking an average of a count.\n\n\nImagine you\u2019d like to know how many \nflights\n there are on average, for all Fridays in a given month from the \nflights\n table. First, we\u2019d need to calculate the number of \nflights\n per day, and then we\u2019d need to calculate the average based on the daily flight count for each day of the week. We can do this all in one step using a subquery:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nSELECT\n \na\n.\ndep_month\n,\n\n       \na\n.\ndep_day_of_week\n,\n\n       \nAVG\n(\na\n.\nflight_count\n)\n \nAS\n \naverage_\n`\nflights\n`\n\n    \nFROM\n \n(\n\n        \nSELECT\n \ndep_month\n,\n\n               \ndep_day_of_week\n,\n\n               \ndep_date\n,\n\n               \nCOUNT\n(\n*\n)\n \nAS\n \nflight_count\n\n        \nFROM\n \n`\nflights\n`\n\n        \nGROUP\n \nBY\n \n1\n,\n2\n,\n3\n\n        \n)\n \na\n\n\nGROUP\n \nBY\n \n1\n,\n2\n\n\nORDER\n \nBY\n \n1\n,\n2\n;\n\n\n\n\n\n\n\nThe inner query provides the count of flights by day, and the outer query uses the inner query\u2019s result set to compute the average by day of week of a given month.\n\n\nUsing a subquery, find the average total distance flown by day of week and month. Be sure to alias the outer query as \naverage_distance\n and the inner query as \nflight_distance\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nSELECT\n \na\n.\ndep_month\n,\n\n       \na\n.\ndep_day_of_week\n,\n\n       \nAVG\n(\na\n.\nflight_distance\n)\n \nAS\n \naverage_distance\n\n    \nFROM\n \n(\n\n        \nSELECT\n \ndep_month\n,\n\n              \ndep_day_of_week\n,\n\n               \ndep_date\n,\n\n               \nsum\n(\ndistance\n)\n \nAS\n \nflight_distance\n\n        \nFROM\n \n`\nflights\n`\n\n        \nGROUP\n \nBY\n \n1\n,\n2\n,\n3\n\n    \n)\n \na\n\n\nGROUP\n \nBY\n \n1\n,\n2\n\n\nORDER\n \nBY\n \n1\n,\n2\n;\n\n\n\n\n\n\n\nCorrelated Subqueries\n\u00b6\n\n\nIn a correlated subquery, the subquery can not be run independently of the outer query. The order of operations is important in a correlated subquery:\n\n\nA row is processed in the outer query. Then, for that particular row in the outer query, the subquery is executed.\n\n\nThis means that for each row processed by the outer query, the subquery will also be processed for that row. In this example, we will find the list of all flights whose distance is above average for their carrier (query on a query, same table).\n\n\n1\n2\n3\n4\n5\n6\nSELECT\n \nid\n\n\nFROM\n \n`\nflights\n`\n \nAS\n \nf\n\n\nWHERE\n \ndistance\n \n>\n \n(\n\n    \nSELECT\n \nAVG\n(\ndistance\n)\n\n    \nFROM\n \n`\nflights\n`\n\n    \nWHERE\n \ncarrier\n \n=\n \nf\n.\ncarrier\n);\n\n\n\n\n\n\n\nIn the above query the inner query has to be re-executed for each flight. Correlated subqueries may appear elsewhere besides the \nWHERE\n clause, they can also appear in the \nSELECT\n.\n\n\nFind the id of the flights whose distance is below average for their carrier.\n\n\n1\n2\n3\n4\n5\n6\nSELECT\n \nid\n\n\nFROM\n \nflights\n \nAS\n \nf\n\n\nWHERE\n \ndistance\n \n<\n \n(\n\n    \nSELECT\n \nAVG\n(\ndistance\n)\n\n    \nFROM\n \nflights\n\n    \nWHERE\n \ncarrier\n \n=\n \nf\n.\ncarrier\n);\n\n\n\n\n\n\n\nIt would also be interesting to order flights by giving them a sequence number based on time, by carrier. For instance, assuming flight_id increments with each additional flight, we could use the following query to view flights by carrier, flight id, and sequence number:\n\n\n1\n2\n3\n4\n5\n6\n7\nSELECT\n \ncarrier\n,\n \nid\n,\n\n    \n(\nSELECT\n \nCOUNT\n(\n*\n)\n\n        \nFROM\n \nflights\n \nf\n\n        \nWHERE\n \nf\n.\nid\n \n<\n \nflights\n.\nid\n\n        \nAND\n \nf\n.\ncarrier\n=\nflights\n.\ncarrier\n)\n \n+\n \n1\n \n        \nAS\n \nflight_sequence_number\n\n        \nFROM\n \nflights\n;\n\n\n\n\n\n\n\nUsing the same pattern, write a query to view flights by origin, flight id, and sequence number. Alias the sequence number column as \nflight_sequence_number\n.\n\n\n1\n2\n3\n4\n5\n6\n7\nSELECT\n \norigin\n,\n \nid\n,\n\n    \n(\nSELECT\n \nCOUNT\n(\n*\n)\n\n    \nFROM\n \nflights\n \nf\n\n    \nWHERE\n \nf\n.\nid\n \n<\n \nflights\n.\nid\n\n    \nAND\n \nf\n.\norigin\n=\nflights\n.\norigin\n)\n \n+\n \n1\n \n    \nAS\n \nflight_sequence_number\n\n    \nFROM\n \nflights\n;\n\n\n\n\n\n\n\nWhat can we generalize so far?\n\u00b6\n\n\n\n\nSubqueries are used to complete an SQL transformation by nesting one query within another query.\n\n\nA non-correlated subquery is a subquery that can be run independently of the outer query and can be used to complete a multi-step transformation.\n\n\nA correlated subquery is a subquery that cannot be run independently of the outer query. The order of operations in a correlated subquery is as follows:\n\n\nA row is processed in the outer query.\n\n\nThen, for that particular row in the outer query, the subquery is executed.\n\n\nSet Operations.\n\n\n\n\n\n\n\n\nUnions allow us to utilize information from multiple tables in our queries. In this lesson, we\u2019ll utilize data from an e-commerce store. Let\u2019s explore the available data we\u2019ll be using. \n\n\nFour tables: \nnew_products\n, \nlegacy_products\n, \norder_items\n and \norder_items_historic\n.\n\n\nIn our database, we have products tables that contain metadata about each product in the store. Select ten rows from the \nnew_products\n table.\n\n\n1\nSELECT\n \n*\n \nFROM\n \nnew_products\n \nLIMIT\n \n10\n;\n\n\n\n\n\n\n\nMerging Tables Together\n\u00b6\n\n\nSometimes, in order to answer certain questions based on data, we need to merge two tables together and then query the merged result. Perhaps we have two tables that contain information about products in an ecommerce store that we would like to combine. There are two ways of doing this:\n\n\n\n\nMerge the rows, called a \nJOIN\n.\n\n\nMerge the columns, called a \nUNION\n.\n\n\n\n\nUNION\n\u00b6\n\n\nWe\u2019ll focus on unions here. Union combines the result of two or more \nSELECT\n statements, using the following syntax:\n\n\n1\n2\n3\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable1\n\n\nUNION\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable2\n;\n\n\n\n\n\n\n\nEach \nSELECT\n statement within the \nUNION\n must have the same number of columns with similar data types. The columns in each \nSELECT\n statement must be in the same order. By default, the \nUNION\n operator selects only distinct values.\n\n\nSuppose we are a growing ecommerce store and recently acquired another store to diversify our offering. The product data still exists in two separate tables: a \nlegacy_products\n table and a \nnew_products\n table. To get the complete list of product names from both tables, we can perform the following union.\n\n\n1\n2\n3\nSELECT\n \nitem_name\n \nFROM\n \nlegacy_products\n\n\nUNION\n \n\nSELECT\n \nitem_name\n \nFROM\n \nnew_products\n;\n\n\n\n\n\n\n\nSelect a complete list of brand names from the \nlegacy_products\n and \nnew_products\n tables.\n\n\n1\n2\n3\nSELECT\n \nbrand\n \nFROM\n \nlegacy_products\n\n\nUNION\n\n\nSELECT\n \nbrand\n \nFROM\n \nnew_products\n;\n\n\n\n\n\n\n\nWhat if we wanted to allow duplicate values? We can do this by using the \nALL\n keyword with \nUNION\n, with the following syntax:\n\n\n1\n2\n3\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable1\n\n\nUNION\n \nALL\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable2\n;\n\n\n\n\n\n\n\nIn our ecommerce store, if we learned that we had records from historic order items in an additional table, we could use the following query to combine the tables for a complete analysis of sale price:\n\n\n1\n2\n3\nSELECT\n \nid\n,\n \nsale_price\n \nFROM\n \norder_items\n\n\nUNION\n \nALL\n\n\nSELECT\n \nid\n,\n \nsale_price\n \nFROM\n \norder_items_historic\n;\n\n\n\n\n\n\n\nThen we can perform an analysis on top of the combined result set, like finding the total count of order items.\n\n\n1\n2\n3\n4\nSELECT\n \ncount\n(\n*\n)\n \nFROM\n \n(\n\n    \nSELECT\n \nid\n,\n \nsale_price\n \nFROM\n \norder_items\n\n    \nUNION\n \nALL\n\n    \nSELECT\n \nid\n,\n \nsale_price\n \nFROM\n \norder_items_historic\n)\n \nas\n \na\n;\n\n\n\n\n\n\n\nUsing the same pattern, utilize a subquery to find the average sale price over both \norder_items\n and \norder_items_historic\n tables.\n\n\n1\n2\n3\n4\n5\nSELECT\n \nid\n,\n \navg\n(\na\n.\nsale_price\n)\n \nFROM\n \n(\n\n    \nSELECT\n \nid\n,\n \nsale_price\n \nFROM\n \norder_items\n\n    \nUNION\n \nALL\n\n    \nSELECT\n \nid\n,\n \nsale_price\n \nFROM\n \norder_items_historic\n)\n \nAS\n \na\n \n    \nGROUP\n \nBY\n \n1\n;\n\n\n\n\n\n\n\nBefore running the top analysis, create an alias a with the preliminary results, run the \navg(a.sale_price)\n, and group by 1 to view separate records and not a unique aggregate record!!!\n\n\nINTERSECT\n\u00b6\n\n\n\u2026is used to combine two \nSELECT\n statements, but returns rows only from the first \nSELECT\n statement that are identical to a row in the second \nSELECT\n statement. This means that it returns only common rows returned by the two \nSELECT\n statements.\n\n\n1\n2\n3\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable1\n\n\nINTERSECT\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable2\n;\n\n\n\n\n\n\n\nFor instance, we might want to know what brands in our newly acquired store are also in our legacy store. We can do so using the following query:\n\n\n1\n2\n3\nSELECT\n \nbrand\n \nFROM\n \nnew_products\n\n\nINTERSECT\n\n\nSELECT\n \nbrand\n \nFROM\n \nlegacy_products\n;\n\n\n\n\n\n\n\nSelect the items in the category column that are both in the newly acquired \nnew_products\n table and the \nlegacy_products\n table.\n\n\n1\n2\n3\nSELECT\n \ncategory\n \nFROM\n \nnew_products\n\n\nINTERSECT\n\n\nSELECT\n \ncategory\n \nFROM\n \nlegacy_products\n;\n\n\n\n\n\n\n\nEXCEPT\n\u00b6\n\n\n\u2026is constructed in the same way, but returns distinct rows from the first \nSELECT\n statement that aren\u2019t output by the second \nSELECT\n statement.\n\n\n1\n2\n3\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable1\n\n\nEXCEPT\n\n\nSELECT\n \ncolumn_name\n(\ns\n)\n \nFROM\n \ntable2\n;\n\n\n\n\n\n\n\nSuppose we want to see if there are any categories that are in the \nnew_products\n table that aren\u2019t in the \nlegacy_products\n table. We can use an \nEXCEPT\n query to perform this analysis:\n\n\n1\n2\n3\nSELECT\n \ncategory\n \nFROM\n \nnew_products\n\n\nEXCEPT\n\n\nSELECT\n \ncategory\n \nFROM\n \nlegacy_products\n;\n\n\n\n\n\n\n\nConversely, select the items in the category column that are in the \nlegacy_products\n table and not in the \nnew_products\n table.\n\n\n1\n2\n3\nSELECT\n \ncategory\n \nFROM\n \nlegacy_products\n\n\nEXCEPT\n\n\nSELECT\n \ncategory\n \nFROM\n \nnew_products\n;\n\n\n\n\n\n\n\nWhat can we generalize so far?\n\u00b6\n\n\n\n\nThe \nUNION\n clause allows us to utilize information from multiple tables in our queries.\n\n\nThe \nUNION ALL\n clause allows us to utilize information from multiple tables in our queries, including duplicate values.\n\n\nINTERSECT\n is used to combine two \nSELECT\n statements, but returns rows only from the first \nSELECT\n statement that are identical to a row in the second \nSELECT\n statement.\n\n\nEXCEPT\n returns distinct rows from the first \nSELECT\n statement that aren\u2019t output by the second \nSELECT\n statement\n\n\n\n\nConditional Aggregates\n\u00b6\n\n\nAggregate functions compute a single result from a set of multiple input values. You can think of aggregate data as data collected from multiple rows at a time. In this lesson, we\u2019ll continue learning about aggregate functions by focusing on conditionals, sums, and combining aggregate functions.\n\n\nConditional Aggregates are aggregate functions that compute a result set based on a given set of conditions.\n\n\nThe count function is an aggregate function, since it aggregates data from multiple rows.\n\n\nCount the number of rows in the \nflights\n table, representing the total number of \nflights\n contained in the table.\n\n\n1\nSELECT\n \nCOUNT\n(\n*\n)\n \nFROM\n \nflights\n;\n\n\n\n\n\n\n\nWhile working with databases, it\u2019s common to have empty or unknown \u201ccells\u201d in data tables.\n\n\nWhat do we do when we need to test whether a value is or is not null? We use the special keywords \nIS NULL\n or \nIS NOT NULL\n in the \nWHERE\n clause (\n= NULL\n does not work).\n\n\nCount the number of rows from the \nflights\n table, where \narr_time\n is not null and the destination is \n'ATL'\n.\n\n\n1\nSELECT\n \nCOUNT\n(\n*\n)\n \nFROM\n \nflights\n \nWHERE\n \narr_time\n \nIS\n \nNOT\n \nNULL\n \nAND\n \ndestination\n \n=\n \n'ATL'\n;\n\n\n\n\n\n\n\nAlmost every programming language has a way to represent \u201cif, then, else\u201d, or conditional logic. In SQL, we represent this logic with the \nCASE\n statement, as follows:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nSELECT\n\n    \nCASE\n\n        \nWHEN\n \nelevation\n \n<\n \n500\n \nTHEN\n \n'Low'\n\n        \nWHEN\n \nelevation\n \nBETWEEN\n \n500\n \nAND\n \n1999\n \nTHEN\n \n'Medium'\n\n        \nWHEN\n \nelevation\n \n>=\n \n2000\n \nTHEN\n \n'High'\n\n        \nELSE\n \n'Unknown'\n\n    \nEND\n \nAS\n \nelevation_tier\n,\n \n    \nCOUNT\n(\n*\n)\n\n\nFROM\n \nairports\n\n\nGROUP\n \nBY\n \n1\n;\n\n\n\n\n\n\n\nIn the above statement, \nEND\n is required to terminate the statement, but \nELSE\n is optional. If \nELSE\n is not included, the result will be \nNULL\n. Also notice the shorthand method of referencing columns to use in \nGROUP BY\n, so we don\u2019t have to rewrite the entire \nCASE\n Statement.\n\n\nModify the case statement\u2019s such that when the elevation is less than 250, the \nelevation_tier\n column returns \n'Low'\n, when between 250 and 1749 it returns \n'Medium'\n, and when greater than or equal to 1750 it returns \n'High'\n.\n\n\nBe sure to alias the conditional statement as \nelevation_tier\n, in your query.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nSELECT\n\n    \nCASE\n\n        \nWHEN\n \nelevation\n \n<\n \n250\n \nTHEN\n \n'Low'\n\n        \nWHEN\n \nelevation\n \nBETWEEN\n \n250\n \nAND\n \n1749\n \nTHEN\n \n'Medium'\n\n        \nWHEN\n \nelevation\n \n>=\n \n1750\n \nTHEN\n \n'High'\n\n        \nELSE\n \n'Unknown'\n\n    \nEND\n \nAS\n \nelevation_tier\n,\n \n    \nCOUNT\n(\n*\n)\n\n\nFROM\n \nairports\n\n\nGROUP\n \nBY\n \n1\n;\n\n\n\n\n\n\n\nSometimes you want to look at an entire result set, but want to implement conditions on certain aggregates.\n\n\nFor instance, maybe you want to identify the total amount of airports as well as the total amount of airports with high elevation in the same result set. We can accomplish this by putting a \nCASE WHEN\n statement in the aggregate.\n\n\n1\n2\n3\n4\nSELECT\n \nstate\n,\n \n    \nCOUNT\n(\nCASE\n \nWHEN\n \nelevation\n \n>=\n \n2000\n \nTHEN\n \n1\n \nELSE\n \nNULL\n \nEND\n)\n \nas\n \ncount_high_elevation_aiports\n \n\nFROM\n \nairports\n \n\nGROUP\n \nBY\n \nstate\n;\n\n\n\n\n\n\n\nUsing the same pattern, write a query to count the number of low elevation airports by state where low elevation is defined as less than 1000 ft. Be sure to alias the counted airports as \ncount_low_elevation_airports\n.\n\n\n1\n2\n3\n4\nSELECT\n \nstate\n,\n \n    \nCOUNT\n(\nCASE\n \nWHEN\n \nelevation\n \n<\n \n1000\n \nTHEN\n \n1\n \nELSE\n \nNULL\n \nEND\n)\n \nas\n \ncount_low_elevation_aiports\n \n\nFROM\n \nairports\n \n\nGROUP\n \nBY\n \nstate\n;\n\n\n\n\n\n\n\nWe can do that same thing for other aggregates like \nSUM()\n. For instance, if we wanted to sum the total flight distance and compare that to the sum of flight distance from a particular airline (in this case, United Airlines) by origin airport, we could run the following query:\n\n\nsum(distance)\n for all carriers (\ntotal_flight_distance\n)\n\n\nsum(distance)\n for \n'UA'\n only (others are turned to 0) (\ntotal_united_flight_distance\n).\n\n\n1\n2\n3\nSELECT\n \norigin\n,\n \nsum\n(\ndistance\n)\n \nas\n \ntotal_flight_distance\n,\n \nsum\n(\nCASE\n \nWHEN\n \ncarrier\n \n=\n \n'UA'\n \nTHEN\n \ndistance\n \nELSE\n \n0\n \nEND\n)\n \nas\n \ntotal_united_flight_distance\n \n\nFROM\n \nflights\n \n\nGROUP\n \nBY\n \norigin\n;\n\n\n\n\n\n\n\nUsing the same pattern, find both the total flight distance as and flight distance by origin for Delta (\ncarrier = 'DL'\n).\n\n\nAlias the flight distance as \ntotal_flight_distance\n and the and flight distance by origin as \ntotal_delta_flight_distance\n.\n\n\n1\n2\n3\nSELECT\n \norigin\n,\n \nsum\n(\ndistance\n)\n \nas\n \ntotal_flight_distance\n,\n \nsum\n(\nCASE\n \nWHEN\n \ncarrier\n \n=\n \n'DL'\n \nTHEN\n \ndistance\n \nELSE\n \n0\n \nEND\n)\n \nas\n \ntotal_delta_flight_distance\n \n\nFROM\n \nflights\n \n\nGROUP\n \nBY\n \norigin\n;\n\n\n\n\n\n\n\nUsing the same pattern, find the percentage of \nflights\n from Delta by origin (\ncarrier = 'DL'\n):\n\n\n1\n2\nSELECT\n \norigin\n,\n \n100\n.\n0\n*\n(\nsum\n(\nCASE\n \nWHEN\n \ncarrier\n \n=\n \n'DL'\n \nTHEN\n \ndistance\n \nELSE\n \n0\n \nEND\n)\n/\nsum\n(\ndistance\n))\n \nas\n \npercentage_flight_distance_from_delta\n \nFROM\n \nflights\n \n\nGROUP\n \nBY\n \norigin\n;\n\n\n\n\n\n\n\nFind the percentage of high elevation airports (\nelevation >= 2000\n) by state from the \nairports\n table. In the query, alias the percentage column as \npercentage_high_elevation_airports\n.\n\n\n(sum of '1') / count(*)\n and not \n(count of '1') / count(*)\n:\n\n\n1\nSELECT\n \nstate\n,\n \n100\n.\n0\n \n*\n \nsum\n(\nCASE\n \nWHEN\n \nelevation\n \n>=\n \n2000\n \nTHEN\n \n1\n \nELSE\n \n0\n \nEND\n)\n \n/\n \ncount\n(\n*\n)\n  \nas\n \npercentage_high_elevation_airports\n \nFROM\n \nairports\n \nGROUP\n \nBY\n \nstate\n;\n\n\n\n\n\n\n\nWhat can we generalize so far?\n\u00b6\n\n\n\n\nConditional Aggregates are aggregate functions the compute a result set based on a given set of conditions.\n\n\nNULL\n can be used to denote an empty field value\n\n\nCASE\n statements allow for custom classification of data\n\n\nCASE\n statements can be used inside aggregates (like \nSUM()\n and \nCOUNT()\n) to provide filtered measures\n\n\n\n\nDate, Number, and String Functions\n\u00b6\n\n\nOftentimes, data in columns of tables is not in the exact format we need to complete our desired analysis. We may need to extract a date from a full timestamp, manipulate a number, or combine first and last name columns to create a full name.\n\n\nIn this lesson, we\u2019ll be learning about some of SQL\u2019s built-in functions for transforming dates, numbers and strings. We\u2019ll be using database of bakeries in this lesson.\n\n\nIt is important to note that date, number, and string functions are highly database dependent. Here, we focus on built-in functions in the SQLite database management system.\n\n\nSelect ten rows from the bakeries table:\n\n\n1\nSELECT\n \n*\n \nFROM\n \nbakeries\n \nLIMIT\n \n10\n;\n\n\n\n\n\n\n\nWe\u2019ll begin with dates. Dates are often written in the following format\n\n\n\n\nDate: \u201cYYYY-MM-DD\u201d\n\n\nDatetime or Timestamp: \u201cYYYY-MM-DD hh: mm:ss\u201d\n\n\n\n\nWe can use SQL\u2019s date functions to transform data into a desired format. Since date functions can be database specific, verify the functions that exist on your relational database management system. For example, this statement:\n\n\n1\nSELECT\n \nDATETIME\n(\nmanufacture_time\n)\n \nFROM\n \nbaked_goods\n;\n\n\n\n\n\n\n\nUsing the datetime function, select the date and time of all deliveries in the \nbaked_goods\n table using the column \ndelivery_time\n.\n\n\n1\nSELECT\n \nDATETIME\n(\ndelivery_time\n)\n \nFROM\n \nbaked_goods\n;\n\n\n\n\n\n\n\nNow let\u2019s assume that we have a column in our \nbaked_goods\n table named \nmanufacture_time\n in the format \u201cYYYY-MM-DD hh: mm:ss\u201d. We\u2019d like to know the number of baked_goods manufactured by day, and not by second. We can use the \nDATE()\n function to easily convert timestamps to dates and complete the following query:\n\n\n1\n2\n3\nSELECT\n \nDATE\n(\nmanufacture_time\n),\n \ncount\n(\n*\n)\n \nas\n \ncount_baked_goods\n\n\nFROM\n \nbaked_goods\n\n\nGROUP\n \nBY\n \nDATE\n(\nmanufacture_time\n);\n\n\n\n\n\n\n\nSimilarly, we can query the time with:\n\n\n1\n2\n3\nSELECT\n \nTIME\n(\nmanufacture_time\n),\n \ncount\n(\n*\n)\n \nas\n \ncount_baked_goods\n\n\nFROM\n \nbaked_goods\n\n\nGROUP\n \nBY\n \nTIME\n(\nmanufacture_time\n);\n\n\n\n\n\n\n\nFind the number of baked goods by date of delivery. Be sure to alias the total count of baked goods as \ncount_baked_goods\n.\n\n\n1\nSELECT\n \nDATE\n(\ndelivery_time\n),\n \ncount\n(\n*\n)\n \nas\n \ncount_baked_goods\n \nFROM\n \nbaked_goods\n \nGROUP\n \nBY\n \nDATE\n(\ndelivery_time\n);\n\n\n\n\n\n\n\nGiven a datepart and a column of date or timestamp data type, we can increment date or timestamp values by a specified interval. For example, in SQLite, the statement:\n\n\n1\nDATETIME\n(\ntime1\n,\n \n'+3 hours'\n,\n \n'40 minutes'\n,\n \n'2 days'\n);\n\n\n\n\n\n\n\nWould return a time 3 hours, 20 minutes, and 2 days after time1.\n\n\nImagine that each dessert in our \nbaked_goods\n table is inspected 2 hours, 30 minutes, and 1 day after the manufacture time. To derive the inspection date for each baked good, we can use the following query:\n\n\n1\n2\nSELECT\n \nDATETIME\n(\nmanufacture_time\n,\n \n'+2 hours'\n,\n \n'30 minutes'\n,\n \n'1 day'\n)\n \nas\n \ninspection_time\n\n\nFROM\n \nbaked_goods\n;\n\n\n\n\n\n\n\nEach of the baked goods is packaged by Baker\u2019s Market exactly five hours, twenty minutes, and two days after the delivery (designated by delivery_time). Create a query returning all the packaging times for the goods in the \nbaked_goods\n table. Be sure to alias the package time column as \npackage_time\n.\n\n\n1\n2\nSELECT\n \nDATETIME\n(\ndelivery_time\n,\n \n'+5 hours'\n,\n \n'20 minutes'\n,\n \n'2 days'\n)\n \nas\n \npackage_time\n\n\nFROM\n \nbaked_goods\n;\n\n\n\n\n\n\n\nNumeric functions can be used to transform numbers. Some common SQLite mathematical functions are included below that take numeric data types as inputs:\n\n\n\n\nSELECT (number1 + number2);\n: returns the sum of two numbers. Similar, SQL can be used for subtraction, multiplication, and division.\n\n\nSELECT CAST(number1 AS REAL) / number3;\n: returns the result as a real number by casting one of the values as a real number, rather than an integer.\n\n\nSELECT ROUND(number, precision);\n: returns the numeric value rounded off to the next value specified.\n\n\n\n\nIn our \nbaked_goods\n table, we have information about cost designated by \ningredients_cost\n. For accounting purposes, we\u2019d like to make sure that each ingredient cost is rounded to four decimal places rather than two, to account for currency fluctuations.\n\n\n1\n2\nSELECT\n \nROUND\n(\ningredients_cost\n,\n \n4\n)\n \nas\n \nrounded_cost\n\n\nFROM\n \nbaked_goods\n;\n\n\n\n\n\n\n\nFind the bakery\u2019s distance from the market rounded to two decimal places. Be sure to alias the column as \ndistance_from_market\n.\n\n\n1\n2\nSELECT\n \nROUND\n(\ndistance\n,\n \n2\n)\n \nas\n \ndistance_from_market\n\n\nFROM\n \nbakeries\n;\n\n\n\n\n\n\n\nA couple more useful numeric SQL functions are included below: \nMAX\n and \nMIN\n. \nMAX(n1,n2,n3,...)\n: returns the greatest value in the set of the input numeric expressions \nMIN(n1,n2,n3,...)\n: returns the least value in the set of the input numeric expressions\n\n\nIn our \nbaked_goods table\n, in addition to the numeric \ningredients_cost\n we have information about the packaging cost located in the packaging_cost column. We can use the MAX function to determine the overall greatest value of cost for each item using the following query:\n\n\n1\n2\nSELECT\n \nid\n,\n \nMAX\n(\ningredients_cost\n,\n \npackaging_cost\n)\n\n\nFROM\n \nbaked_goods\n;\n\n\n\n\n\n\n\nWe also have information about cook time designated as cook_time and cool down time designated as \ncool_down_time\n in the \nbaked_goods\n table. Find the greatest time value for each item in the table.\n\n\n1\n2\nSELECT\n \nid\n,\n \nMAX\n(\ncook_time\n,\n \ncool_down_time\n)\n\n\nFROM\n \nbaked_goods\n;\n\n\n\n\n\n\n\nFind the least time value for each item in the table.\n\n\n1\n2\nSELECT\n \nid\n,\n \nMIN\n(\ncook_time\n,\n \ncool_down_time\n)\n\n\nFROM\n \nbaked_goods\n;\n\n\n\n\n\n\n\nString manipulation can be useful to derive information from columns. We\u2019ll cover a couple of the common string functions here.\n\n\nA common use case for string manipulation in SQL is concatenation of strings. In SQLite, this is written as:\n\n\n1\nSELECT\n \nstring1\n \n||\n \n' '\n \n||\n \nstring2\n;\n\n\n\n\n\n\n\nFor example, the bakeries table contains both city and state columns. In order to create a route for these columns, we use the \n||\n function to concatenate them as in the following query:\n\n\n1\n2\nSELECT\n \ncity\n \n||\n \n' '\n \n||\n \nstate\n \nas\n \nlocation\n\n\nFROM\n \nbakeries\n;\n\n\n\n\n\n\n\nString functions are again, very database specific, and it is best practice to consult documentation before proceeding. \n\n\nCombine the \nfirst_name\n and \nlast_name\n columns from the bakeries table as the full_name to identify the owners of the bakeries. Be sure to add a space between the names in the \nfull_name\n as shown in the example.\n\n\n1\nSELECT\n \nfirst_name\n \n||\n \n' '\n \n||\n \nlast_name\n \nas\n \nfull_name\n \nFROM\n \nbakeries\n;\n\n\n\n\n\n\n\nAnother useful string function in SQL is \nREPLACE()\n:\n\n\n1\nREPLACE\n(\nstring\n,\nfrom_string\n,\nto_string\n)\n\n\n\n\n\nThe function returns the string string with all occurrences of the string \nfrom_string\n replaced by the string to_string. For example in \nbaked_goods\n, there is a column named ingredients. The ingredients strings are formatted with underscores, such as \nbaking_soda\n and \nvanilla_extract\n. To make these values more readable, we might like to replace the underscores with spaces. We can do so by using the following query:\n\n\n1\n2\nSELECT\n \nid\n,\n \nREPLACE\n(\ningredients\n,\n'_'\n,\n' '\n)\n \nas\n \nitem_ingredients\n\n\nfrom\n \nbaked_goods\n;\n\n\n\n\n\n\n\nAny time \nenriched_flour\n appears in the ingredients list, we\u2019d like to replace it with just flour. Apply this transformation and also be sure to remove the underscore in \nenriched_flour\n.\n\n\n1\n2\nSELECT\n \nREPLACE\n(\ningredients\n,\n'enriched_'\n,\n' '\n)\n \nas\n \nitem_ingredients\n\n\nFROM\n \nbaked_goods\n;\n\n\n\n\n\n\n\nWhat can we generalize so far?\n\u00b6\n\n\n\n\nDate Functions:\n\n\nDATETIME\n; Returns the date and time of the column specified. This can be modified to return only the date or only the time.\n\n\nDATETIME(time1, +X hours, Y minutes, Z days)\n: Increments the specificed column by a given number of hours, minutes, or days.\n\n\n\n\n\n\nNumeric Functions:\n\n\n(number1 + number2);\n: Returns the sum of two numbers, or other mathematical operations, accordingly.\n\n\nCAST(number1 AS REAL) / number2;\n: Returns the result as a real number by casting one of numeric inputs as a real number\n\n\nROUND(number, precision);\n: Returns the numeric value rounded off to the next value specified.\n\n\n\n\n\n\nString Functions:\n\n\n'string1' || ' ' || 'string2';\n: Concatenates string1 and string 2, with a space between.\n\n\nREPLACE(string,from_string,to_string)\n: Returns the string with all occurrences of the string \nfrom_string\n replaced by the string \nto_string\n.",
            "title": "Codecademy, SQL, Table Transformation"
        },
        {
            "location": "/Codecademy SQL, Table Transformation/#non-correlated-subqueries",
            "text": "A non-correlated subquery is a subquery that can be run independently of the outer query and as we saw, can be used to complete a multi-step transformation.  Perhaps we\u2019d like to look at a selection of  flights  whose origin airport is a seaplane base, designated by  'SEAPLANE_BASE' . The facility type of an airport is located in the  fac_type  field of the  airports  table.  1\n2 SELECT   *   FROM   flights   WHERE   origin   in   ( \n     SELECT   code   FROM   airports   WHERE   fac_type   =   'SEAPLANE_BASE' );    Using the same pattern, find flight information about  flights  where the Federal Aviation Administration region ( faa_region ) is the Southern region ( 'ASO' ).  1\n2 SELECT   *   FROM   flights   WHERE   origin   in   ( \n     SELECT   code   FROM   airports   WHERE   faa_region   =   'ASO' );    Perform transformations on a single table. For instance, sometimes we need to aggregate in multiple steps \u2013 like taking an average of a count.  Imagine you\u2019d like to know how many  flights  there are on average, for all Fridays in a given month from the  flights  table. First, we\u2019d need to calculate the number of  flights  per day, and then we\u2019d need to calculate the average based on the daily flight count for each day of the week. We can do this all in one step using a subquery:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 SELECT   a . dep_month , \n        a . dep_day_of_week , \n        AVG ( a . flight_count )   AS   average_ ` flights ` \n     FROM   ( \n         SELECT   dep_month , \n                dep_day_of_week , \n                dep_date , \n                COUNT ( * )   AS   flight_count \n         FROM   ` flights ` \n         GROUP   BY   1 , 2 , 3 \n         )   a  GROUP   BY   1 , 2  ORDER   BY   1 , 2 ;    The inner query provides the count of flights by day, and the outer query uses the inner query\u2019s result set to compute the average by day of week of a given month.  Using a subquery, find the average total distance flown by day of week and month. Be sure to alias the outer query as  average_distance  and the inner query as  flight_distance .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 SELECT   a . dep_month , \n        a . dep_day_of_week , \n        AVG ( a . flight_distance )   AS   average_distance \n     FROM   ( \n         SELECT   dep_month , \n               dep_day_of_week , \n                dep_date , \n                sum ( distance )   AS   flight_distance \n         FROM   ` flights ` \n         GROUP   BY   1 , 2 , 3 \n     )   a  GROUP   BY   1 , 2  ORDER   BY   1 , 2 ;",
            "title": "Non-Correlated Subqueries"
        },
        {
            "location": "/Codecademy SQL, Table Transformation/#correlated-subqueries",
            "text": "In a correlated subquery, the subquery can not be run independently of the outer query. The order of operations is important in a correlated subquery:  A row is processed in the outer query. Then, for that particular row in the outer query, the subquery is executed.  This means that for each row processed by the outer query, the subquery will also be processed for that row. In this example, we will find the list of all flights whose distance is above average for their carrier (query on a query, same table).  1\n2\n3\n4\n5\n6 SELECT   id  FROM   ` flights `   AS   f  WHERE   distance   >   ( \n     SELECT   AVG ( distance ) \n     FROM   ` flights ` \n     WHERE   carrier   =   f . carrier );    In the above query the inner query has to be re-executed for each flight. Correlated subqueries may appear elsewhere besides the  WHERE  clause, they can also appear in the  SELECT .  Find the id of the flights whose distance is below average for their carrier.  1\n2\n3\n4\n5\n6 SELECT   id  FROM   flights   AS   f  WHERE   distance   <   ( \n     SELECT   AVG ( distance ) \n     FROM   flights \n     WHERE   carrier   =   f . carrier );    It would also be interesting to order flights by giving them a sequence number based on time, by carrier. For instance, assuming flight_id increments with each additional flight, we could use the following query to view flights by carrier, flight id, and sequence number:  1\n2\n3\n4\n5\n6\n7 SELECT   carrier ,   id , \n     ( SELECT   COUNT ( * ) \n         FROM   flights   f \n         WHERE   f . id   <   flights . id \n         AND   f . carrier = flights . carrier )   +   1  \n         AS   flight_sequence_number \n         FROM   flights ;    Using the same pattern, write a query to view flights by origin, flight id, and sequence number. Alias the sequence number column as  flight_sequence_number .  1\n2\n3\n4\n5\n6\n7 SELECT   origin ,   id , \n     ( SELECT   COUNT ( * ) \n     FROM   flights   f \n     WHERE   f . id   <   flights . id \n     AND   f . origin = flights . origin )   +   1  \n     AS   flight_sequence_number \n     FROM   flights ;",
            "title": "Correlated Subqueries"
        },
        {
            "location": "/Codecademy SQL, Table Transformation/#what-can-we-generalize-so-far",
            "text": "Subqueries are used to complete an SQL transformation by nesting one query within another query.  A non-correlated subquery is a subquery that can be run independently of the outer query and can be used to complete a multi-step transformation.  A correlated subquery is a subquery that cannot be run independently of the outer query. The order of operations in a correlated subquery is as follows:  A row is processed in the outer query.  Then, for that particular row in the outer query, the subquery is executed.  Set Operations.     Unions allow us to utilize information from multiple tables in our queries. In this lesson, we\u2019ll utilize data from an e-commerce store. Let\u2019s explore the available data we\u2019ll be using.   Four tables:  new_products ,  legacy_products ,  order_items  and  order_items_historic .  In our database, we have products tables that contain metadata about each product in the store. Select ten rows from the  new_products  table.  1 SELECT   *   FROM   new_products   LIMIT   10 ;",
            "title": "What can we generalize so far?"
        },
        {
            "location": "/Codecademy SQL, Table Transformation/#merging-tables-together",
            "text": "Sometimes, in order to answer certain questions based on data, we need to merge two tables together and then query the merged result. Perhaps we have two tables that contain information about products in an ecommerce store that we would like to combine. There are two ways of doing this:   Merge the rows, called a  JOIN .  Merge the columns, called a  UNION .",
            "title": "Merging Tables Together"
        },
        {
            "location": "/Codecademy SQL, Table Transformation/#union",
            "text": "We\u2019ll focus on unions here. Union combines the result of two or more  SELECT  statements, using the following syntax:  1\n2\n3 SELECT   column_name ( s )   FROM   table1  UNION  SELECT   column_name ( s )   FROM   table2 ;    Each  SELECT  statement within the  UNION  must have the same number of columns with similar data types. The columns in each  SELECT  statement must be in the same order. By default, the  UNION  operator selects only distinct values.  Suppose we are a growing ecommerce store and recently acquired another store to diversify our offering. The product data still exists in two separate tables: a  legacy_products  table and a  new_products  table. To get the complete list of product names from both tables, we can perform the following union.  1\n2\n3 SELECT   item_name   FROM   legacy_products  UNION   SELECT   item_name   FROM   new_products ;    Select a complete list of brand names from the  legacy_products  and  new_products  tables.  1\n2\n3 SELECT   brand   FROM   legacy_products  UNION  SELECT   brand   FROM   new_products ;    What if we wanted to allow duplicate values? We can do this by using the  ALL  keyword with  UNION , with the following syntax:  1\n2\n3 SELECT   column_name ( s )   FROM   table1  UNION   ALL  SELECT   column_name ( s )   FROM   table2 ;    In our ecommerce store, if we learned that we had records from historic order items in an additional table, we could use the following query to combine the tables for a complete analysis of sale price:  1\n2\n3 SELECT   id ,   sale_price   FROM   order_items  UNION   ALL  SELECT   id ,   sale_price   FROM   order_items_historic ;    Then we can perform an analysis on top of the combined result set, like finding the total count of order items.  1\n2\n3\n4 SELECT   count ( * )   FROM   ( \n     SELECT   id ,   sale_price   FROM   order_items \n     UNION   ALL \n     SELECT   id ,   sale_price   FROM   order_items_historic )   as   a ;    Using the same pattern, utilize a subquery to find the average sale price over both  order_items  and  order_items_historic  tables.  1\n2\n3\n4\n5 SELECT   id ,   avg ( a . sale_price )   FROM   ( \n     SELECT   id ,   sale_price   FROM   order_items \n     UNION   ALL \n     SELECT   id ,   sale_price   FROM   order_items_historic )   AS   a  \n     GROUP   BY   1 ;    Before running the top analysis, create an alias a with the preliminary results, run the  avg(a.sale_price) , and group by 1 to view separate records and not a unique aggregate record!!!",
            "title": "UNION"
        },
        {
            "location": "/Codecademy SQL, Table Transformation/#intersect",
            "text": "\u2026is used to combine two  SELECT  statements, but returns rows only from the first  SELECT  statement that are identical to a row in the second  SELECT  statement. This means that it returns only common rows returned by the two  SELECT  statements.  1\n2\n3 SELECT   column_name ( s )   FROM   table1  INTERSECT  SELECT   column_name ( s )   FROM   table2 ;    For instance, we might want to know what brands in our newly acquired store are also in our legacy store. We can do so using the following query:  1\n2\n3 SELECT   brand   FROM   new_products  INTERSECT  SELECT   brand   FROM   legacy_products ;    Select the items in the category column that are both in the newly acquired  new_products  table and the  legacy_products  table.  1\n2\n3 SELECT   category   FROM   new_products  INTERSECT  SELECT   category   FROM   legacy_products ;",
            "title": "INTERSECT"
        },
        {
            "location": "/Codecademy SQL, Table Transformation/#except",
            "text": "\u2026is constructed in the same way, but returns distinct rows from the first  SELECT  statement that aren\u2019t output by the second  SELECT  statement.  1\n2\n3 SELECT   column_name ( s )   FROM   table1  EXCEPT  SELECT   column_name ( s )   FROM   table2 ;    Suppose we want to see if there are any categories that are in the  new_products  table that aren\u2019t in the  legacy_products  table. We can use an  EXCEPT  query to perform this analysis:  1\n2\n3 SELECT   category   FROM   new_products  EXCEPT  SELECT   category   FROM   legacy_products ;    Conversely, select the items in the category column that are in the  legacy_products  table and not in the  new_products  table.  1\n2\n3 SELECT   category   FROM   legacy_products  EXCEPT  SELECT   category   FROM   new_products ;",
            "title": "EXCEPT"
        },
        {
            "location": "/Codecademy SQL, Table Transformation/#what-can-we-generalize-so-far_1",
            "text": "The  UNION  clause allows us to utilize information from multiple tables in our queries.  The  UNION ALL  clause allows us to utilize information from multiple tables in our queries, including duplicate values.  INTERSECT  is used to combine two  SELECT  statements, but returns rows only from the first  SELECT  statement that are identical to a row in the second  SELECT  statement.  EXCEPT  returns distinct rows from the first  SELECT  statement that aren\u2019t output by the second  SELECT  statement",
            "title": "What can we generalize so far?"
        },
        {
            "location": "/Codecademy SQL, Table Transformation/#conditional-aggregates",
            "text": "Aggregate functions compute a single result from a set of multiple input values. You can think of aggregate data as data collected from multiple rows at a time. In this lesson, we\u2019ll continue learning about aggregate functions by focusing on conditionals, sums, and combining aggregate functions.  Conditional Aggregates are aggregate functions that compute a result set based on a given set of conditions.  The count function is an aggregate function, since it aggregates data from multiple rows.  Count the number of rows in the  flights  table, representing the total number of  flights  contained in the table.  1 SELECT   COUNT ( * )   FROM   flights ;    While working with databases, it\u2019s common to have empty or unknown \u201ccells\u201d in data tables.  What do we do when we need to test whether a value is or is not null? We use the special keywords  IS NULL  or  IS NOT NULL  in the  WHERE  clause ( = NULL  does not work).  Count the number of rows from the  flights  table, where  arr_time  is not null and the destination is  'ATL' .  1 SELECT   COUNT ( * )   FROM   flights   WHERE   arr_time   IS   NOT   NULL   AND   destination   =   'ATL' ;    Almost every programming language has a way to represent \u201cif, then, else\u201d, or conditional logic. In SQL, we represent this logic with the  CASE  statement, as follows:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 SELECT \n     CASE \n         WHEN   elevation   <   500   THEN   'Low' \n         WHEN   elevation   BETWEEN   500   AND   1999   THEN   'Medium' \n         WHEN   elevation   >=   2000   THEN   'High' \n         ELSE   'Unknown' \n     END   AS   elevation_tier ,  \n     COUNT ( * )  FROM   airports  GROUP   BY   1 ;    In the above statement,  END  is required to terminate the statement, but  ELSE  is optional. If  ELSE  is not included, the result will be  NULL . Also notice the shorthand method of referencing columns to use in  GROUP BY , so we don\u2019t have to rewrite the entire  CASE  Statement.  Modify the case statement\u2019s such that when the elevation is less than 250, the  elevation_tier  column returns  'Low' , when between 250 and 1749 it returns  'Medium' , and when greater than or equal to 1750 it returns  'High' .  Be sure to alias the conditional statement as  elevation_tier , in your query.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 SELECT \n     CASE \n         WHEN   elevation   <   250   THEN   'Low' \n         WHEN   elevation   BETWEEN   250   AND   1749   THEN   'Medium' \n         WHEN   elevation   >=   1750   THEN   'High' \n         ELSE   'Unknown' \n     END   AS   elevation_tier ,  \n     COUNT ( * )  FROM   airports  GROUP   BY   1 ;    Sometimes you want to look at an entire result set, but want to implement conditions on certain aggregates.  For instance, maybe you want to identify the total amount of airports as well as the total amount of airports with high elevation in the same result set. We can accomplish this by putting a  CASE WHEN  statement in the aggregate.  1\n2\n3\n4 SELECT   state ,  \n     COUNT ( CASE   WHEN   elevation   >=   2000   THEN   1   ELSE   NULL   END )   as   count_high_elevation_aiports   FROM   airports   GROUP   BY   state ;    Using the same pattern, write a query to count the number of low elevation airports by state where low elevation is defined as less than 1000 ft. Be sure to alias the counted airports as  count_low_elevation_airports .  1\n2\n3\n4 SELECT   state ,  \n     COUNT ( CASE   WHEN   elevation   <   1000   THEN   1   ELSE   NULL   END )   as   count_low_elevation_aiports   FROM   airports   GROUP   BY   state ;    We can do that same thing for other aggregates like  SUM() . For instance, if we wanted to sum the total flight distance and compare that to the sum of flight distance from a particular airline (in this case, United Airlines) by origin airport, we could run the following query:  sum(distance)  for all carriers ( total_flight_distance )  sum(distance)  for  'UA'  only (others are turned to 0) ( total_united_flight_distance ).  1\n2\n3 SELECT   origin ,   sum ( distance )   as   total_flight_distance ,   sum ( CASE   WHEN   carrier   =   'UA'   THEN   distance   ELSE   0   END )   as   total_united_flight_distance   FROM   flights   GROUP   BY   origin ;    Using the same pattern, find both the total flight distance as and flight distance by origin for Delta ( carrier = 'DL' ).  Alias the flight distance as  total_flight_distance  and the and flight distance by origin as  total_delta_flight_distance .  1\n2\n3 SELECT   origin ,   sum ( distance )   as   total_flight_distance ,   sum ( CASE   WHEN   carrier   =   'DL'   THEN   distance   ELSE   0   END )   as   total_delta_flight_distance   FROM   flights   GROUP   BY   origin ;    Using the same pattern, find the percentage of  flights  from Delta by origin ( carrier = 'DL' ):  1\n2 SELECT   origin ,   100 . 0 * ( sum ( CASE   WHEN   carrier   =   'DL'   THEN   distance   ELSE   0   END ) / sum ( distance ))   as   percentage_flight_distance_from_delta   FROM   flights   GROUP   BY   origin ;    Find the percentage of high elevation airports ( elevation >= 2000 ) by state from the  airports  table. In the query, alias the percentage column as  percentage_high_elevation_airports .  (sum of '1') / count(*)  and not  (count of '1') / count(*) :  1 SELECT   state ,   100 . 0   *   sum ( CASE   WHEN   elevation   >=   2000   THEN   1   ELSE   0   END )   /   count ( * )    as   percentage_high_elevation_airports   FROM   airports   GROUP   BY   state ;",
            "title": "Conditional Aggregates"
        },
        {
            "location": "/Codecademy SQL, Table Transformation/#what-can-we-generalize-so-far_2",
            "text": "Conditional Aggregates are aggregate functions the compute a result set based on a given set of conditions.  NULL  can be used to denote an empty field value  CASE  statements allow for custom classification of data  CASE  statements can be used inside aggregates (like  SUM()  and  COUNT() ) to provide filtered measures",
            "title": "What can we generalize so far?"
        },
        {
            "location": "/Codecademy SQL, Table Transformation/#date-number-and-string-functions",
            "text": "Oftentimes, data in columns of tables is not in the exact format we need to complete our desired analysis. We may need to extract a date from a full timestamp, manipulate a number, or combine first and last name columns to create a full name.  In this lesson, we\u2019ll be learning about some of SQL\u2019s built-in functions for transforming dates, numbers and strings. We\u2019ll be using database of bakeries in this lesson.  It is important to note that date, number, and string functions are highly database dependent. Here, we focus on built-in functions in the SQLite database management system.  Select ten rows from the bakeries table:  1 SELECT   *   FROM   bakeries   LIMIT   10 ;    We\u2019ll begin with dates. Dates are often written in the following format   Date: \u201cYYYY-MM-DD\u201d  Datetime or Timestamp: \u201cYYYY-MM-DD hh: mm:ss\u201d   We can use SQL\u2019s date functions to transform data into a desired format. Since date functions can be database specific, verify the functions that exist on your relational database management system. For example, this statement:  1 SELECT   DATETIME ( manufacture_time )   FROM   baked_goods ;    Using the datetime function, select the date and time of all deliveries in the  baked_goods  table using the column  delivery_time .  1 SELECT   DATETIME ( delivery_time )   FROM   baked_goods ;    Now let\u2019s assume that we have a column in our  baked_goods  table named  manufacture_time  in the format \u201cYYYY-MM-DD hh: mm:ss\u201d. We\u2019d like to know the number of baked_goods manufactured by day, and not by second. We can use the  DATE()  function to easily convert timestamps to dates and complete the following query:  1\n2\n3 SELECT   DATE ( manufacture_time ),   count ( * )   as   count_baked_goods  FROM   baked_goods  GROUP   BY   DATE ( manufacture_time );    Similarly, we can query the time with:  1\n2\n3 SELECT   TIME ( manufacture_time ),   count ( * )   as   count_baked_goods  FROM   baked_goods  GROUP   BY   TIME ( manufacture_time );    Find the number of baked goods by date of delivery. Be sure to alias the total count of baked goods as  count_baked_goods .  1 SELECT   DATE ( delivery_time ),   count ( * )   as   count_baked_goods   FROM   baked_goods   GROUP   BY   DATE ( delivery_time );    Given a datepart and a column of date or timestamp data type, we can increment date or timestamp values by a specified interval. For example, in SQLite, the statement:  1 DATETIME ( time1 ,   '+3 hours' ,   '40 minutes' ,   '2 days' );    Would return a time 3 hours, 20 minutes, and 2 days after time1.  Imagine that each dessert in our  baked_goods  table is inspected 2 hours, 30 minutes, and 1 day after the manufacture time. To derive the inspection date for each baked good, we can use the following query:  1\n2 SELECT   DATETIME ( manufacture_time ,   '+2 hours' ,   '30 minutes' ,   '1 day' )   as   inspection_time  FROM   baked_goods ;    Each of the baked goods is packaged by Baker\u2019s Market exactly five hours, twenty minutes, and two days after the delivery (designated by delivery_time). Create a query returning all the packaging times for the goods in the  baked_goods  table. Be sure to alias the package time column as  package_time .  1\n2 SELECT   DATETIME ( delivery_time ,   '+5 hours' ,   '20 minutes' ,   '2 days' )   as   package_time  FROM   baked_goods ;    Numeric functions can be used to transform numbers. Some common SQLite mathematical functions are included below that take numeric data types as inputs:   SELECT (number1 + number2); : returns the sum of two numbers. Similar, SQL can be used for subtraction, multiplication, and division.  SELECT CAST(number1 AS REAL) / number3; : returns the result as a real number by casting one of the values as a real number, rather than an integer.  SELECT ROUND(number, precision); : returns the numeric value rounded off to the next value specified.   In our  baked_goods  table, we have information about cost designated by  ingredients_cost . For accounting purposes, we\u2019d like to make sure that each ingredient cost is rounded to four decimal places rather than two, to account for currency fluctuations.  1\n2 SELECT   ROUND ( ingredients_cost ,   4 )   as   rounded_cost  FROM   baked_goods ;    Find the bakery\u2019s distance from the market rounded to two decimal places. Be sure to alias the column as  distance_from_market .  1\n2 SELECT   ROUND ( distance ,   2 )   as   distance_from_market  FROM   bakeries ;    A couple more useful numeric SQL functions are included below:  MAX  and  MIN .  MAX(n1,n2,n3,...) : returns the greatest value in the set of the input numeric expressions  MIN(n1,n2,n3,...) : returns the least value in the set of the input numeric expressions  In our  baked_goods table , in addition to the numeric  ingredients_cost  we have information about the packaging cost located in the packaging_cost column. We can use the MAX function to determine the overall greatest value of cost for each item using the following query:  1\n2 SELECT   id ,   MAX ( ingredients_cost ,   packaging_cost )  FROM   baked_goods ;    We also have information about cook time designated as cook_time and cool down time designated as  cool_down_time  in the  baked_goods  table. Find the greatest time value for each item in the table.  1\n2 SELECT   id ,   MAX ( cook_time ,   cool_down_time )  FROM   baked_goods ;    Find the least time value for each item in the table.  1\n2 SELECT   id ,   MIN ( cook_time ,   cool_down_time )  FROM   baked_goods ;    String manipulation can be useful to derive information from columns. We\u2019ll cover a couple of the common string functions here.  A common use case for string manipulation in SQL is concatenation of strings. In SQLite, this is written as:  1 SELECT   string1   ||   ' '   ||   string2 ;    For example, the bakeries table contains both city and state columns. In order to create a route for these columns, we use the  ||  function to concatenate them as in the following query:  1\n2 SELECT   city   ||   ' '   ||   state   as   location  FROM   bakeries ;    String functions are again, very database specific, and it is best practice to consult documentation before proceeding.   Combine the  first_name  and  last_name  columns from the bakeries table as the full_name to identify the owners of the bakeries. Be sure to add a space between the names in the  full_name  as shown in the example.  1 SELECT   first_name   ||   ' '   ||   last_name   as   full_name   FROM   bakeries ;    Another useful string function in SQL is  REPLACE() :  1 REPLACE ( string , from_string , to_string )   \nThe function returns the string string with all occurrences of the string  from_string  replaced by the string to_string. For example in  baked_goods , there is a column named ingredients. The ingredients strings are formatted with underscores, such as  baking_soda  and  vanilla_extract . To make these values more readable, we might like to replace the underscores with spaces. We can do so by using the following query:  1\n2 SELECT   id ,   REPLACE ( ingredients , '_' , ' ' )   as   item_ingredients  from   baked_goods ;    Any time  enriched_flour  appears in the ingredients list, we\u2019d like to replace it with just flour. Apply this transformation and also be sure to remove the underscore in  enriched_flour .  1\n2 SELECT   REPLACE ( ingredients , 'enriched_' , ' ' )   as   item_ingredients  FROM   baked_goods ;",
            "title": "Date, Number, and String Functions"
        },
        {
            "location": "/Codecademy SQL, Table Transformation/#what-can-we-generalize-so-far_3",
            "text": "Date Functions:  DATETIME ; Returns the date and time of the column specified. This can be modified to return only the date or only the time.  DATETIME(time1, +X hours, Y minutes, Z days) : Increments the specificed column by a given number of hours, minutes, or days.    Numeric Functions:  (number1 + number2); : Returns the sum of two numbers, or other mathematical operations, accordingly.  CAST(number1 AS REAL) / number2; : Returns the result as a real number by casting one of numeric inputs as a real number  ROUND(number, precision); : Returns the numeric value rounded off to the next value specified.    String Functions:  'string1' || ' ' || 'string2'; : Concatenates string1 and string 2, with a space between.  REPLACE(string,from_string,to_string) : Returns the string with all occurrences of the string  from_string  replaced by the string  to_string .",
            "title": "What can we generalize so far?"
        },
        {
            "location": "/Introduction to SQL JOINs/",
            "text": "Foreword\n\n\nNotes.\n\n\n\n\nUsing the code\n\u00b6\n\n\nSeven different ways you can return data from two relational tables; excluding cross joins and self referencing joins:\n\n\n\n\nINNER JOIN\n\n\nLEFT JOIN\n\n\nRIGHT JOIN\n\n\nOUTER JOIN\n\n\nLEFT JOIN\n excluding \nINNER JOIN\n\n\nRIGHT JOIN\n excluding \nINNER JOIN\n\n\nOUTER JOIN\n excluding \nINNER JOIN\n\n\n\n\n\n\nFor the sake of this article, 5, 6, and 7 are \nLEFT EXCLUDING JOIN\n, \nRIGHT\n Excluding \nJOIN\n, and \nOUTER\n Excluding \nJOIN\n, respectively. Some may argue that 5, 6, and 7 are not really joining the two tables, but for simplicity, let\u2019s refer to these as joins because you use a SQL join in each of these queries (but exclude some records with a \nWHERE\n clause).\n\n\nINNER JOIN\n\u00b6\n\n\n\n\nThis is the simplest, most understood join and is the most common. This query will return all of the records in the left table (\nTable_A\n) that have a matching record in the right table (\nTable_B\n). This join is written as follows:\n\n\n1\n2\n3\n4\nSELECT\n \n<\nselect_list\n>\n \n\nFROM\n \nTable_A\n \nA\n\n\nINNER\n \nJOIN\n \nTable_B\n \nB\n\n\nON\n \nA\n.\nKey\n \n=\n \nB\n.\nKey\n\n\n\n\n\n\n\nLEFT JOIN\n\u00b6\n\n\n\n\nThis query will return all of the records in the left table (\nTable_A\n) regardless if any of those records have a match in the right table (\nTable_B\n). It will also return any matching records from the right table. This join is written as follows:\n\n\n1\n2\n3\n4\nSELECT\n \n<\nselect_list\n>\n\n\nFROM\n \nTable_A\n \nA\n\n\nLEFT\n \nJOIN\n \nTable_B\n \nB\n\n\nON\n \nA\n.\nKey\n \n=\n \nB\n.\nKey\n\n\n\n\n\n\n\nRIGHT JOIN\n\u00b6\n\n\n\n\nThis query will return all of the records in the right table (\nTable_B\n) regardless if any of those records have a match in the left table (\nTable_A\n). It will also return any matching records from the left table. This join is written as follows:\n\n\n1\n2\n3\n4\nSELECT\n \n<\nselect_list\n>\n\n\nFROM\n \nTable_A\n \nA\n\n\nRIGHT\n \nJOIN\n \nTable_B\n \nB\n\n\nON\n \nA\n.\nKey\n \n=\n \nB\n.\nKey\n\n\n\n\n\n\n\nOUTER JOIN\n\u00b6\n\n\n\n\nThis Join can also be referred to as a \nFULL OUTER JOIN\n or a \nFULL JOIN\n. This query will return all of the records from both tables, joining records from the left table (\nTable_A\n) that match records from the right table (\nTable_B\n). This join is written as follows:\n\n\n1\n2\n3\n4\nSELECT\n \n<\nselect_list\n>\n\n\nFROM\n \nTable_A\n \nA\n\n\nFULL\n \nOUTER\n \nJOIN\n \nTable_B\n \nB\n\n\nON\n \nA\n.\nKey\n \n=\n \nB\n.\nKey\n\n\n\n\n\n\n\nLEFT\n Excluding \nJOIN\n\u00b6\n\n\n\n\nThis query will return all of the records in the left table (\nTable_A\n) that do not match any records in the right table (\nTable_B\n). This join is written as follows:\n\n\n1\n2\n3\n4\n5\nSELECT\n \n<\nselect_list\n>\n \n\nFROM\n \nTable_A\n \nA\n\n\nLEFT\n \nJOIN\n \nTable_B\n \nB\n\n\nON\n \nA\n.\nKey\n \n=\n \nB\n.\nKey\n\n\nWHERE\n \nB\n.\nKey\n \nIS\n \nNULL\n\n\n\n\n\n\n\nRIGHT\n Excluding \nJOIN\n\u00b6\n\n\n\n\nThis query will return all of the records in the right table (\nTable_B\n) that do not match any records in the left table (\nTable_A\n). This join is written as follows:\n\n\n1\n2\n3\n4\n5\nSELECT\n \n<\nselect_list\n>\n\n\nFROM\n \nTable_A\n \nA\n\n\nRIGHT\n \nJOIN\n \nTable_B\n \nB\n\n\nON\n \nA\n.\nKey\n \n=\n \nB\n.\nKey\n\n\nWHERE\n \nA\n.\nKey\n \nIS\n \nNULL\n\n\n\n\n\n\n\nOUTER\n Excluding \nJOIN\n\u00b6\n\n\n\n\nThis query will return all of the records in the left table (\nTable_A\n) and all of the records in the right table (\nTable_B\n) that do not match. I have yet to have a need for using this type of join, but all of the others, I use quite frequently. This join is written as follows:\n\n\n1\n2\n3\n4\n5\nSELECT\n \n<\nselect_list\n>\n\n\nFROM\n \nTable_A\n \nA\n\n\nFULL\n \nOUTER\n \nJOIN\n \nTable_B\n \nB\n\n\nON\n \nA\n.\nKey\n \n=\n \nB\n.\nKey\n\n\nWHERE\n \nA\n.\nKey\n \nIS\n \nNULL\n \nOR\n \nB\n.\nKey\n \nIS\n \nNULL\n\n\n\n\n\n\n\nExamples\n\u00b6\n\n\nSuppose we have two tables, \nTABLE_A\n and \nTABLE_B\n. The data in these tables are shown below:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nTABLE_A\n\n  \nPK\n \nValue\n\n\n---- ----------\n\n   \n1\n \nFOX\n\n   \n2\n \nCOP\n\n   \n3\n \nTAXI\n\n   \n6\n \nWASHINGTON\n\n   \n7\n \nDELL\n\n   \n5\n \nARIZONA\n\n   \n4\n \nLINCOLN\n\n  \n10\n \nLUCENT\n\n\n\nTABLE_B\n\n  \nPK\n \nValue\n\n\n---- ----------\n\n   \n1\n \nTROT\n\n   \n2\n \nCAR\n\n   \n3\n \nCAB\n\n   \n6\n \nMONUMENT\n\n   \n7\n \nPC\n\n   \n8\n \nMICROSOFT\n\n   \n9\n \nAPPLE\n\n  \n11\n \nSCOTCH\n\n\n\n\n\n\n\nThe results of the seven joins are shown below:\n\n\nINNER JOIN\n\n\n1\n2\n3\n4\n5\n6\n--INNER JOIN\n\n\nSELECT\n \nA\n.\nPK\n \nAS\n \nA_PK\n,\n \nA\n.\nValue\n \nAS\n \nA_Value\n,\n\n       \nB\n.\nValue\n \nAS\n \nB_Value\n,\n \nB\n.\nPK\n \nAS\n \nB_PK\n\n\nFROM\n \nTable_A\n \nA\n\n\nINNER\n \nJOIN\n \nTable_B\n \nB\n\n\nON\n \nA\n.\nPK\n \n=\n \nB\n.\nPK\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nA_PK\n \nA_Value\n    \nB_Value\n    \nB_PK\n\n\n---- ---------- ---------- ----\n\n   \n1\n \nFOX\n        \nTROT\n          \n1\n\n   \n2\n \nCOP\n        \nCAR\n           \n2\n\n   \n3\n \nTAXI\n       \nCAB\n           \n3\n\n   \n6\n \nWASHINGTON\n \nMONUMENT\n      \n6\n\n   \n7\n \nDELL\n       \nPC\n            \n7\n\n\n\n(\n5\n \nrow\n(\ns\n)\n \naffected\n)\n\n\n\n\n\n\n\nLEFT JOIN\n\n\n1\n2\n3\n4\n5\n6\n--LEFT JOIN\n\n\nSELECT\n \nA\n.\nPK\n \nAS\n \nA_PK\n,\n \nA\n.\nValue\n \nAS\n \nA_Value\n,\n\n\nB\n.\nValue\n \nAS\n \nB_Value\n,\n \nB\n.\nPK\n \nAS\n \nB_PK\n\n\nFROM\n \nTable_A\n \nA\n\n\nLEFT\n \nJOIN\n \nTable_B\n \nB\n\n\nON\n \nA\n.\nPK\n \n=\n \nB\n.\nPK\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nA_PK\n \nA_Value\n    \nB_Value\n    \nB_PK\n\n\n---- ---------- ---------- ----\n\n   \n1\n \nFOX\n        \nTROT\n          \n1\n\n   \n2\n \nCOP\n        \nCAR\n           \n2\n\n   \n3\n \nTAXI\n       \nCAB\n           \n3\n\n   \n4\n \nLINCOLN\n    \nNULL\n       \nNULL\n\n   \n5\n \nARIZONA\n    \nNULL\n       \nNULL\n\n   \n6\n \nWASHINGTON\n \nMONUMENT\n      \n6\n\n   \n7\n \nDELL\n       \nPC\n            \n7\n\n  \n10\n \nLUCENT\n     \nNULL\n       \nNULL\n\n\n\n(\n8\n \nrow\n(\ns\n)\n \naffected\n)\n\n\n\n\n\n\n\nRIGHT JOIN\n\n\n1\n2\n3\n4\n5\n6\n--RIGHT JOIN\n\n\nSELECT\n \nA\n.\nPK\n \nAS\n \nA_PK\n,\n \nA\n.\nValue\n \nAS\n \nA_Value\n,\n\n\nB\n.\nValue\n \nAS\n \nB_Value\n,\n \nB\n.\nPK\n \nAS\n \nB_PK\n\n\nFROM\n \nTable_A\n \nA\n\n\nRIGHT\n \nJOIN\n \nTable_B\n \nB\n\n\nON\n \nA\n.\nPK\n \n=\n \nB\n.\nPK\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nA_PK\n \nA_Value\n    \nB_Value\n    \nB_PK\n\n\n---- ---------- ---------- ----\n\n   \n1\n \nFOX\n        \nTROT\n          \n1\n\n   \n2\n \nCOP\n        \nCAR\n           \n2\n\n   \n3\n \nTAXI\n       \nCAB\n           \n3\n\n   \n6\n \nWASHINGTON\n \nMONUMENT\n      \n6\n\n   \n7\n \nDELL\n       \nPC\n            \n7\n\n\nNULL\n \nNULL\n       \nMICROSOFT\n     \n8\n\n\nNULL\n \nNULL\n       \nAPPLE\n         \n9\n\n\nNULL\n \nNULL\n       \nSCOTCH\n       \n11\n\n\n\n(\n8\n \nrow\n(\ns\n)\n \naffected\n)\n\n\n\n\n\n\n\nOUTER JOIN\n\n\n1\n2\n3\n4\n5\n6\n--OUTER JOIN\n\n\nSELECT\n \nA\n.\nPK\n \nAS\n \nA_PK\n,\n \nA\n.\nValue\n \nAS\n \nA_Value\n,\n\n\nB\n.\nValue\n \nAS\n \nB_Value\n,\n \nB\n.\nPK\n \nAS\n \nB_PK\n\n\nFROM\n \nTable_A\n \nA\n\n\nFULL\n \nOUTER\n \nJOIN\n \nTable_B\n \nB\n\n\nON\n \nA\n.\nPK\n \n=\n \nB\n.\nPK\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nA_PK\n \nA_Value\n    \nB_Value\n    \nB_PK\n\n\n---- ---------- ---------- ----\n\n   \n1\n \nFOX\n        \nTROT\n          \n1\n\n   \n2\n \nCOP\n        \nCAR\n           \n2\n\n   \n3\n \nTAXI\n       \nCAB\n           \n3\n\n   \n6\n \nWASHINGTON\n \nMONUMENT\n      \n6\n\n   \n7\n \nDELL\n       \nPC\n            \n7\n\n\nNULL\n \nNULL\n       \nMICROSOFT\n     \n8\n\n\nNULL\n \nNULL\n       \nAPPLE\n         \n9\n\n\nNULL\n \nNULL\n       \nSCOTCH\n       \n11\n\n   \n5\n \nARIZONA\n    \nNULL\n       \nNULL\n\n   \n4\n \nLINCOLN\n    \nNULL\n       \nNULL\n\n  \n10\n \nLUCENT\n     \nNULL\n       \nNULL\n\n\n\n(\n11\n \nrow\n(\ns\n)\n \naffected\n)\n\n\n\n\n\n\n\nLEFT\n Excluding \nJOIN\n\n\n1\n2\n3\n4\n5\n6\n7\n--LEFT EXCLUDING JOIN\n\n\nSELECT\n \nA\n.\nPK\n \nAS\n \nA_PK\n,\n \nA\n.\nValue\n \nAS\n \nA_Value\n,\n\n\nB\n.\nValue\n \nAS\n \nB_Value\n,\n \nB\n.\nPK\n \nAS\n \nB_PK\n\n\nFROM\n \nTable_A\n \nA\n\n\nLEFT\n \nJOIN\n \nTable_B\n \nB\n\n\nON\n \nA\n.\nPK\n \n=\n \nB\n.\nPK\n\n\nWHERE\n \nB\n.\nPK\n \nIS\n \nNULL\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\nA_PK\n \nA_Value\n    \nB_Value\n    \nB_PK\n\n\n---- ---------- ---------- ----\n\n   \n4\n \nLINCOLN\n    \nNULL\n       \nNULL\n\n   \n5\n \nARIZONA\n    \nNULL\n       \nNULL\n\n  \n10\n \nLUCENT\n     \nNULL\n       \nNULL\n\n\n(\n3\n \nrow\n(\ns\n)\n \naffected\n)\n\n\n\n\n\n\n\nRIGHT\n Excluding \nJOIN\n\n\n1\n2\n3\n4\n5\n6\n7\n-\nRIGHT\n \nEXCLUDING\n \nJOIN\n\n\nSELECT\n \nA\n.\nPK\n \nAS\n \nA_PK\n,\n \nA\n.\nValue\n \nAS\n \nA_Value\n,\n\n\nB\n.\nValue\n \nAS\n \nB_Value\n,\n \nB\n.\nPK\n \nAS\n \nB_PK\n\n\nFROM\n \nTable_A\n \nA\n\n\nRIGHT\n \nJOIN\n \nTable_B\n \nB\n\n\nON\n \nA\n.\nPK\n \n=\n \nB\n.\nPK\n\n\nWHERE\n \nA\n.\nPK\n \nIS\n \nNULL\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\nA_PK\n \nA_Value\n    \nB_Value\n    \nB_PK\n\n\n---- ---------- ---------- ----\n\n\nNULL\n \nNULL\n       \nMICROSOFT\n     \n8\n\n\nNULL\n \nNULL\n       \nAPPLE\n         \n9\n\n\nNULL\n \nNULL\n       \nSCOTCH\n       \n11\n\n\n\n(\n3\n \nrow\n(\ns\n)\n \naffected\n)\n\n\n\n\n\n\n\nOUTER\n Excluding \nJOIN\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n--OUTER EXCLUDING JOIN\n\n\nSELECT\n \nA\n.\nPK\n \nAS\n \nA_PK\n,\n \nA\n.\nValue\n \nAS\n \nA_Value\n,\n\n\nB\n.\nValue\n \nAS\n \nB_Value\n,\n \nB\n.\nPK\n \nAS\n \nB_PK\n\n\nFROM\n \nTable_A\n \nA\n\n\nFULL\n \nOUTER\n \nJOIN\n \nTable_B\n \nB\n\n\nON\n \nA\n.\nPK\n \n=\n \nB\n.\nPK\n\n\nWHERE\n \nA\n.\nPK\n \nIS\n \nNULL\n\n\nOR\n \nB\n.\nPK\n \nIS\n \nNULL\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nA_PK\n \nA_Value\n    \nB_Value\n    \nB_PK\n\n\n---- ---------- ---------- ----\n\n\nNULL\n \nNULL\n       \nMICROSOFT\n     \n8\n\n\nNULL\n \nNULL\n       \nAPPLE\n         \n9\n\n\nNULL\n \nNULL\n       \nSCOTCH\n       \n11\n\n   \n5\n \nARIZONA\n    \nNULL\n       \nNULL\n\n   \n4\n \nLINCOLN\n    \nNULL\n       \nNULL\n\n  \n10\n \nLUCENT\n     \nNULL\n       \nNULL\n\n\n\n(\n6\n \nrow\n(\ns\n)\n \naffected\n)\n\n\n\n\n\n\n\nConclusion\n\u00b6\n\n\nNote on the \nOUTER JOIN\n that the inner joined records are returned first, followed by the right joined records, and then finally the left joined records (at least, that\u2019s how my Microsoft SQL Server did it; this, of course, is without using any \nORDER BY\n statement).",
            "title": "Introduction to SQL JOINs"
        },
        {
            "location": "/Introduction to SQL JOINs/#inner-join",
            "text": "This is the simplest, most understood join and is the most common. This query will return all of the records in the left table ( Table_A ) that have a matching record in the right table ( Table_B ). This join is written as follows:  1\n2\n3\n4 SELECT   < select_list >   FROM   Table_A   A  INNER   JOIN   Table_B   B  ON   A . Key   =   B . Key",
            "title": "INNER JOIN"
        },
        {
            "location": "/Introduction to SQL JOINs/#left-join",
            "text": "This query will return all of the records in the left table ( Table_A ) regardless if any of those records have a match in the right table ( Table_B ). It will also return any matching records from the right table. This join is written as follows:  1\n2\n3\n4 SELECT   < select_list >  FROM   Table_A   A  LEFT   JOIN   Table_B   B  ON   A . Key   =   B . Key",
            "title": "LEFT JOIN"
        },
        {
            "location": "/Introduction to SQL JOINs/#right-join",
            "text": "This query will return all of the records in the right table ( Table_B ) regardless if any of those records have a match in the left table ( Table_A ). It will also return any matching records from the left table. This join is written as follows:  1\n2\n3\n4 SELECT   < select_list >  FROM   Table_A   A  RIGHT   JOIN   Table_B   B  ON   A . Key   =   B . Key",
            "title": "RIGHT JOIN"
        },
        {
            "location": "/Introduction to SQL JOINs/#outer-join",
            "text": "This Join can also be referred to as a  FULL OUTER JOIN  or a  FULL JOIN . This query will return all of the records from both tables, joining records from the left table ( Table_A ) that match records from the right table ( Table_B ). This join is written as follows:  1\n2\n3\n4 SELECT   < select_list >  FROM   Table_A   A  FULL   OUTER   JOIN   Table_B   B  ON   A . Key   =   B . Key",
            "title": "OUTER JOIN"
        },
        {
            "location": "/Introduction to SQL JOINs/#left-excluding-join",
            "text": "This query will return all of the records in the left table ( Table_A ) that do not match any records in the right table ( Table_B ). This join is written as follows:  1\n2\n3\n4\n5 SELECT   < select_list >   FROM   Table_A   A  LEFT   JOIN   Table_B   B  ON   A . Key   =   B . Key  WHERE   B . Key   IS   NULL",
            "title": "LEFT Excluding JOIN"
        },
        {
            "location": "/Introduction to SQL JOINs/#right-excluding-join",
            "text": "This query will return all of the records in the right table ( Table_B ) that do not match any records in the left table ( Table_A ). This join is written as follows:  1\n2\n3\n4\n5 SELECT   < select_list >  FROM   Table_A   A  RIGHT   JOIN   Table_B   B  ON   A . Key   =   B . Key  WHERE   A . Key   IS   NULL",
            "title": "RIGHT Excluding JOIN"
        },
        {
            "location": "/Introduction to SQL JOINs/#outer-excluding-join",
            "text": "This query will return all of the records in the left table ( Table_A ) and all of the records in the right table ( Table_B ) that do not match. I have yet to have a need for using this type of join, but all of the others, I use quite frequently. This join is written as follows:  1\n2\n3\n4\n5 SELECT   < select_list >  FROM   Table_A   A  FULL   OUTER   JOIN   Table_B   B  ON   A . Key   =   B . Key  WHERE   A . Key   IS   NULL   OR   B . Key   IS   NULL",
            "title": "OUTER Excluding JOIN"
        },
        {
            "location": "/Introduction to SQL JOINs/#examples",
            "text": "Suppose we have two tables,  TABLE_A  and  TABLE_B . The data in these tables are shown below:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 TABLE_A \n   PK   Value  ---- ---------- \n    1   FOX \n    2   COP \n    3   TAXI \n    6   WASHINGTON \n    7   DELL \n    5   ARIZONA \n    4   LINCOLN \n   10   LUCENT  TABLE_B \n   PK   Value  ---- ---------- \n    1   TROT \n    2   CAR \n    3   CAB \n    6   MONUMENT \n    7   PC \n    8   MICROSOFT \n    9   APPLE \n   11   SCOTCH    The results of the seven joins are shown below:  INNER JOIN  1\n2\n3\n4\n5\n6 --INNER JOIN  SELECT   A . PK   AS   A_PK ,   A . Value   AS   A_Value , \n        B . Value   AS   B_Value ,   B . PK   AS   B_PK  FROM   Table_A   A  INNER   JOIN   Table_B   B  ON   A . PK   =   B . PK    1\n2\n3\n4\n5\n6\n7\n8\n9 A_PK   A_Value      B_Value      B_PK  ---- ---------- ---------- ---- \n    1   FOX          TROT            1 \n    2   COP          CAR             2 \n    3   TAXI         CAB             3 \n    6   WASHINGTON   MONUMENT        6 \n    7   DELL         PC              7  ( 5   row ( s )   affected )    LEFT JOIN  1\n2\n3\n4\n5\n6 --LEFT JOIN  SELECT   A . PK   AS   A_PK ,   A . Value   AS   A_Value ,  B . Value   AS   B_Value ,   B . PK   AS   B_PK  FROM   Table_A   A  LEFT   JOIN   Table_B   B  ON   A . PK   =   B . PK     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 A_PK   A_Value      B_Value      B_PK  ---- ---------- ---------- ---- \n    1   FOX          TROT            1 \n    2   COP          CAR             2 \n    3   TAXI         CAB             3 \n    4   LINCOLN      NULL         NULL \n    5   ARIZONA      NULL         NULL \n    6   WASHINGTON   MONUMENT        6 \n    7   DELL         PC              7 \n   10   LUCENT       NULL         NULL  ( 8   row ( s )   affected )    RIGHT JOIN  1\n2\n3\n4\n5\n6 --RIGHT JOIN  SELECT   A . PK   AS   A_PK ,   A . Value   AS   A_Value ,  B . Value   AS   B_Value ,   B . PK   AS   B_PK  FROM   Table_A   A  RIGHT   JOIN   Table_B   B  ON   A . PK   =   B . PK     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 A_PK   A_Value      B_Value      B_PK  ---- ---------- ---------- ---- \n    1   FOX          TROT            1 \n    2   COP          CAR             2 \n    3   TAXI         CAB             3 \n    6   WASHINGTON   MONUMENT        6 \n    7   DELL         PC              7  NULL   NULL         MICROSOFT       8  NULL   NULL         APPLE           9  NULL   NULL         SCOTCH         11  ( 8   row ( s )   affected )    OUTER JOIN  1\n2\n3\n4\n5\n6 --OUTER JOIN  SELECT   A . PK   AS   A_PK ,   A . Value   AS   A_Value ,  B . Value   AS   B_Value ,   B . PK   AS   B_PK  FROM   Table_A   A  FULL   OUTER   JOIN   Table_B   B  ON   A . PK   =   B . PK     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 A_PK   A_Value      B_Value      B_PK  ---- ---------- ---------- ---- \n    1   FOX          TROT            1 \n    2   COP          CAR             2 \n    3   TAXI         CAB             3 \n    6   WASHINGTON   MONUMENT        6 \n    7   DELL         PC              7  NULL   NULL         MICROSOFT       8  NULL   NULL         APPLE           9  NULL   NULL         SCOTCH         11 \n    5   ARIZONA      NULL         NULL \n    4   LINCOLN      NULL         NULL \n   10   LUCENT       NULL         NULL  ( 11   row ( s )   affected )    LEFT  Excluding  JOIN  1\n2\n3\n4\n5\n6\n7 --LEFT EXCLUDING JOIN  SELECT   A . PK   AS   A_PK ,   A . Value   AS   A_Value ,  B . Value   AS   B_Value ,   B . PK   AS   B_PK  FROM   Table_A   A  LEFT   JOIN   Table_B   B  ON   A . PK   =   B . PK  WHERE   B . PK   IS   NULL    1\n2\n3\n4\n5\n6 A_PK   A_Value      B_Value      B_PK  ---- ---------- ---------- ---- \n    4   LINCOLN      NULL         NULL \n    5   ARIZONA      NULL         NULL \n   10   LUCENT       NULL         NULL  ( 3   row ( s )   affected )    RIGHT  Excluding  JOIN  1\n2\n3\n4\n5\n6\n7 - RIGHT   EXCLUDING   JOIN  SELECT   A . PK   AS   A_PK ,   A . Value   AS   A_Value ,  B . Value   AS   B_Value ,   B . PK   AS   B_PK  FROM   Table_A   A  RIGHT   JOIN   Table_B   B  ON   A . PK   =   B . PK  WHERE   A . PK   IS   NULL    1\n2\n3\n4\n5\n6\n7 A_PK   A_Value      B_Value      B_PK  ---- ---------- ---------- ----  NULL   NULL         MICROSOFT       8  NULL   NULL         APPLE           9  NULL   NULL         SCOTCH         11  ( 3   row ( s )   affected )    OUTER  Excluding  JOIN  1\n2\n3\n4\n5\n6\n7\n8 --OUTER EXCLUDING JOIN  SELECT   A . PK   AS   A_PK ,   A . Value   AS   A_Value ,  B . Value   AS   B_Value ,   B . PK   AS   B_PK  FROM   Table_A   A  FULL   OUTER   JOIN   Table_B   B  ON   A . PK   =   B . PK  WHERE   A . PK   IS   NULL  OR   B . PK   IS   NULL     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 A_PK   A_Value      B_Value      B_PK  ---- ---------- ---------- ----  NULL   NULL         MICROSOFT       8  NULL   NULL         APPLE           9  NULL   NULL         SCOTCH         11 \n    5   ARIZONA      NULL         NULL \n    4   LINCOLN      NULL         NULL \n   10   LUCENT       NULL         NULL  ( 6   row ( s )   affected )",
            "title": "Examples"
        },
        {
            "location": "/Introduction to SQL JOINs/#conclusion",
            "text": "Note on the  OUTER JOIN  that the inner joined records are returned first, followed by the right joined records, and then finally the left joined records (at least, that\u2019s how my Microsoft SQL Server did it; this, of course, is without using any  ORDER BY  statement).",
            "title": "Conclusion"
        },
        {
            "location": "/Web_CS/",
            "text": "Foreword\n\n\nCheat sheets.\n\n\n\n\nHTML\n\u00b6\n\n\n\n\nHTML Markups\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHTML Basics\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCore HTML\n. PDF only.\n\n\nHTML Character Entities 1\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHTML Character Entities 2\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHTML5 Quick Reference\n. PDF only.\n\n\n\n\nCSS\n\u00b6\n\n\n\n\nCSS Basics\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlueprint CSS\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Programming\n\u00b6\n\n\n\n\nWeb Programming\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHTTP Status Code 1\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHTTP Status Code 2\n. PDF.\n\n\n\n\n\n\n\n\n\n\nProject Management\n\u00b6\n\n\n\n\nScrum\n. PDF only.",
            "title": "Web Cheat Sheets"
        },
        {
            "location": "/Web_CS/#css",
            "text": "CSS Basics . PDF.         Blueprint CSS . PDF.",
            "title": "CSS"
        },
        {
            "location": "/Web_CS/#web-programming",
            "text": "Web Programming . PDF.         HTTP Status Code 1 . PDF.        HTTP Status Code 2 . PDF.",
            "title": "Web Programming"
        },
        {
            "location": "/Web_CS/#project-management",
            "text": "Scrum . PDF only.",
            "title": "Project Management"
        },
        {
            "location": "/HTML and CSS/",
            "text": "Foreword\n\n\nNotes.\n\n\n\n\nOption One: Convert\n\u00b6\n\n\n\n\nWrite a document in Word, Markdown and save it in HTML, including a style (no or minimal CSS).\n\n\nUse Pandoc to convert Word, Markdown, and other format to HTML.\n\n\n\n\nOption Two: Generate\n\u00b6\n\n\n\n\nLook for a HTML generator online.\n\n\nUse a PC generator: \nStaticGen\n\n\n\n\nOption Three: Learn and Code\n\u00b6\n\n\nCreate a webpage with notepad, notepad++ or gedit (among many text editors) using simple HTML and CSS.\n\n\n\n\nPDF: \nWeb Page Design\n.\n\n\nTutorial: \nBuilding your First Web Page\n.\n\n\nComprehensive Tutorial: \nHTML 4.01 Specification\n.\n\n\n\n\nAdd CSS\n\u00b6\n\n\n\n\nAdding CSS\n.\n\n\n\n\nCSS Generators\n\u00b6\n\n\n\n\nCSS Creator\n.\n\n\nCSSlayoutgenerator\n.\n\n\nUncle Jim\u2019s Web Design\n.\n\n\nCSS 3.0 Maker\n.\n\n\nGenerateCSS\n.\n\n\nHTMLBasix\n\n\nMadSubmitter\n\n\n\n\nWeb Generator Lists\n\u00b6\n\n\n\n\nHTML\n.\n\n\nWebdesignerDepot\n.",
            "title": "HTML and CSS"
        },
        {
            "location": "/HTML and CSS/#option-two-generate",
            "text": "Look for a HTML generator online.  Use a PC generator:  StaticGen",
            "title": "Option Two: Generate"
        },
        {
            "location": "/HTML and CSS/#option-three-learn-and-code",
            "text": "Create a webpage with notepad, notepad++ or gedit (among many text editors) using simple HTML and CSS.   PDF:  Web Page Design .  Tutorial:  Building your First Web Page .  Comprehensive Tutorial:  HTML 4.01 Specification .",
            "title": "Option Three: Learn and Code"
        },
        {
            "location": "/HTML and CSS/#add-css",
            "text": "Adding CSS .",
            "title": "Add CSS"
        },
        {
            "location": "/HTML and CSS/#css-generators",
            "text": "CSS Creator .  CSSlayoutgenerator .  Uncle Jim\u2019s Web Design .  CSS 3.0 Maker .  GenerateCSS .  HTMLBasix  MadSubmitter",
            "title": "CSS Generators"
        },
        {
            "location": "/HTML and CSS/#web-generator-lists",
            "text": "HTML .  WebdesignerDepot .",
            "title": "Web Generator Lists"
        },
        {
            "location": "/embedding/",
            "text": "Foreword\n\n\nNotes, snippets, and results.\n\n\n\n\nEmbedding outputs\n\u00b6\n\n\n\n\nEmbedding static visualizations from packages can be done with image outputs (.png, .pdf) or HTML outputs.\n\n\nWe can embed just about any HTML snippet with \n<iframe>\n or \n<embed>\n. \n\n\n\n\n1\n<iframe\n \nseamless\n \nsrc=\n\"../leaflet_frag.html\"\n \nwidth=\n520px\n \nheight=\n520px\n \n></iframe>\n\n\n\n\n\n\n\n\n\n\n1\n<embed\n \nseamless\n \nsrc=\n\"../leaflet_frag.html\"\n \nwidth=\n520px\n \nheight=\n520px\n \n></embed>",
            "title": "Embedding HTML into HTML"
        },
        {
            "location": "/Responsive_HTML/",
            "text": "Foreword\n\n\nNotes.\n\n\n\n\nInitial grid looks\n\u00b6\n\n\n\n\n\n\n\n\nHere is the HTML:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n<\ndiv\n \nclass\n=\n\"container\"\n>\n\n  \n<\ndiv\n>\n1\n</\ndiv\n>\n\n  \n<\ndiv\n>\n2\n</\ndiv\n>\n\n  \n<\ndiv\n>\n3\n</\ndiv\n>\n\n  \n<\ndiv\n>\n4\n</\ndiv\n>\n\n  \n<\ndiv\n>\n5\n</\ndiv\n>\n\n  \n<\ndiv\n>\n6\n</\ndiv\n>\n\n\n</\ndiv\n>\n\n\n\n\n\n\n\nAnd the CSS:\n\n\n1\n2\n3\n4\n5\n.\ncontainer\n \n{\n\n    \ndisplay\n:\n \ngrid\n;\n\n    \ngrid-template-columns\n:\n \n100\npx\n \n100\npx\n \n100\npx\n;\n\n    \ngrid-template-rows\n:\n \n50\npx\n \n50\npx\n;\n\n\n}\n\n\n\n\n\n\n\nBasic responsiveness with the fraction unit\n\u00b6\n\n\nCSS Grid brings with it a whole new value called a fraction unit: written like \nfr\n. It splits the container into as many fractions.\n\n\n1\n2\n3\n4\n5\n.\ncontainer\n \n{\n\n    \ndisplay\n:\n \ngrid\n;\n\n    \ngrid-template-columns\n:\n \n1\nfr\n \n1\nfr\n \n1\nfr\n;\n\n    \ngrid-template-rows\n:\n \n50\npx\n \n50\npx\n;\n\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we change the grid-template-columns value to \n1fr 2fr 1fr\n, the second column will now be twice as wide as the two other columns:\n\n\n\n\n\n\n\n\nAdvanced responsiveness\n\u00b6\n\n\nrepeat()\n\u00b6\n\n\nhighlight line 3\n\n\n1\n2\n3\n4\n5\n.\ncontainer\n \n{\n\n    \ndisplay\n:\n \ngrid\n;\n\n\n    \ngrid-template-columns\n:\n \nrepeat\n(\n3\n,\n \n100\npx\n);\n\n\n    \ngrid-template-rows\n:\n \nrepeat\n(\n2\n,\n \n50\npx\n);\n\n\n}\n\n\n\n\n\n\n\nIn other words, \nrepeat(3, 100px)\n is identical to \n100px 100px 100px\n:\n\n\n\n\n\n\n\n\nauto-fit\n\u00b6\n\n\nhighlight line 4\n\n\n1\n2\n3\n4\n5\n6\n.\ncontainer\n \n{\n\n    \ndisplay\n:\n \ngrid\n;\n\n    \ngrid-gap\n:\n \n5\npx\n;\n\n\n    \ngrid-template-columns\n:\n \nrepeat\n(\nauto\n-\nfit\n,\n \n100\npx\n);\n\n\n    \ngrid-template-rows\n:\n \nrepeat\n(\n2\n,\n \n100\npx\n);\n\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe grid now varies the amount of columns with the width of the container.\n\n\nminmax()\n\u00b6\n\n\nhighlight line 4\n\n\n1\n2\n3\n4\n5\n6\n.\ncontainer\n \n{\n\n    \ndisplay\n:\n \ngrid\n;\n\n    \ngrid-gap\n:\n \n5\npx\n;\n\n\n    \ngrid-template-columns\n:\n \nrepeat\n(\nauto\n-\nfit\n,\n \nminmax\n(\n100\npx\n,\n \n1\nfr\n));\n\n\n    \ngrid-template-rows\n:\n \nrepeat\n(\n2\n,\n \n100\npx\n);\n\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe \nminmax()\n function defines a size range greater than or equal to min and less than or equal to max.\n\n\nSo the columns will now always be at least 100px. However if there are more available space, the grid will simply distribute this equally to each of the columns, as the columns turn into a fraction unit instead of 100 px.\n\n\nAdding the images\n\u00b6\n\n\nNow the final step is to add the images.\n\n\n1\n<\ndiv\n><\nimg\n \nsrc\n=\n\"img/forest.jpg\"\n/></\ndiv\n>\n\n\n\n\n\n\n\nTo make the image fit into the item, we\u2019ll set the it to be as wide and tall as the item itself, and then use \nobject-fit\n:\n \ncover\n;\n. This will make the image cover its entire container, and the browser will crop it if it\u2019s needed.\n\n\n1\n2\n3\n4\n5\n.\ncontainer\n \n>\n \ndiv\n \n>\n \nimg\n \n{\n\n    \nwidth\n:\n \n100\n%\n;\n\n    \nheight\n:\n \n100\n%\n;\n\n    \nobject-fit\n:\n \ncover\n;\n\n\n}",
            "title": "Responsive HTML"
        },
        {
            "location": "/Responsive_HTML/#basic-responsiveness-with-the-fraction-unit",
            "text": "CSS Grid brings with it a whole new value called a fraction unit: written like  fr . It splits the container into as many fractions.  1\n2\n3\n4\n5 . container   { \n     display :   grid ; \n     grid-template-columns :   1 fr   1 fr   1 fr ; \n     grid-template-rows :   50 px   50 px ;  }       If we change the grid-template-columns value to  1fr 2fr 1fr , the second column will now be twice as wide as the two other columns:",
            "title": "Basic responsiveness with the fraction unit"
        },
        {
            "location": "/Responsive_HTML/#advanced-responsiveness",
            "text": "",
            "title": "Advanced responsiveness"
        },
        {
            "location": "/Responsive_HTML/#repeat",
            "text": "highlight line 3  1\n2\n3\n4\n5 . container   { \n     display :   grid ;       grid-template-columns :   repeat ( 3 ,   100 px );       grid-template-rows :   repeat ( 2 ,   50 px );  }    In other words,  repeat(3, 100px)  is identical to  100px 100px 100px :",
            "title": "repeat()"
        },
        {
            "location": "/Responsive_HTML/#auto-fit",
            "text": "highlight line 4  1\n2\n3\n4\n5\n6 . container   { \n     display :   grid ; \n     grid-gap :   5 px ;       grid-template-columns :   repeat ( auto - fit ,   100 px );       grid-template-rows :   repeat ( 2 ,   100 px );  }       The grid now varies the amount of columns with the width of the container.",
            "title": "auto-fit"
        },
        {
            "location": "/Responsive_HTML/#minmax",
            "text": "highlight line 4  1\n2\n3\n4\n5\n6 . container   { \n     display :   grid ; \n     grid-gap :   5 px ;       grid-template-columns :   repeat ( auto - fit ,   minmax ( 100 px ,   1 fr ));       grid-template-rows :   repeat ( 2 ,   100 px );  }       The  minmax()  function defines a size range greater than or equal to min and less than or equal to max.  So the columns will now always be at least 100px. However if there are more available space, the grid will simply distribute this equally to each of the columns, as the columns turn into a fraction unit instead of 100 px.",
            "title": "minmax()"
        },
        {
            "location": "/Responsive_HTML/#adding-the-images",
            "text": "Now the final step is to add the images.  1 < div >< img   src = \"img/forest.jpg\" /></ div >    To make the image fit into the item, we\u2019ll set the it to be as wide and tall as the item itself, and then use  object-fit :   cover ; . This will make the image cover its entire container, and the browser will crop it if it\u2019s needed.  1\n2\n3\n4\n5 . container   >   div   >   img   { \n     width :   100 % ; \n     height :   100 % ; \n     object-fit :   cover ;  }",
            "title": "Adding the images"
        },
        {
            "location": "/Markdown & Pandoc/",
            "text": "Foreword\n\n\nNotes.\n\n\n\n\nRMarkdown Documentation\n\u00b6\n\n\n\n\nRMarkdown\n.\n\n\nQuicktour\n.\n\n\n\n\nPandoc, Document Converter\n\u00b6\n\n\nInstall\n\u00b6\n\n\n\n\nPandoc\n\n\nFor Windows and Linux, download the \npackages\n.\n\n\nInstall Pandoc on Ubuntu with \nhaskell\n\n\n\n\nUse in a terminal\n\u00b6\n\n\n\n\npandoc --version\n.\n\n\npandoc --help\n, \nman pandoc\n on Linux.\n\n\nman pandoc_markdown\n.\n\n\nPandoc is located in a directory. It is where conversions are done. For example, on Windows: \ncd D:\\User\\Documents\\Pandoc\n\n\nSnippets",
            "title": "Markdown & Pandoc"
        },
        {
            "location": "/Markdown & Pandoc/#pandoc-document-converter",
            "text": "",
            "title": "Pandoc, Document Converter"
        },
        {
            "location": "/Markdown & Pandoc/#install",
            "text": "Pandoc  For Windows and Linux, download the  packages .  Install Pandoc on Ubuntu with  haskell",
            "title": "Install"
        },
        {
            "location": "/Markdown & Pandoc/#use-in-a-terminal",
            "text": "pandoc --version .  pandoc --help ,  man pandoc  on Linux.  man pandoc_markdown .  Pandoc is located in a directory. It is where conversions are done. For example, on Windows:  cd D:\\User\\Documents\\Pandoc  Snippets",
            "title": "Use in a terminal"
        },
        {
            "location": "/tests/",
            "text": "Foreword\n\n\nThis page tests possibilities.\n\n\n\n\nSane list\n\u00b6\n\n\n\n\n\u2018Apprenez \u00e0 programmer en Python\u2019\n\n\n\u201cAutomate the Boring Stuff with Python\u201d\n\n\n\n\n\n\n\u2018Apprenez \u00e0 programmer en Python\u2019\n\n\n\u201cAutomate the Boring Stuff with Python\u201d\n\n\n\n\n\n\n\u2018Apprenez \u00e0 programmer en Python\u2019\n\n\n\u201cAutomate the Boring Stuff with Python\u201d\n\n\n\n\nSmart symbols\n\u00b6\n\n\n\u00b1  \u2192  \u2190  \u2194  \u2260  \u00bc  1\nst\n 2\nnd\n 3\nrd\n 4\nth\n \u00a9 \u2122\n\n\u2018a\u2019 \u201cb\u201d  \u00abc\u00bb  \u2026 - \u2013 \u2014\n\n\nSub/Superscripts\n\u00b6\n\n\nEnter superscript: 2\n10\n is 1024.\n\n\nsuperscript\n\n\npip install MarkdownSuperscript\n\n\nEnter subscript: The molecular composition of water is H\n2\nO.\n\n\nsubscript\n\n\npip install MarkdownSubscript\n\n\nEmbed videos\n\u00b6\n\n\n\n\n\npip install pyembed-markdown\n.\n\nAdd \npyembed.markdown\n.\n\n\nBuild tables\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na\n\n\nb\n\n\n\n\n\n\nc\n\n\nd\n\n\n\n\n\n\n\n\nWithout outside borders:\n\n\n\n\n\n\n\n\nFirst Header\n\n\nSecond Header\n\n\nThird Header\n\n\n\n\n\n\n\n\n\n\nContent Cell\n\n\nContent Cell\n\n\nContent Cell\n\n\n\n\n\n\nContent Cell\n\n\nContent Cell\n\n\nContent Cell\n\n\n\n\n\n\n\n\nWith outside borders:\n\n\n\n\n\n\n\n\nFirst Header\n\n\nSecond Header\n\n\nThird Header\n\n\n\n\n\n\n\n\n\n\nContent Cell\n\n\nContent Cell\n\n\nContent Cell\n\n\n\n\n\n\nContent Cell\n\n\nContent Cell\n\n\nContent Cell\n\n\n\n\n\n\n\n\nAligned:\n\n\n\n\n\n\n\n\nFirst Header\n\n\nSecond Header\n\n\nThird Header\n\n\n\n\n\n\n\n\n\n\nLeft\n\n\nCenter\n\n\nRight\n\n\n\n\n\n\nLeft\n\n\nCenter\n\n\nRight\n\n\n\n\n\n\n\n\nArithmatex\n\u00b6\n\n\nInline: \n\\frac{1}{n}\n\\frac{1}{n}\n and\u2026\n\n\nBlock:\n\n\n\n\n\\sqrt{9}\n\n\n\\sqrt{9}\n\n\n\n\nFootnotes\n\u00b6\n\n\nLorem ipsum\n1\n\n\nLorem ipsum Lorem ipsum\n2\n\n\nAdmonition\n\u00b6\n\n\n\n\nTip\n\n\nLorem ipsum\n\n\n\n\nCode block\n\u00b6\n\n\n1\n2\n3\na\n=\n1\n\n\nb\n=\n1\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\nCode hilite\n\u00b6\n\n\n1\n2\n3\na\n=\n1\n\n\nb\n=\n1\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\nInline code hilite\n\u00b6\n\n\na\n=\n1\n\n\nBold italic\n\u00b6\n\n\nLorem ipsum\n\n\nUnderline\n\u00b6\n\n\nLorem ipsum\n\n\nHighlight\n\u00b6\n\n\nLorem ipsum\n\n\nEmoji\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\n\nLorem ipsum\u00a0\n\u21a9\n\n\n\n\n\n\nLorem ipsum 2.1\n\nLorem ipsum 2.2\u00a0\n\u21a9",
            "title": "tests"
        },
        {
            "location": "/tests/#smart-symbols",
            "text": "\u00b1  \u2192  \u2190  \u2194  \u2260  \u00bc  1 st  2 nd  3 rd  4 th  \u00a9 \u2122 \n\u2018a\u2019 \u201cb\u201d  \u00abc\u00bb  \u2026 - \u2013 \u2014",
            "title": "Smart symbols"
        },
        {
            "location": "/tests/#subsuperscripts",
            "text": "Enter superscript: 2 10  is 1024.  superscript  pip install MarkdownSuperscript  Enter subscript: The molecular composition of water is H 2 O.  subscript  pip install MarkdownSubscript",
            "title": "Sub/Superscripts"
        },
        {
            "location": "/tests/#embed-videos",
            "text": "pip install pyembed-markdown . \nAdd  pyembed.markdown .",
            "title": "Embed videos"
        },
        {
            "location": "/tests/#build-tables",
            "text": "a  b    c  d     Without outside borders:     First Header  Second Header  Third Header      Content Cell  Content Cell  Content Cell    Content Cell  Content Cell  Content Cell     With outside borders:     First Header  Second Header  Third Header      Content Cell  Content Cell  Content Cell    Content Cell  Content Cell  Content Cell     Aligned:     First Header  Second Header  Third Header      Left  Center  Right    Left  Center  Right",
            "title": "Build tables"
        },
        {
            "location": "/tests/#arithmatex",
            "text": "Inline:  \\frac{1}{n} \\frac{1}{n}  and\u2026  Block:   \\sqrt{9}  \\sqrt{9}",
            "title": "Arithmatex"
        },
        {
            "location": "/tests/#footnotes",
            "text": "Lorem ipsum 1  Lorem ipsum Lorem ipsum 2",
            "title": "Footnotes"
        },
        {
            "location": "/tests/#admonition",
            "text": "Tip  Lorem ipsum",
            "title": "Admonition"
        },
        {
            "location": "/tests/#code-block",
            "text": "1\n2\n3 a = 1  b = 1  print ( a )",
            "title": "Code block"
        },
        {
            "location": "/tests/#code-hilite",
            "text": "1\n2\n3 a = 1  b = 1  print ( a )",
            "title": "Code hilite"
        },
        {
            "location": "/tests/#inline-code-hilite",
            "text": "a = 1",
            "title": "Inline code hilite"
        },
        {
            "location": "/tests/#bold-italic",
            "text": "Lorem ipsum",
            "title": "Bold italic"
        },
        {
            "location": "/tests/#underline",
            "text": "Lorem ipsum",
            "title": "Underline"
        },
        {
            "location": "/tests/#highlight",
            "text": "Lorem ipsum",
            "title": "Highlight"
        },
        {
            "location": "/tests/#emoji",
            "text": "Lorem ipsum\u00a0 \u21a9    Lorem ipsum 2.1 \nLorem ipsum 2.2\u00a0 \u21a9",
            "title": "Emoji"
        }
    ]
}