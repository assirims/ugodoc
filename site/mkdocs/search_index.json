{
    "docs": [
        {
            "location": "/", 
            "text": "Let there be light!\n\n\nA Data Science documentation website.\n\n\nContent\n\n\n\n\nCollection (corpus of document titles)\n\n\nCLI\n\n\nDocuments about Linux bash, Windows shell\n\n\n\n\n\n\nResources for Data Science\n\n\nDocuments about digital humanities, Git, storytelling\n\n\n\n\n\n\nSQL\n\n\nDocuments about SQL\n\n\n\n\n\n\nWeb\n\n\nDocument about HTML, CSS, JS, building, deploying\n\n\n\n\n\n\n\n\nLayout\n\n\n\n\nugodoc\n is a corpus: a catalog of books, manuals, articles, presentations, videos, podcasts, summaries, notes, code snippets, excerpts, websites, etc.\n\n\nThe \nCollection\n page is a high-level catalogue. The rest of the webpages are more comprehensive.\n\n\nA Mkdocs site is automatically indexed. The \ndocs\n is a searchable knowledge-based system. \n\n\nYou type a keyword, it leads to several sources, you identify the document the source belongs to, and you go retrieve the document; whether it is a digital or a material document. Fast and easy!\n\n\nFor that matter, \ngenerously\n adding keywords to the \ndocs\n is crucial (adding them in subscript makes them stand apart).\n\n\nCitations, keywords, links, etc.; they all provide leads.\n\n\nThe corpus is unstructured. There is no unique chapter dedicated to one topic.\n\n\nKnowledge is rather cumulated, layer after layer, resulting in a hotchpotch of information. \n\n\nInformation may be repeted among many documents, with different explanations, or some more comprehensive.\n\n\nNewer entries might also supplement or contradict older entries.", 
            "title": "Home"
        }, 
        {
            "location": "/#layout", 
            "text": "ugodoc  is a corpus: a catalog of books, manuals, articles, presentations, videos, podcasts, summaries, notes, code snippets, excerpts, websites, etc.  The  Collection  page is a high-level catalogue. The rest of the webpages are more comprehensive.  A Mkdocs site is automatically indexed. The  docs  is a searchable knowledge-based system.   You type a keyword, it leads to several sources, you identify the document the source belongs to, and you go retrieve the document; whether it is a digital or a material document. Fast and easy!  For that matter,  generously  adding keywords to the  docs  is crucial (adding them in subscript makes them stand apart).  Citations, keywords, links, etc.; they all provide leads.  The corpus is unstructured. There is no unique chapter dedicated to one topic.  Knowledge is rather cumulated, layer after layer, resulting in a hotchpotch of information.   Information may be repeted among many documents, with different explanations, or some more comprehensive.  Newer entries might also supplement or contradict older entries.", 
            "title": "Layout"
        }, 
        {
            "location": "/collection/", 
            "text": "Collection\n\n\nForeword\n\n\nCorpus of documents: titles and keywords.\n\n\n\n\nCONTENT\n\n\nCollection\n\n\nEmployment, Education\n\n\nIndustry\n\n\nBash, Powershell\n\n\nCloud Computing\n\n\nData\n\n\nDataCamp\n\n\nEconometrics, Spatial, GIS\n\n\nGit\n\n\nHadoop\n\n\nHTML, CSS\n\n\nImport.io\n\n\nInternet of Things\n\n\nJavaScript\n\n\nLaTeX, Markdown\n\n\nLinux\n\n\nNLP, Digital Humanities\n\n\nNoSQL\n\n\nPredictive Modeling, Data Science, Data Mining, Marketing Analytics\n\n\nProgrammation for Kids\n\n\nProject Management\n\n\nPython\n\n\nAdvanced, Specialized\n\n\nEntry-level\n\n\n\n\n\n\nR, Statistics\n\n\nAdvanced, Specialized\n\n\nEntry-level\n\n\nR Compilation\n\n\n\n\n\n\nRegex\n\n\nSQL\n\n\nStorytelling, Visualization\n\n\nTableau\n\n\nTeamTreeHouse\n\n\nUdemy\n\n\nvim\n\n\nMiscellanous\n\n\n\n\n\n\n\n\n\n\n\n\nEmployment, Education\n\n\nArticles (notes)\n\n\nCartes de visite (notes, examples)\n\n\nInfographies (notes)\n\n\nResume templates (notes)\n\n\nThe Infographic Resume, How to Create a Visual Portfolio that Showcases Your Skills and Lands the Job (+ notes), McGraw-Hill Education, 2014, \nworkforce, technology, paper, morphing, job boards, databases, referrals, mismatch, gig, masses, linkedIn, power to the people, online portfolio, assets, explaining, pitching, social medias, work, visual flair, graphic designer, data visualization, shareability, success story, design, profile, checklist, interactive, slideshows, talent, checklists\n\n\nA Game Plan for Success in Data Analytics (pdf, epub), Fru Nde, \nhiring, demand, jobs, postings, industry, companies, roles, specialization, salaries, growth, data analysis, responsibilities, skill sets, tools, training, job description, excel, pivot tables, presentations, business intelligence, sql, warehouse, multidimensional, cube, programming,  programs, lifecycle, predictive analytics, changes\n\n\nCracking the Coding Interview, 150 Programming Interview Questions and Solutions, 4th Edition, CareerCup, 2010, \ninterview, behind the scenes, while stories, before, resume, behavioural, preparation, questions, mistakes, frequently asked questions, data structure, algorithms, programming languages, problem solving, knowledge\n\n\nGetting a Big Data Job For Dummies, 1st Edition, 2015, \ncareer path, public sector, academia, commercial organizations, corporate information technology, marketing departments, business units, big data firms, consulting companies, Groves, predictions, skills, attitudes, self-assessment, gaps, education, experience, milestones, timeline, risk, compliance, financial services, healthcare, government, retail, business analyst, data scientist, software developer, foundation, books, online tutorials, on-the-job training, goals, inventory, knowledge, tools, sql, nosql, hadoop, pig, hive, spark, business analytics, business intelligence, visualization, sentiment analysis, machine learning, consultant, specialist, startup, job landing, resume, interview, story, behaviour, motivation\n\n\nPeter Days World Of Business, In Business Recruiting By Algorithm (podcast), 2016, \nalgorithms, online assessment, recruiting, business, interview, computer, filter, ranking, tracking, bias, workforce, blind recruiting, pool, diversity, the voice, blind audition, personality, questionnaires, psychology, sift, sieve, filter, keywords, saber software, dating websites, successful relationship, assessing, interview, expand, candidates, recruitment, robot, dating, core behaviours, templates, looking, similar, traits, compare, drivers, in modeling applicants, successful employees, training, models\n\n\nPoste de Data Scientist (article), \nresume\n\n\nTechnical Recruiter Cheat Sheet\n\n\nUdacity, Ultimate Skill Checklist For Your First Data Analyst Job, Udacity, 2015, \nskills, competencies, talent, data analyst, programming, statistics, experimental design, mathematics, machine learning, data wrangling, communication, visualization, intuition, learning\n\n\nData Analyst Nanodegree, Udacity, \nstatistics, databases, data wrangling, exploration, identification, fraud, machine learning, visualization, web, a/b testing\n\n\nMachine Learning Engineer Nanodegree, Udacity, \nexploration, prediction, identification, segmentation, training, learning, artificial intelligence, machine learning\n\n\nTech Entrepreneur Nanodegree, Udacity, \nProduct design, monetization, interactive prototype, marketing, distribution, plan, storytelling, pitch, submission\n\n\nUniversity, Microprograms (notes) \n\n\nIndustry\n\n\nApplied Business Analytics: Integrating Business Process, Big Data, and Advanced Analytics, Pearson, 2015, \nbusiness, analytics, financial management, customer management, hr management, internal operations, value, governance, quality, methodology, process, forecast, sales force, decision, feedback, identify opportunities, patterns, causality, events, workflows, customers, crm, competencies, ecosystem, exploratory, data preparation, cube\n\n\nBig Data, Tirer parti des donn\u00e9es massives pour d\u00e9velopper l\nentreprise, Wiley, 2013, \nwalmart, transformation, perspectives, activity optimisation, data monetization, metrics, medias, target users, life cycle analysis, business intelligence, not why but what, decision-making, client management, value, visualization, satisfaction, segmentation, merchandising, user experience, ux, profitability, priority, client behaviour, efficiency, fraud, network, hadoop, mapreduce, hive, hbase, pig, etl\n\n\nEnjeux et usages du big data : Technologies, m\u00e9thodes et mise en oeuvre, Hermes, 2013, \ndeluge, it systems, data volume, open data, velocity, variety, finance, banks, telecoms, utilities, business, public services, health, transports, distribution, sales, wholesales, medical, education, building, leisure, value, gains, data mining, methodology, machine learning, automated learning, statistical learning, social networks, a/b testing, crownsourcing, geomarketing, time series, visualization, competencies, hr, data scientists, qualities, it, profile, resume, manage a project, methodology\n\n\nInfographies (notes)\n\n\nInternet, ce qui nous \u00e9chappe, Yves Michel, 2015, \ninternet, how it works, changes, changes, black oil, moore\ns law, connected objects, disruptive innovation, silicon valley, platforms, economic war, bytes, society, behaviour, backlash, repositories, data centers\n\n\nLa R\u00e9volution Big Data, Dunod, 2015, \nsocial networks, mobile, cloud, social business, volume, velocity, variety, repositories, hadoop, real time visualization, privacy, personal data, governance, business models, changes, marketing, sales, production, logistics, risks, frauds, hr, telecoms, energy, utilities, banks, public sector, health, open data, cognitive computing\n\n\nLe Big Data, Que sais-je, 2015, \nvolume, sequencers, and, genetics, internet of things, connected objects, google flu, prediction, computer structure, architecture, software, inferential, decision, correlation, reason, causality, scientific research, users, crowdsourcing, business models, competencies\n\n\nTraque Interdite (\u00e9mission web), R-C, 2015, \nweb personalization, categorization, five personality traits, algorithms, surveillance, massive, target users, demography, password, hacking, piracy, follow, mobile phone, protect, manipulation, bugs, facebook, tracking, data policy, cookies, chrome, firefox, twitter, publicity, instagram, personal data, bubble, anonymity, metadata, myths, video games, smart city, account, access, rights, linkedin, netflix, culture, personality, rick taking, wifi, friends, links, social networks, secure, apps, skynet, state, smartphone, revealing, online\n\n\nBig Data - A Business and Legal Guide, Wiley, 2014, \nexecutive, security, compliance, information, privacy, Big Data, laws, risk, license, antitrust, insurance, human resources, hr,  \n\n\nBig Data et machine learning, Manuel du data scientist, Dunod, 2015, \ndata explosion, economic cause, internet giants, web, price decrease, competitive advantage, demanding clients, value, eldorado, ecosystem, needs, cloud, value creation, resulting, velocity, variety, applications, competencies, hadoop, modeling, new tools, impacts, consequences, companies, clients, suppliers, legal, big brother, nosql, large-scale, repositories, data centers, documents, column-oriented, document-oriented, graph-oriented, future, algorithm, mapreduce, statistical analysis, joints, matrices, framework, data scientist, data collection, workflow, deluge, sources, formats, quality, exploratory, statistics, cleaning, enrich, transform, etl, programming, machine learning, understanding, predicting, models, overfitting, supervised, unsupervised, regression, classification, linear, nonlinear, parametric, nonparametric, incremental, geometric, probabilistic, linear regression, k-nn, knn, na\u00efve bayes, logistic regression, k-means, decision trees, random forest, support vector machines, examples, noise, visualization, graphs, complex, interactive, hadoop distributions, hadoop ecosystem\n\n\nBig Data For Dummies, 2013, \nAnd data management, waves, content management, structured data, unstructured data, distributed computing, parallel computing, redundant physical infrastructure, storage, servers, security, interfaces, databases, services, tools, warehouses, analytics, virtualization, network, processor, memory, storage, cloud, relational, nonrelational database, key-value, document, mongodb, couchdb, columnar, column-bases, hbase, graph, neo4j, special, postgis, opengio suite, polyglot, persistence, mapreduce, hadoop, distributed file system, hdfs, nodes, ecosystem, yarn, hbase, hive, pig, pig latin, sqoop, zookeeper, extraction,  extract, transformation, transform, loading, load, elt, operationalized, business intelligence, analytical, infrastructure, text, taxonomies, voice of the customer, social media, exploratory, codifying, integration, incorporation, traditional, validity, talent, return on investment, roi, roadmap, urgency, capacity, methodology, budget, skill sets, risk, governance, experiment, experimentation, streaming, improving business, needs, customer experience, best action, fraud, sources\n\n\nBig Data MBA Driving Business Strategies with Data Science, Wiley, 1st Edition, 2015, \nbusiness mandate, model maturity, strategy, prioritization, user experience, customer engagement, employees, channels, analyst, characteristics, approaches, models, business, exploratory, algorithms, models, date lake, learning, data science, data scientist, techniques, score, monetization, metamorphosis, creative thinking, creativity\n\n\nThe Patient Revolution, How Big Data and Analytics Are Transforming the Health Care Experience, Wiley, 2015, \nstartup, experience is a teacher, healthcare, banking, retail, patient empowerment, health cloud, analytics, trends, behavioural, personal health, population, consumer, choices, incentive, adoption, costs, outcomes, providers, diagnoses, machine learning, artificial intelligence, natural language processing, nlp, modernization, workflows,  experiences\n\n\nBig Data, A Revolution that will Transform who we Live, Work, and Think, 2013, \nnow, more, messy, correlation, date, value, implications, risks, control, next\n\n\nBlockchain Revolution: How the Technology Behind Bitcoin Is Changing Money, Business, and the World, Portfolio, 2016, \nprotocol, ledger, digital age, internet, prosperity, platform, design, principles, integrity, power, incentive, security, privacy, rights, inclusion, financial services, banks, banking, reputation, credit score, prediction, governance, accounting, business model, apps, agent, autonomous, open, networked, hacking, people, computing, internet of things, disruption, smart, smart phone, smart things, smart cities, prosperity, remittances, humanitarian aid, ownership, state, government, empowerment, democracy, voting, politics, justice, engaging citizens, crowd sourcing, streaming, connecting, education, energy, paradigm, NASCAR operation, job killer, big brother, criminals, ecosystem, cautionary tale, regulation, framework, agenda, trust\n\n\nLa R\u00e9volution Big data, Les donn\u00e9es au coeur de la transformation de l\nentreprise, Dunod, 2014\n\n\nWho\ns tracking you in public (article), \nfacial recognition, department stores, casinos, cruise ships, churches, crowd, facebook, face Prince, 3d modeling, surveillance, database, privacy, public\n\n\nBash, Powershell\n\n\ncli, command line, interface, directory, bash, shell, powershell, file, filename, terminal, folders, print, search, uppercase, lowercase, occurrence, text, file names, line, lines, sort, reverse, random, randomly, number, numbers, size, characters, programs, inputs, outputs, input, output, pause, time, touch, create, execute, modify, view, delete, run, process, ps, pstree, killall, graphic, load, list, top, bottom, end, kill, pid, sudo, superuser, do, group, user, permission, permissions, right, write, chain, pipe, archive, compress, decompress, zip, unzip, install, update, upgrade, configure, bc, arbitrary precision calculator language, format-control, date, time, copy, move, retrieve, find, locate, which, find within, grep, variable, conditional, if, else, loops, for, while, logs, import, csv, text, array, gui, email, error, reporting, html, database, mailbox, extract, formatting, jobs, debug, breakpoints\n\n\nCheat sheets\n\n\nLearn the Command Line (completed course, notes, snapshots, codes, manual, exercises), Codecademy, 2015\n\n\nCommand Line Crash Course (completed course, notes, snapshots, codes, manual, exercises), Learn Python the Hard Way, 2015\n\n\nThe Linux Command Line (completed course, notes, snapshots, codes, manual, exercises), No Starch Press, living book!\n\n\nLes commandes Fondamentales De Linux, CC, 2016\n\n\nLinux Shortcuts and Commands (notes), \n i/o, pipes, filter, permissions, job control, script, flow control, shortcuts, sanity, commands, operations, network, file compression, process control, administration commands, program installation, drive, partition, music-related, graphic-related, files, folders, directory\n\n\nThe Administrator Crash Course, Windows PowerShell v2, Realtime, 2010\n\n\nCloud Computing\n\n\nHow to Become A Data Scientist Using Azure Machine Learning (course, video)\n\n\nSpark Big Data Cluster Computing in Production, Wiley, 2016, \ncloud, text, sequence, avro, parquet, cluster, standalone, yarn, mesos, tuning, partitioning, shuffling, serialization, cache, memory, shared, locality, security, acl, network, encryption, fault, execution\n\n\nCloud computing, s\u00e9curit\u00e9, gouvernance du SI hybride et panorama du march\u00e9, Dunod, 2016\n\n\nOpen Source Cloud Computing for Complete Beginners, Building and Scaling High-Performance Web Servers on the Amazon Cloud (tutorial)\n\n\nData\n\n\nData Source Handbook, O\nReilly, 2011, \nopen data, public data, repositories, api, whois, bit.ly, compete, github, whitepages, facebook, twitter, boss, wikipedia, simplegeo, us census, crunchbase, zoominfo, maxming, books, films, music, products\n\n\nOpen Data, consommation, traitement, analyse et visualisation de la donn\u00e9e publique, eni, 2016, \nopen data, consumption, processing, analysis, visualization, public data\n\n\nDataCamp\n\n\nIntroduction to R (Beta) (completed course, slides), \nvariable, workspace, comments, scripts, data type, logical, numeric, character, double, complex, raw, coercion, coerce, as., create name, vectors, calculus, element, sum, subset, matrix, rbind, cbind, bind, rownames, colnames, colsums, rowsums, scalar, factor, categorical, order, levels, nominal, ordinal, order, list, extend, data frame, data.frame, sort, add column, row, sorting, graphics, plot numerical, plot categorical, hist, bins, barplot, boxplot, pairs, par, parameters, line type, lty, plot symbol, pch, multiple plots, mfrow, mfcol, grid, layout, stack\n\n\nIntermediate R (completed course, slides, snapshots), \nrelational operators, greater, lower, equal, logical operators, and , or, not, conditional, if, else, else if, loops, while, infinite, break, for, list, function, print, function documentation, na.rm, built-in functions, arguments, args, create functions, optional argument, return, packages, install, load, search, apply, lapply, sapply, vapply, identical, abs, absolute, round, sum, mean, sequence, seq, repeat, rep, sort, string, str, is., as., append, reverse, rev, regular expressions, grepl, grep, sub, gsub, subset, time, date, sys.date, sys.time, posixct, lubridate, zoo, xts, time series packages\n\n\nImporting Data into R (completed course, snapshots), \nflat files, excel files, spss files, sas files, stata files, postgresql database, mysql database, sqlite database, web, utils, read.table, header, separator, stringasfactors,  read.csv, comma, dot, , semi-colon, tab, delimited, states_eu.csv, readr, data.table, read_delim, col_names, col_types, skip, n_max, n_min, fread, readxl, excel_sheets, gdata, xlsx, xlconnect, loadworkbook, getsheets, readworksheet, createsheet, writeworksheet, saveworkbook, haven, foreign, read_sas, read_stata, read_dta,  as_factor, read_spss, read.dta, convert.factors, read.spss, rmysql, dbconnect, dblisttables, dbreadtable, dbgetquery, dbfetch, dbdisconnect, rpostgresql, web, path, download.file, url, authentication, dest_path, httr, api, json, twitter, jsonlite, array, nesting, tojson, prettify, minify\n\n\nCleaning Data in R (completed course, snapshots), \nexplore, structure, class, dim, names, str, glimpse, summary, visualize, head, tail, print, hist, plot, tidy, dirty, wide datasets, long datasets, tidyr, key, value, gather column, spread column, separate, unite, convert, conversion, as., lubridate, date, tmd, mdy, hms, ymd_hms, string manipulation, stringr, str_trim, trim, str_pad, str_detect, str_replace, replace, tolower, toupper, lowercase, uppercase, missing, na, not available, empty, dot, infinite value, inf, not a number, nan, outlier, errors, extreme, unexpected\n\n\nWriting Functions in R (completed course), \ndefine, return value, object, scope, scoping, lookup, data structures, atomic, list, missing very, not available, na, subset, less code, clearer, snippet, temporary, name, naming, argument, order, good function, functional programming, domain, variable, common code, functions as arguments, map, map_dbl, map_lgl, map_int, map_chr, failure, safely, possibly, quietly, map2, iterate, pmap, invoke_map, mapping, map2_dbl, map2_lgl, pmap_dbl, walk, pipe, pipeline, robust, errors, hidden argument, getting, setting options\n\n\nData Manipulation in R with dplyr (completed course, snapshots), \npipe, pipeline, tbl, select, filter, arrange, mutate, summarize, summarise, tidyr, tidy, starts_with, ends_with, contains, matches, num_range, one_of, is.na, !is.na, min, sum, sd, max, mean, var, length, median, iqr, first, last, nth, n, n_distinct, %\n%, magrittr, group_by, data frame, data table, database, tbl_dt, tbl\n\n\nData Analysis in R, the data.table Way (completed course, snapshots, cheat sheet), \ndata table, dt, select row, i, column, j, compute, by, group, subset, chain, chaining, :=, set, setnames, setcolorder, index, indexing, key, mult, nomatch, two-column key, join, setkey\n\n\nData Visualization in R with ggvis (completed course, snapshots), \npoints, bars, maps, histograms, scatter, density, data, coordinate system, mark, properties, html, javascript, js, THE%\n%, magrittr, pipe, pipeline, :=, sets, maps, =, stroke, width, opacity, dash, fill, lines, paths, ribbons, smooths, prediction, model_predictions, later, layer_smooths,  dplyr, layer_paths, layer_lines, group_by, mapping, chart, charting, interactivity, multilayered, axes, axis, legends\n\n\nData Visualization with ggplot2 (1) (completed course, snapshots, videos), \ngraphical data analysis, design, communication, exploratory, explanatory, anscombe plot, fit, fitting, grammar of graphics, layers, data, aesthetics, statistics, geometries, geom_, facets, coordinates, themes, base plot, points, tidy, color, size, shape, attribute, alpha, line type, label, shape, axes, positions, identity, dodge, stack, fill, jitter, jitterdodge, scale, scale functions, limit, breaks, expand, labs, abline, area, bar, bin2d, blank, boxplot, contour, crossbar, density, density2d, dotplot, errorbar, errorbar, freqpoly, hex, histogram, hline, line, linerange, map, path, pointrange, polygon, quantile, raster, rect, vline, ribbon, rug, segment, smooth, step, text, tile, violin, eas, geom, pch, crosshairs, remarks, time series, linetype, size, qplot\n\n\nData Visualization with ggplot2 (2) (completed course, snapshots, videos), \nstatistics, coordinates, facets, themes, best practices, function, geom, geom_, stat_, fill,  bin, histogram, bar, freqpoly, smooth, boxplot, bindot, bin2d, binhex, contour, quantile, sum, boxplot, dotplot, bin2d, hex, contour, quantile, count, summary, confidence interval, qq, quantile-quantile, coord_, coord_cartesian, scale_x_continuous, xlim, coord_cartesian, aspect ratio, facets, edward tufte, tidy, theme layer, element_, text, line, rectangle inheritance, blank, recycling, save theme, reuse theme, discrete x-axis, derivative theme, built-in theme templates, ggthemes, theme update, theme_set, pitfalls, dynamic plot, error bar, pointrange, pie charts, stacked bar chart, haircol, horizontal, heat maps\n\n\nReporting with R Markdown (completed course, snapshots, videos), \nreproducible, research, knitr, pandoc, shiny,  interactive, web-based, rstudio, html, css, pdf, word, beamer, slidy, ioslides, markdown::render, rmd, latex\n\n\nIntroduction to Machine Learning (completed course, snapshots, notes), \nregression, shopping basket analysis, recommendation systems, decision-making, classification, clustering, cluster, k-means, supervised, unsupervised, model performance, error, accuracy, competition time, interpretability, limits, confusion matrix, precision, recall, true, false, real mean squared error, cluster similarity, between cluster sum of squares, inter-cluster distance, dunn\ns index, training, train, testing, test, predictive power, split the dataset, cross validation, n-fold, number of validation, bias, variance, quadratic data, overfitting, underfitting, decision tree, numerical, categorical, classify, choose, split, sticking criteria, information gain, pruning, k-nearest neighbours, k-nn, distance, euclidean, scaling, dummy, roc curve, receiver operator characteristic curve, probability, simple regression, linear regression, r-squared, multiple linear regression, adjusted, predictors, assumptions, non-parametric, kernel regression, regression trees, generalized regression, within cluster sums of squares, between cluster sums of squares, k, scree plot, choosing, intercluster, hierarchical clustering, dendrogram, pros and cons\n\n\nIntro to Statistics with R (completed course, snapshots, videos):\n\n\n\n\nCourse One Introduction, \ntypes of variables, nominal, ordinal, interval, ratio, histogram, distribution, bimodal, skewed, skewness, uniform, platykurtic, leptokurtic, scales, z-score, percentile rank, measure of central tendency, average, median, mode, variability, variance, standard deviation\n\n\nCourse Two Student\ns T-test, \nz-test, t-test,  single sample, dependent, independent, cohen, cohen\ns d, confidence interval, t-value, upper bound, lower bound\n\n\nCourse Three Analysis of Variance (ANOVA), \nanova, analysis of variance, continuous, independent t-test, dependent t-test, between groups, repeated measures, f-test, f-ratio, post-hoc, tukey\ns procedure, tukey, factorial anova, two independent variables, one dependent variable, main effect, interaction effect, simple effect, effect size, homogeneity of variance, normal distribution\n\n\nCourse Four Repeated Measures (ANOVA), \nstatistical power, order effects, counterbalancing, missing data, extra assumption, homogeneity of variance, homogeneity of covariance, sphericity, systematic, between, unsystematic, within, subjects, f-test, mean-squared, ms, post-hoc test, holm\n\n\nCourse Five Correlation and Regression, \ncorrelation, r, sum of cross products, sp, covariance, variance, magnitude, causation, causality, sampling, sample, measurement, regression, r-squared, estimation, coefficients, assumptions, normal, linear, homoscedasticity, anscombe\ns quartet, anscombe, scatterplot, residuals\n\n\nCourse Six Multiple Regression, \nregression, simple, multiple, predictors, r-squared, dummy, dummies, matrix algebra, data frame, variance, covariance, standard deviation, correlation, estimation, coefficients, unweighted coding, weighted coding\n\n\nCourse Seven Moderation and Mediation, \nexperimental manipulation, moderator, moderation, enhance, test for moderation, centering predictors, avoid multicollinearity, constant, mediation, partial, full, sobel test\n\n\n\n\nBig Data Analysis with Revolution R Enterprise (completed course, snapshots, videos), \nchallenge, move, merge, manage, munge, large datasets, inspect, transform, create available, summarize, xdf, parallel, visualize, histogram, correlation, subset, linear model, nonlinear relationship, categorical, revoscaler, revolution, distributed environments, cluster, information, dayofweek, summary, quantile, crosstabs, facets, weights, scatterplot, log, data frame, import, min, max, apply, means, drop, predict, machine learning, split, train, test, split, cross validation, standard errors, covariance, logit, logistic regression, generalized least square, cluster, clustering, decision trees, regression tree, revotree, roc curve, classification tree, confusion matrix\n\n\nDataCamp, Data Analysis and Statistical Inference (completed course, snapshots, videos, notes, manuals, codes, datasets), \nimport, plot, inspect, web, load, table, barplot, mosaicplot, mosaic, summarize, structure, nrow, head, tail, str, summary, boxplot, names, descriptive statistics, histogram, sample, simulation, distribution, population, sampling distribution, for loop, sample size, confidence intervals, standard error, dotplot of samples, inferential statistics, bootstrap, interference, confidence level, bootstrap method, parameter of interest, relationships between two variables, by, anova, continuous, categorical, margin of error, proportion, regression, correlation, plot, moneyball, abline, linearity, normal residuals, constant variability, qqnorm, qqline, boxplot, mosaicplot, kitter, adding, removing, residuals, openintro statistics manual\n\n\nData Exploration With Kaggle Scripts (completed course, snapshots, videos, codes, datasets), \nportfolio, github, cases, map, mapping, spanish production of silver, chopsticks effectiveness, pigeon data, kaggle account, phd earnings, american community survey\n\n\nDrivenData Water Pumps Challenge (case), \ndata mining, water table, train, test, ggplots, visualization, map, well, location, random forest\n\n\nExploring Polling Data in R (case), \nvisualization, pools, election\n\n\nHaving Fun with googleVis (case), \nvisualization, gapminder, interactive, graphs\n\n\nHow to work with Quandl in R (completed course, snapshots), \ndata connection, api, python, excel, web, r, database aggregator, library, stocks, finance, economics, census, public services, government, world organizations\n\n\nIntro to Computational Finance with R (completed course, snapshots, notes, codes, datasets, manuals), \ntseries, stock returns, zoo, performanceanalytics, econometrics, risk analysis, financial data, returns, plot, chart, matrix, histogram, boxplot, plot, qqnorm, return distribution, descriptive statistics, args, apply, annualized monthly estimates, graphical analysis, covariance matrix, mvtnorm, simulate, set.seed, sigma, joint probability, rho, probability, expected return model, global minimum variance portfolio, efficient portfolio, efficiency frontier, tangency portfolio, calculate returns, standard error of the variances, hypothesis test of mean, of the correlation, normality, asset returns, class, portfolio theory, t-bills, sharpe slope, the global minimum variance portfolio, efficiency portfolio, quantiles, densities, normal curve, value-at-risk, continuously compounded monthly returns, simple total returns, dividend yields, annual returns, portfolio shares, portfolio returns, price data, index, indices, subset, continuously compounded 1-month returns, monthly compounding, xts, time series, moving average, ma, autoregressive, ar\n\n\nKaggle R Tutorial on Machine Learning (completed course, snapshot, codes), \ntitanic, prediction, decision trees, interpret, predict, summit, overfitting, reengineering, survival rates, pruning, random forest\n\n\nPlotly Tutorial Plotly and R (case)\n\n\nR for the Intimidated (introductory course, videos)\n\n\nR, Yelp and the Search for Good Indian Food (case), \ndata wrangling, web scraping, filter, sieve, sift, data mining, dplyr\n\n\nAssessing Tank Production (case), \nmonte carlo, bootstrap\n\n\nCredit Risk Modeling in R (completed course, IPython notebook, slides, datasets), \nexpected loss, probability, default, exposure, loss, crosstable, loan, mortgage, gmodel, interest, grade, ownership, annual income, age, histogram, outlier, missing, confusion matrix, model prediction, classification accuracy, sensitivity, specificity, logistic regression, logit, generalized linear model, glm, table, logarithmic, predicting, prediction, discriminative, train, training, test, testing, probabilistic regression, probit, log-log, loglog, cutoff, decision tree, gini-measure, gini, rpart, sample, dataset, set, glass matrix, pruning, plotcp, printcp, complexity parameter, cp, prune, plotcp, prp, bad rate, fixed acceptance rate, strategy curve, table, bank, roc, comparison, proc, area under curve, auc, model reduction, discriminant analysis, random forest, neural networks, support vector machine, survival analysis\n\n\nExploring Pitch Data with R (completed course, IPython notebook, slides, notes, images, manuals, datasets), \nvelocity, graphical skills, distribution, fastball, hitting, outcomes, game, date, subset, histogram, abline, vertical line, ifelse, tapply, plot, time series, overlap, jitter, multimodal, bimodal, mix, change, table, prop.table, ball-strike cout, pitch usage, expectancy, paste, concatenate, pitch location, strike zone, locational variable, horizontal, vertical, binning, grid, loop, plot, visual interpretation, bat, batting, batted, contact rate, ggplot2, wide, long, locgrid, layer\n\n\nIntroduction to Python \n Machine Learning (completed course, IPython notebook, slides, notes, images, manuals, datasets)\n\n\nKaggle Python Tutorial on Machine Learning (completed course, IPython notebook, slides, notes, images, manuals, datasets)\n\n\nPython 3 (including Importing Data, completed courses, IPython notebook, slides, notes, datesets), \ndata types, float, int, str, bool, list, subset, slice, change, add, remove, function, type, round, method, built-in, max, len, capitalize, replace, bit_length, index, count, index, append, package, library, pip, python3, numpy, array, pip3, 3d array, ndarray, mean, median, corrcoef, std, round, column_stack, print, concatenation, inline, offline, matplotlib, pyplot, plot, show, scatter, hist, xlabel, ylabel, title, yticks, xticks, dictionary, dictionaries, keys, pandas, tabular, table, data frame, dataframe, index, read_csv, select, bracket, row, column, label-based, loc, integer position-based, iloc, comparison, greater, lower, equal, true, false, boolean, and, or, not, conditional, if, else, elif, filter, compare, loop, while, for, enumerate, in, list, dictionary, array, 2d array, my_dict.items, nditer(my_array), iterrows, apply, random, rand, seed, randint, random walk, range, tails, append, recfromcsv, xscale, clean frame, clf, transpose, flat file, csv, txt, comma, tab, delimiters, semi-colon, numpy, np, vector, array, import, essential for scikit-learn, loadtxt, genfromtxt, datatypes, pandas, data frame, dataframe, scipy, excel, matlab, sas, stata, hdf5, pickled, serialize, bytestream, spreadsheet, pickle, pickle.load, pd.excelfile, parse, sas7bdat, file.to_data_frame, pd.read_stata, h5py.file, data.keys, key, keys, value, values, scipy.io, sci.io.loadmat, scipy.io.savemat, .mat, relational database, postgresql, mysql, sqlite, sql, sqlalchemy, create_engine, engine.connect, con.execute, engine_table_name, query, execute, pd.dataframe, fetch, fetchall, close, rs.keys, engine.connect, join, read_sql_query, web, urllib, urlopen, urlretrieve, request, read, read_csv, close,  pd.read_excel, requests, get, scrape, scraping, beautifulsoup, prettify, find_all, api, json, tweepy, authentification, oauthhandler, os, getcsd, listdir\n\n\nStatistical Thinking in Python (Part 1) (completed course, IPython notebook, slides, notes, datasets)\n\n\nText Mining, Bag of Words (completed course, IPython notebook, slides, notes, datasets), \nworkflow, problem definition, specific goals, identify text, text organization, feature extraction, analysis, insight, recommendation, bag of words, semantic parsing, Documents, corpus, corpora, vcorpus, read.csv, cleaning, preprocessing, tolower, removepunctuation, removenumbers, stripwhitespace, removewords, word stemming, stemdocument, stem_words, stem_completion, turn document matrix, tdm, document time matrix, dtm, word frequency matrix, wfm, tm, rowsums, sort, colsums, term_frequency, qdap, freq_terms, word cloud, stop word, tm_map, function, commonality clouds, commonality.cloud, union, paste, vectorsource, vcorpus, as.matrix, comparison cloud, pyramid plot, pyramid.plot, subset, abs, cbind, data.frame, rownames, word network, word clustering,  hclust, dendrogram, unigram, bifram, trigram, ngram, term weight, single word count, tfldf, penalize word, metadata, readtabular, hr analytics\n\n\nTidy Data in Python (completed courses, IPython notebook, datasets)\n\n\nTime Series in R The Power of xts and zoo (completed courses, IPython notebook, datasets), \nextensible time, xts, zoo, matrix, date, seq, core, coredata, convert, as.xts, as.matrix, import, export, iso8601, date, time, intraday, interval, extraction, first, last, coredata, as.numeric, merge, index, inner join, outer, left, right, rbind, cbind, na.locf, na.approx, lag, difference, diff, endpoints, apply, lapply, split, do.call, convert, to.period, to.quarterly, rolling value, time zone, timezones, indexclass, indextz, indexformat, time, periodicity, to.yearly, nmonths, nquarters, nyears, timestamps, indexwday, unique\n\n\nVisualization with Bokeh (completed courses, IPython notebook, datasets)\n\n\nEconometrics, Spatial, GIS\n\n\nA Primer for Spatial Econometrics with Applications in R, Palgrave, 2014\n\n\nApplied Econometrics, Palgrave, 2011\n\n\nEconometrics by Example, Palgrave Macmillan, 2014\n\n\n\u00c9conom\u00e9trie non param\u00e9trique, Economica, 2008\n\n\nEconomics and History: Surveys in Cliometrics, Wiley-Blackwell, 2011\n\n\nIntroduction \u00e0 l\n\u00e9conom\u00e9trie, Puf, 2009\n\n\nMastering Metrics, Princeton, 2015\n\n\nThe Esri Guide to GIS Analysis, Volume 2: Spatial Measurements and Statistics, Esri Press, 2005\n\n\nVisualizing Data Patterns, CRC Press, 2010, \nmap, visualization, graph, geographical, geointelligence, interactive\n\n\nAn Introduction to R for Spatial Analysis and Mapping, SAGE, 2015\n\n\nAnalyzing Financial Data and Implementing Financial Models using R, Springer, 2015\n\n\nApplied Econometrics with R, Springer, 2008\n\n\nHandbook of Applied Spatial Analysis, Springer, 2010\n\n\nIntroduction to Spatial Econometrics, CRC Press, 2009\n\n\nPerspectives on Spatial Data Analysis, Springer, 2010\n\n\nQGIS-2.14-PyQGISDeveloperCookbook-fr (manual)\n\n\nQGIS-2.14-QGISTrainingManual-fr (manual)\n\n\nQGIS-2.14-UserGuide-fr (manual)\n\n\nSpatial Analysis, Statistics, Visualization, and Computational Methods, CRC Press, 2016\n\n\nSpatial Statistics \n Geostatistics, SAGE, 2013\n\n\nUsing Econometrics A Practical Guide, 6th edition, Pearson, 2014\n\n\nGit\n\n\nCheat sheets\n\n\nLearn Git (completed course, notes, snapshots, manual), Codecademy, 2015\n\n\nGit maitrisez la gestion de vos versions, eni, 2016\n\n\nPragmatic Guide to Git, 1st edition, Pragmatic Bookshelf, 2010\n\n\nPragmatic Version Control Using Git, Pragmatic Bookshelf, 2009\n\n\nPro Git, 2nd ed\n\n\nHadoop\n\n\nhadoop, vmware, conception, development, developer, training, skills, administration, big data, apps, building, projects\n\n\nBig Data Concept et mise en oeuvre de Hadoop, eni, 2014\n\n\nCheatsheets\n\n\nProjects in Hadoop and Big Data, Learn by Building Apps, Udemy, 2015\n\n\nUdemy potential courses:\n\n\n\n\nBecome a certified hadoop developer training tutorial\n\n\nintroduction to big data hadoop map-reduce\n\n\nMaster apache hadoop infinite skills hadoop training\n\n\nMaster big data and hadoop administration\n\n\nMaster apache hadoop, infinite skills hadoop training\n\n\nMaster big data and hadoop administration\n\n\nMaster big data and Hadoop step-by-step from scratch\n\n\nProjects in hadoop and big data, learn by building apps\n\n\nand many more\n\n\n\n\nHTML, CSS\n\n\nCheat sheets\n\n\nDeploy a Website (completed course, notes, snapshots, codes, manual), Codecademy, 2015, \nstatic website generator\n\n\nHTML \n CSS Final Project (uncompleted course, notes, snapshots, codes, manual), Codecademy, 2015\n\n\nMake a Website (completed course, notes, snapshots, codes, manual) , Codecademy, 2015, \nstatic website generator\n\n\nMake an Interactive Website (completed course, notes, snapshots, codes, manual) , Codecademy, 2016, \nstatic website generator\n\n\nConcevez votre site web avec PHP et MySQL, Simple IT, Livre du Z\u00e9ro, 2010, \nhtml, css, dynamic, wamp, mamp, xampp, editor, variable, conditional, loops, function, table, url, form, transmit, transmission, password, session, cookie, read, write, open, close, files, sql, database, record, save, error, insert, update, delete, chat, comment, joint, image, regular expression, mvc architecture, member, script\n\n\nR\u00e9alisez votre site web avec HTML5 et CSS3, Simple IT, Livre du Z\u00e9ro, 2011, \nhtml, css, static, web page, markup, attributes, comments, paragraph, title, list, links, image, figure, style, class, id, selector, size, police, alignment, floating, italic, bold, underline, color, background, transparency, border, round, shadow, click, select, block, inline, dimension, margin, absolute, relative, design, content, table, form, video, audio, adaptative, responsive, query, dom, javascript, canvas, svg, sockets, dynamic, php, jee, asp, .net, domain, host, heading, footer, debug\n\n\nImport.io\n\n\nInternet of Things\n\n\nwearable, electronics, design, prototype, wear, interactive, garments, hackable, magazine, raspberry pi, arduino, internet of things, connected object, domotic, quantify-self\n\n\nArduino, maitrisez sa programmation et ses cartes d\ninterface (shields), 2e \u00e9dition, Dunod, 2014\n\n\nArduino 3, Premiers pas en informatique embarqu\u00e9e, 2014\n\n\nLe guide de la maison et des objets connect\u00e9s, Eyrolles, 2016\n\n\nMake: Wearable Electronics\n\n\nObjects connect\u00e9s, la nouvelle r\u00e9volution num\u00e9rique, eni, 2016\n\n\nJavaScript\n\n\nCheat sheets\n\n\nJavaScript (completed course, notes, snapshots, codes, manual), Codecademy, 2016, \nstatic website generator\n\n\nJavaScript API (uncompleted course, notes, snapshots, codes, manual) , Codecademy, 2016, \napi, soundcloud, youtube\n\n\nJavaScript Final Project (uncompleted course, notes, snapshots, manual), Codecademy, 2016, \nstatic website generator\n\n\nD3.js in Action, Manning, 2015, \nvisualization, interactive, online, server-side, back-end, web, website, web page\n\n\nData Visualization with JavaScript, No Starch Press, 2015\n\n\nData Visualization with Python and JavaScript, O\nReilly, 2016\n\n\nJavaScript, Dunod, 2010\n\n\nWeb Development with Node and Express, O\nReilly, 2014\n\n\nLaTeX, Markdown\n\n\nmathematics, markdown, r, text, formulas, tex, packages, markup\n\n\nCheat sheets\n\n\nLaTeX pour l\nimpatient, 3e \u00e9dition, MiniMax, 2009\n\n\nLaTeX appliqu\u00e9 aux sciences humaines, Atramenta, 2012\n\n\nPandoc - Demos, \npandoc, cli document converter, pdf, html, markdown,  md, latex, tex, word, doc, txt, rft\n\n\nR\u00e9digez des documents de qualit\u00e9 avec LaTeX, Simple IT, Livre du Z\u00e9ro, 2010\n\n\nThe Not So Short Introduction to LaTeX, CC, 2015\n\n\nTout ce que vous avez toujours voulu savoir sur LaTeX sans jamais oser le demander, CC, 2018\n\n\nLinux\n\n\nCheat sheets\n\n\nLinux (notes)\n\n\nLinux in a Nutshell, 6th Edition, O\nReilly, 2009\n\n\nLinux, base d\nadministration et de programmation (scans), \nvirtual, virtualization, server, samba, administration, debug, commands\n\n\nReprenez le contr\u00f4le \u00e0 l\naide de linux, Simple IT, Livre du Z\u00e9ro, 2010\n\n\nThe Linux Command Line, 2nd Edition, CC, 2013\n\n\nNLP, Digital Humanities\n\n\ndigital humanities, history, text mining, machine learning\n\n\nCheat sheets\n\n\nStatistiques en sciences humaines avec R, PUL, 2013, \nquantitative methods, social sciences, quantitative research, hypotheses, database, univariate, seminary, histogram, list, frequency table, text exploration, multivariate, relation, correlation, Association, regression, cohen, time series, residuals, error margin, function, significance, tests, power, inferential, partial correlation, real estate, voting, road death toll, logistic, principal component, correspondence analysis\n\n\nA century of trends in adult human height (article), eLIFE, 2015, \npublic health\n\n\nComputational Folklorists (article), ACM, 2012\n\n\nExploring Big Historical Data, Imperial College Press, 2015\n\n\nNLTK Essentials, Build cool NLP and machine learning applications using NLTK and other Python libraries, Packt, 2015\n\n\nText Mining Infrastructure in R (article), Journal of Statistical Software, 2008\n\n\nNoSQL\n\n\nBig Data and Reporting with MongoDB, PluralSight, 2014\n\n\nCheat sheets\n\n\nThe Magical Marvels of MongoDB (completed course, videos, notes, snapshots, manual), Code School, 2015, \ninstall, run, launch, commands, cli, shell, format, robomongo, gui, driver, university, course, tutorial, pluralsight\n\n\nLearn NoSQL Database From Scratch, MongoDB, Udemy, 2014\n\n\nMongoDB Initiating the Next Step, Udemy, 2016\n\n\nBases de donn\u00e9es orient\u00e9es graphes avec Neo4j, Eyrolles, 2016\n\n\nLes bases de donn\u00e9es NoSQL et le Big Data, 2e \u00e9dition, Eyrolles, 2016\n\n\nMongoDB Data Modeling, Focus on data usage and better design schemas with the help of MongoDB, Packt, 2015\n\n\nMongoDB Applied Design Patterns, O\nReilly, 2013\n\n\nNoSQL For Dummies, 2015\n\n\nPredictive Modeling, Data Science, Data Mining, Marketing Analytics\n\n\npreferences, choices, market basket, economic data analysis, operation management, text analysis, sentiment analysis, sports analytics, spatial data analysis, brands and prices, linear models, regression, na\u00efve bayes, clustering, k-means, optimization, non-linear programming, algorithms, time series, forecasting, smoothing, monte carlo, outliers, data preprocessing, data analysis, exploratory, statistical analysis, statistics, k-nn, decision trees, neural networks, k-means, kohonen, association rules, missing data, charts, models, visualization, data mining, machine learning, words clouds, charts, keywords search, semantic search, svn, markov model, logistic regression, classification, curve fitting, scoring, supervises, unsupervised, cases, classification, distribution, relationship, random forest, pruning, prediction, misclassification, evaluation, errors, generalized linear regression, polynomial regression, piecewise-linear regression, least squares, correlation, distance, k-medians, k-medoids, k-centers, models, average, weighted average, kernel, svr, transformation, attribute, hypothesis, tests, learning, testing, support vector machine, missing data, segmentation, overfitting, fitting, underfitting, neighbours, confusion matrix, roc, Lift, text mining, bag of words, term frequency, topic analysis, co-occurence, redux, mapreduce, hadoop, parallel processing, nosql, sql, random, expectations, descriptive, inferential, ordinary least squares, topology, spatial plot, map, mapping, factor analysis, principal component analysis, correspondence analysis, association analysis, bayesian methods, bootstrapping, scoring, web mining, social media mining, mail mining, email mining, github mining, facebook mining, linkedin mining, twitter mining\n\n\nSEE: Econometrics, Spatial, GIS; R, Statistics; PYTHON\n\n\nAnalytics for Insurance, The Real Business of Big Data, Wiley, 2016, \nrisk, management, underwriting, claim, marketing, property, liability, life, pensions, people, \n\n\nData Smart Excel: Using Data Science to Transform Information into Insight, Wiley, 2013, \nexcel\n\n\nMarketing Analytics, A Practical Guide to Real Marketing Science (excerpts, BANQ), Wiley, 2015, \nstatistics, consumer, behaviour, strategy, regression, business case, segmentation, elasticity, test, control, lift, collinearity, logistic, market basket analysis, survival, lifetime, value, ltv, descriptive, predictive, simultaneous equations, segment, k0means, lca, rfm, behavioural, missing value, research, conjoint analysis, structural equation, sem, sample, a/b, testing, factorial, engagement\n\n\nStatistical Methods in Customer Relationship Management (excerpts, BANQ), Wiley, 2012, \nwith sas, crm, acquisition, retention, churn, win-back, models, vector autoregressive, lifetime, tobit, multinomial, logit, hazard, survival, event, poisson, regression, binomial\n \n\n\nData science, Fondamentaux et \u00e9tudes de cas_R_Py, Machine Learning avec Python et R, Eyrolles, 2015\n\n\nData Science for Business, What You Need to Know About Data Mining and Data-Analytic Thinking, O\nReilly, 2013\n\n\nMarketing Analytics, Wiley, 2014, \nexcel, spreadsheet, pivot table, pricing, bundling, skimming, revenue, forecasting, regression, trend, winter\ns, moving average, neural networks, conjoint analysis, logistic, discrete choice, lifetime customer value, monte carlo, acquisition, retention, segmentation, cluster, tree, s curve, bass diffusion, copernican, duration, market basket, lift, rfm, scan*pro, retail space, sales, point of sales, pos, advertising, media, pay per click, online, principal compenent, pca, multidimentional scaling, mds, naive bayes, discriminant analysis, anova, internet, network, viral, text, mining\n\n\nMarketing Data Science, Modeling Techniques in Predictive Analytics with R and Python (epub), Wiley, 2015, \nmarket, consumer choice, customer, retention, acquisition, positioning, promiting, recommending, brands, prices, social networks, competitors, predicting, sales, database, regression, bayesian, data mining, machine learning, data visualization, text, sentiment analysis, time series, market response, sampling, www, social media, surveys, experiments, interviews, focus groups, field search cases\n\n\nPractical Data Analysis, Transform, model, and visualize your data through hands-on projects, developped in open source tools, Packt, 2013, \npython, mlpy, d3.js, mongodb, data source, open data, text files, excel files, sql databases, nosql databases, multimedia, web scraping, data scrubing, csv, json, xml, yaml, openrefine, d3.js, html, dom, css, javascript, svg, text classification, image retrieval, stock prices, predicting prices, modeling epidemiology, social graphs, social networks, facebook, gephi, gdf, json, twitter, oauth, twython, sentiment classification, nltk, mapreduce, ipython, wakari, infrastructure, ubuntu, windows, python3, numpy, scipy, mlpy, openrefine\n\n\nProgrammation for Kids\n\n\nCahier d\nactivit\u00e9, Scratch pour les kids, Eyrolles, 2015\n\n\nJavaScript for Kids, No Starch Press, 2014\n\n\n\u00c0 l\naventure avec Arduino !, Eyrolles, 2015\n\n\nApprendre \u00e0 coder en Python grace \u00e0 Minecraft, Eyrolles, 2014\n\n\nBBC micro:bit, 27 Projects for Students, Level 1, 2016\n\n\nJavaScript For Kids For Dummies 2015\n\n\nMaking Games with Python \n Pygame, CC, 2012\n\n\nMicroPython for ESP8266 Development Workshop, 2016\n\n\nPython pour les kids, La programmation accessible \u00e0 tous ! D\u00e8s 10 ans, Eyrolles, 2013\n\n\nPython pour les kids, D\u00e8s 10 ans, Eyrolles, 2016\n\n\nSuper Scratch Programming Adventure, No Starch Press, 2010\n\n\nThe Official ScratchJr Book Help Your Kids Learn to Code, No Starch Press, 2016\n\n\nThe R Student Companion, CRC Press, 2013, \nscript, function, graphs, input, output, loops, logic, control, mathematic functions, arithmetics, trigonometrics, matrix, system linear equation, graphs, probability, simulation, fitting\n\n\nProject Management\n\n\nLearning Path, DevOps with Docker (course, video)\n\n\nDevOps for Digital Leaders, Reignite Business with a Modern DevOps-Enabled Software Factory, 1st Ed, CA Press, 2016\n\n\nEffective DevOps, O\nReilly, 2015\n\n\nMastering IT Project Management (PCO et ITIL) (notes), \npco, pmi, pmp, itil, skills, process\n\n\nScrum (cheat sheet)\n\n\nPython\n\n\nAdvanced, Specialized\n\n\nprogramming, coding, script\n\n\nApprendre la programmation avec Python et Django, Eyrolles, 2012\n\n\nData Structures and Algorithms in python, Wiley, 2013\n\n\nData Wrangling With Python, Tips and Tools to Make Your Life Easier, O\nReilly, 2016\n\n\nFluent Python, O\nReilly, 2014\n\n\nFreeCAD, Solid Modeling with the Power of Python, Packt, 2012\n\n\nFundamentals of Python From First Programs through Data Structures, CC, 2010\n\n\nHacking Secret Ciphers With Python, CC, 2013\n\n\nLearning Data Mining with Python, Harness the power of Python to analyze, data and create insightful predictive models, Packt, 2015\n\n\nLearning Python, 5th Edition, O\nReilly, 2013\n\n\nMastering Pycharm, Use PyCharm with fluid efficiency, Packt, 2015\n\n\nMatplotlib (manual), 2013\n\n\nProgrammation Python, conception et optimisation, 2e \u00e9dition, Eyrolles, 2009\n\n\nPython Cookbook, 3rd Edition, O\nReilly, 2013\n\n\nPython Essential Reference, 4th Edition, Addison-Wesley, 2009\n\n\nPython for Data Analysis, O\nReilly, 2012\n\n\nPython Geospatial Development, Learn to build sophisticated mapping applications from scratch using Python tools for geospatial development, 2nd Edition, Packt, 2013\n\n\nPython Programming for Hackers and Reverse Engineers, No Starch Press, 2009\n\n\nReal World Instrumentation with Python, O\nReilly, 2010\n\n\nWeb Scraping with Python, O\nReilly, 2015\n\n\nEntry-level\n\n\nAutomate the Boring Stuff with Python, No Starch Press, 2015 \n\n\nCheat sheets\n\n\nPython (completed course, notes, snapshots, codes, manual), Codecademy, 2015\n\n\nPython API (completed course, notes, snapshots, codes, manual), Codecademy, 2016, \napi, dwolla, nhtsa, npr, wepay\n\n\nPython Final Project (uncompleted course, notes, snapshots, manual), Codecademy, 2015, \nmarkov chain, github\n\n\nPython Projects (notes, codes), Codecademy, 2015, \nbank account, battleship, calendar, grading, guess game, converter, game\n\n\nData Science and Machine Learning with Python (course, videos)\n\n\nData Science and Machine Learning with Python (videos, 9h), Udemy, 2015\n\n\nIntro to Python for Data Science (completed course, notes, snapshots, manual), DataCamp, 2015,\n\n\npython3\n\n\nLearn Python the Hard Way (completed course, videos, notes, snapshots, codes, manual)\n\n\nManaging Your Biological Data with Python (notes, script, images, files), CRC Press, 2014\n\n\nUnderstanding Machine Learning with Python (course, videos), 2016\n\n\nLPTHW Python +\n\n\nPython Glossary\n\n\nA Comprehensive Introduction to Python Programming and GUI Design Using Tkinter (article)\n\n\nAmazon Web Services in Action, Manning, 2016\n\n\nAn introduction to GUI programming with Tkinter (presentation), SciNet, 2014\n\n\nAn Introduction to Tkinter (presentation), CC, 2008\n\n\nApprenez \u00e0 programmer en Python, Simple IT, Livre du Z\u00e9ro, 2011\n\n\nBayesian Methods for Hackers, Addison-Wesley, 2016, \nbayesian, inference, pymc, python, a/b testing, cases, distribution, algorithms, plot, graphs, statistics, probability, loss function, machine learning\n\n\nBuilding Machine Learning Systems with Python, Master the art of machine learning with Python and build effective machine learning systems with this intensive hands-on guide, Packt, 2013\n\n\nBuilding Machine Learning Systems with Python, Get more from your data through creating practical machine learning systems with Python, 2nd Edition, Packt, 2015, \niris, classifying, scikit-learn, clustering, bag of words, k-means, topic modelling, data mining, knn, logistic, regression, sentiment analysis, naive bayes, classifier, tweets, clean, word,  cross-validation, penalize, regulatize, lasso, elasticnet, text, predictions, recommendations, basket analysis, classification, music, computer vision, dimension reduction, big data, amazon web service\n\n\nData Science from Scratch First Principles with Python, O\nReilly, 2015, \nvisualization, linear algebra, statistics, probability, hypothesis, inference, gradient descent, fetch, read, file, scraping, web, api, twitter, exploring, machine learning, k-nearest neighbours, knn, naive bayes, regression, logistic, decision tree, neural networks, clustering, natural language processing, network analysis, recommender system, databases, sql, mapreduce\n\n\nData Wrangling with Python Tips and Tools to Make Your Life Easier, O\nReilly, 2016, \nexcel, spreadsheet, pdf, storing, clean, format, outilier, bad, duplicates, match, regex, standardizing, scripting, exploration, presenting, reporting, web, scraping, screen, spiders, api, automation, scaling\n \n\n\nFlask By Example, Unleash the full potential of the Flask web framework by creating simple yet powerful web applications, Packt, 2016\n\n\nFlask Framework Cookbook, Over 80 hands-on recipes to help you create small-to-large web applications using Flask, Packt, 2014\n\n\nFlask Web Development, Developing Web Applications with Python, O\nReilly, 2014\n\n\nIntroducing Data Science, Big Data, Machine Learning and more, using Python tools, Manning, 2016, \nbig data, project management, machine learning, large data, steps, nosql, graph databases, text mining, text analytics, data visualization, user\n\n\nIntroduction to Computation and Programming Using Python (theory), Revised, MIT Press, 2013, \ntesting, debugging, exceptions, assertions, classes, oop, algorithmic complexity, algorithms, plotting, stochastic, programs, probability, statistics, random walks, data visualization, monte carlo, experimental data, dynamic programming, machine learning\n\n\nIntroduction to Machine Learning with Python, O\nReilly, 2017, \nscikit-learn, jupyter, ipython, knn, regression naive bayes, decision trees, ensembles, support vector machines, neural networks, decision function, train, test, scale, pca, principal component, non-negative matrix factorization, nmf, manifold learning with t-sne, clustering, k-means, binning, trees, polynomials, nonlinear, regression, models, forms, statistics, cross-validation, grid search, metrics, scoring, preprocessing, pipeline, text, strings, bag of words, ngram, topic modeling, token, document, stemming, lemmatization\n\n\nIntroduction to Python for Econometrics, Statistics and Data Analysis, CC, 2014\n\n\nIPython Interactive Computing and Visualization Cookbook, Over 100 hands-on recipes to sharpen your skills in high-performance numerical computing and data science with Python, Packt, 2014\n\n\nIPython Notebook Essentials, Compute scientific data and execute code interactively with NumPy and SciPy, Packt, 2014\n\n\nLarge Scale Machine Learning with Python, Learn to build powerful machine learning models quickly and deploy large-scale predictive applications, Packt, 2016, \nscalability, ipython, scikit-learn, streaming, stochastic, learning, support vector machines, nonlinearity, hyperparameter, neural networks, deep learning, h2o, tensorflow, neural networks, keras, tree, classification, cart, boosting, xgboost, random forest, pca, principal component analysis, k-means, lda, cpu, memory, machine, distributed, hadoop, spark, virtual box, hdfs, yarn, spark, pyspark, machine learning, cluster, preprocessing, \n\n\nLearning IPython for Interactive Computing and Data Visualization, Learn IPython for interactive Python programming, high-performance numerical computing, and data visualization, Packt, 2013\n\n\nMachine Learning in Python, Essential Techniques for Predictive Analysis, Wiley, 2015, \nmachine learning, classification, visualization, predictive, balancing performance, big data, linear regression, penalize, predictive model, logistic, binary, multinomial, emsembles, tree, bootstrap, gradient boosting, random forest, bagging, binary classification\n\n\nMastering Machine Learning with scikit-learn, Apply effective learning algorithms to real-world problems using scikit-learn, Packt, 2014, \nlinear regression, polynomial, regularizatino, gradient descent, machine learning, scikit-learn, extraction, preprocessing, text, images, logistic regression, binary classifier, tree, clustering, k-means, pca, principal component analysis, perceptron, support vector machines, neural networks, \n\n\nMining the Social Web Data Mining Facebook, Twitter, LinkedIn, Google+, GitHub, O\nReilly, 2013\n\n\nProgrammation en Python pour les math\u00e9matiques, 2e \u00e9dition, Dunod, 2011\n\n\nProgramming ArcGIS 10.1 with Python Cookbook, Over 75 recipes to help you automate geoprocessing tasks, create solutions, and solve problems for ArcGIS with Python, Packt, 2013, \ngeoprocessing script, ArcPy, map, documents, layers, referencing, data frame, layers, tables, symbology, finding, fixing, broken data, find, replace, automating, production, printing, exporting, map book, geoprocessing, retrieving, tool, creating tools, querying, data, table, listing, gis, describing, customizing, interface, add-ins, error handling, troubleshooting, python, script\n \n\n\nPython 3 niveau avanc\u00e9, CC, 2016\n\n\nPython Data Visualization Cookbook, Over 70 recipes, based on the principal concepts of data visualization, to get you started with popular Python libraries, 2nd edition, Packt, 2015, \nimporting, exporting, reading, cleaning, generating random, smoothing noise, plotting, customizing plot, 3d, visualiztion, charts, images, maps, capchat, logarithm, spectogram, vector flow, colormaps, cross correlation, autocorrelation, matplotlib, drawing, latex, api, plotly, cloud\n\n\nPython For Data Analysis, O\nReilly, 2013, \nipython, numpy, pandas, loading, storage, formats, data, wrangling, cleaning, transforming, merging, reshaping, plotting, visualizing, visulization, aggregation, operations, time series, financial, economic, data, app, manipulation, advanced, sorting, matrix, broadcasting\n\n\nPython for Data Science For Dummies, 2015\n\n\nPython for Education, Learning Maths \n Science using Python and writing them in LATEX, CC, 2010\n\n\nPython Machine Learning, Unlock deeper insights into machine learning with this vital guide to cutting-edge predictive analytics, Packt, 2015, \nalgorithms, training, classification, neural networks, classifier, scikit-learn, logistic,  regression, support vector machines, kernel, nonlinear problems, tree, knn, missing data, categorical data, train, test, regularization, dimensionality reduction, discriminant analysis, nonlinear mapping, hyperparameter, tuning, cross-validation, pipelines, debugging, metrics, emsembles, bagging, bootstrap, sentiment analysis, bag of words, online algorithm, sqlite, flash, into web app, flask, web application, displaying, predicting, ols, regularize, polynomial,  clustering, k-means, tree, neural networks, handwritten, backpropagation, gradient, parallelizing\n\n\nPython Pocket Reference, 5th Edition, O\nReilly, 2014\n\n\nPython Real World Machine Learning, Learn to solve challenging data science problems by building powerful machine learning models using Python, Packt, 2016, \npreprocessing, label, endoding, linear, regression, model persitence, ridge, polynomial, price, demand, classifier, regression, logistic, naive bayes, train, test, cross-validation, confusion matrix, performance, curves, support vector machines, hyperparameters, predictor, clustering, k-means, vector, cluster, algorithm, recommendation, pipeline, euclidean distance, pearson distance, movie recommendation, stemming, lemmatization, chunking, bad of words, text, gender, sentiment, topic modeling, speech recognition, audio, frequency, music, markov model, speech recognizer, time series, conditional random fields, images, detecting, biometric, face, recognition, deep neural networks, visualizing, pca, principal component analysis, k-means, deep learning, bolzmann, denoising autoendoders, convolutional, cnn, text, engineering, cleaning, tagging, nltk, correlation, lasso, recursive feature elimination, genetic model, restful, api, twitter, ensembles, random forests, xgboost, stacking, lasagne, tensorflow, scalability, python, scikit-learn, h2o, gensim, theano, sknn, theanets, keras, streaming, stochastic, deep learning, cart, boosting, gradient boosting, virtual box, hadoop, hdfs, yarn, spark\n\n\nPython Testing with unittest, nose, pytest, Leanpub, 2014\n\n\nReal Python, An introduction to Python through practical examples, CC, 2013\n\n\nscikit-learn Cookbook, Over 50 recipes to incorporate scikit-learn into every step of the data science pipeline, from feature extraction to model building and model evaluation, Packt, 2014, \nworkflow, source, sample, categorical, label, missing value, pipelines, pca, principal component analysis, factor analysis, nonlinear dimensionality, truncated, support machines, gradient descent, fitting, linear model, regression, ridge, sparsity, regularize, lars, classification, logistic, bayesian, boosting, k-means, centroids, cluster, knn, tree, random forests, support vector machines, multiclass, classification, lda, qda, stochastic, naive bayes, propagation, cross-validation, grid search, dummy estimator\n\n\nScipy, Lectures Notes, CC, living book\n\n\nLearn Web Scraping With Python In A Day, Acodemy, 2015\n\n\nR, Statistics\n\n\nAdvanced, Specialized\n\n\nA User\ns Guide to Network Analysis in R, 1st edition, Springer, 2015\n\n\nAn Introduction to R for Spatial Analysis and Mapping, SAGE, 2015\n\n\nBasic Data Analysis for Time Series with R, Wiley, 2014\n\n\nBiostatistique : une approche intuitive (BANQ), De Boeck, 2013, \ntests, hypothesis, p-value, proportion confidence interval, survival analysis, censored, poisson, error, bias, continuous, categorical, spread, gauss, log-normal, geometric, significance, tests, type i, type ii, equivalence, comparison, group, outlier, khi-squared, propective, experimental, relative risk, survey, two mean, groups, correlation, regression, model, nonlinear, logistic, anova, nonparametric, sensitivity, roc, bayes, sample, sampling, cases\n\n\nComprendre et r\u00e9aliser les tests statistiques \u00e0 l\naide de R : manuel de biostatistique (BANQ), De Boeck, 2014, \nr, descriptive statistics, random variable, data table, scientific methodology, survey, outliers, tests, probability distribution, binomial, multinomial, pascal, negative binomial, geometric, hypergeometric, poisson, laplace-gauss, normal, exponential, gamma, chi-squared, fisher-snedecor, student, mann-whitney, wilcoxon, hypothesis, type i, type ii, bias, error, proportion comparison, conformity, homogeneity, g, mantel-haenszel, mac nemar, mean comparison, welch, anova, median comparison, mann-whitney-wilcoxon, kruskal-wallis, variance comparison, ansari-bradley, bartlett, fligner-killeen, correlation comparison, pearson, spearman, kendall, distribution comparison, kolmogorov-smirnov, shapiro-wilk, regression, survival, covariance, central limit\n\n\nLearning Bayesian Models with R, Become an expert in Bayesian machine learning methods using R and apply them to solve real-world Big Data problems, Packt, 2015 \n\n\nMaking your Case, Using R for Program Evaluation (BANQ, c7, c8, c8, ssdanalysis.com), Oxford University Press, 2015, \ntest, variance, var.test, t, t.test, cohen.d, dchange, aov, wilcox, wilcox.test, kruskal, kruskal.test, mcnemar, mcnemar.test, wald, wald.test, nonconstant variance, ncvtest, log-lin, logistic regression\n\n\nEntry-level\n\n\nCheat sheets\n\n\nData Mining Applications with R, Elsevier, 2013, \npower grid, hadoop, property value, web, random forest, data mining, classification, images, crime analyses, football mining, internet dns, traffic, optimization, bayesian classifiers, text mining, topic modeling, social network,  recommender systems, response modeling, direct marketing, insurance customer profile, predicting bank loan default, customer preference analysis\n\n\nDiscovering Knowledge in Data, An Introduction to Data Mining, 2nd edition, Wiley, 2014, \nlarose\n\n\nDoing Bayesian Data Analysis, 2nd edition, AP, 2014, \ncredibility, model, parameters, bayesian, r, probability, bayes\ns rule, inferring, binomial, probability, markov chain, monte carlo, jags, hierarchival model, model comparison, hypothesis, testing, sample size, stan, generalized linear model, metric, predicted, predictor, dichotomous, qualitative, nominal, ordinal, count, tools\n\n\nGuidebook to R Graphics Using Microsoft Windows, Wiley, 2012\n\n\nIntroducing Survival and Event History Analysis, SAGE, 2011, \nsurvival, event, history, r, data, exploration, descriptive statistics, data structures, nonparametric, kaplan-meier estimator, \n\nIntroduction to Stochastic Process with R, Wiley, 2016, \nmarkov chain, stationary, periodicity, ergodic, time, absorbing, branching, probability, extinction, monte carlo, gibbs sampler, sampler, eigenvalue, card shuffling, poisson process, arrival, interarrival, thinning, superposition, uniform distribution, spatial poisson, nonhomogeneous, parting, continuous-time, brownian motion, gaussian process, transformation, properties, variations, applications, ito integral, discrete random variable, joint distribution, continuous random variable, common probability distributions, moment-generating functino, matrix algebra\n\n\nIntroduction to Stochastic Process with R, Wiley, 2016, \nmarkov chain, branching, monte carlo, poisson process, continuous-time markov chain, brownian motion, stochastic calculus, probability, algebra\n\n\nLearning RStudio (notes)\n\n\nModeling Techniques in Predictive Analytics, Business Problems and Solutions with R, Pearson, 2014\n\n\nMod\u00e9lisation pr\u00e9dictive et apprentissage statistique avec R (Tuff\u00e9ry, aper\u00e7u), Technip, 2015\n\n\nModern Multivariate Statistical Techniques, Regression, Classification, and Manifold Learning (extracts), Springer, 2008\n\n\nMonte Carlo Simulation and Resampling Methods for Social Science with R, SAGE, 2013, \nprobability, random number generation, statistical stimulation, linear model, generalized linear model, simulation, testing theory, resampling\n\n\nMultivariate generalized linear mixed models using R, CRC Press, 2011, \ngeneralized linear models, continuous, interval, scale, logistic, logit, probit, likelihood, binary data, ordinal data, ordered logit, ordered categories, poisson regression models, count data, mixed models, continuous, interval, scale data, two-level logistic, general two-level logistic, intraclass correlation, two-level ordered logit model, likelihood, tow-level poisson model, three-level, duration, event history, left censoring, right censoring, time-varying, competing risk, single-level, two-level, three-level, renewal models, stayer, non-susceptible, endpoint, mover-stayer, handling initial condition, state dependence, random effect, initial contition problems, condition analysis, woordridge conditinal model, joint analysis, random effect, link function, dummy, sabrer, r\n\n\nNonlinear Parameter Optimization Using R Tools, Wiley, 2014, \noptimization, algorithms, one-parameter, root-finding, minimization, optimize, nonlinear least squares, nonlinear equations, function, calculation, derivatives, bound, constraints, masks, scaling, centering, mathematical programming, global optimization, reparameterization, solution, kkt, tuning, terminating, neos, nlopt, bugs, differential equation, miscellaneous, generalized nonlinear model, systems of equations, noisy objective functino, moving forward\n\n\nR Graphics, 2nd edition, CRC Press, 2011\n\n\nSpatial and Spatio-temporal Bayesian Models with R-INLA, Wiley, 2015, \nbayesian methods, computing, regression, hierarchical, nonlinear regression, random walk, household and income, generalized linear model, cd4 counts, aids, cancer, mortality, spatial modeling, gmrf, mapping, mym, suicide, disease, zero-inflated model, geostatistical model, spde, spatial prediction, spatio-temporal, bivariate model, joint model, gaussian distribution, non-gaussian distribution, semicontinuous model, rainfall, spatio-temporal dynamic model, besag, space-time model\n\n\nSpatial Regression Model, SAGE, 2011, \ninteraction, social science, spatial dependence, map, spatial association, correlation, proximity, spatial model, spatially lagged dependent variable, maximum likelihood, equilibrium effect, spatial dependence, italy, different weights matrices, ols, dummy variable, spatial error, maximum likelihood, democracy, development, dyadic trade flow, connectivities, inference\n\n\nStatistical Hypothesis Testing with SAS and R, Wiley, 2014, \nnormal distribution, tests, mean, t-test, z-test, variance, sample, f-test, binomial distribution, proportion, sample, k-sample, poisson distribution, exponential distribution, correlation, nonparametric, location, wilcoxon, krustal-wallis, scale difference, siegel-tukey, ansari-bradley, goodness-of-fit, edf, kolmogorov-smirnov, anderson-darling, cramer-von mises, shapiro-wilk, jarque-bera, distribution, randomness, wald-wolfowitz, run, von neumann, von newmann rank, contingency, fisher, pearson, likelihood, cohen, kappa, large sample, outliers, grubb, david-hartley-pearson, regression, slope, intercept, anova, bartlett, levene\n\n\nStatistiques et probabilit\u00e9s appliqu\u00e9es, puf, 2008, \ndescriptive, frequency, centre, average, median, pourcentile, variance, distribution, range, absolute, parameters, probability, events, conditional, independence, random variable, discrete, binomial, continuous, normal, gaussian, test, poisson distribution, exponential distribution, function, sampling, confidence interval, hypothesis, difference, regression\n\n\nText Mining and Visualization, Case Studies Using Open-Source ToolsCRC Press (RapidMiner, Knime, Python, R), 2016, \nrapidminer, text analytics, corpus, token, repository, mining, visualization, documents, rank-frequency, sequential window, zipf-mandelbrot, knime, preprocessing, frequencies, transformation, data table, social media, network mining, slashdot, python, mongodb, sparse matrix, character encoding, web scraping, cleansing, visualization, exploration, classification, clustering, pca, principal compenent analysis, sentiment, mining search, logs, r\n\n\nAn Introduction to R (pdf)\n\n\nAn Introduction to R for Quantitative Economics, Springer, 2015, \nrstudio, crude oil price, supply, demand, fish, function, derivative, elasticity, linear, log-log, cobb,douglas, matrix, statistics, regression, simulation, normal, uniform, binomial, central limit theorem, t-test, logit, anscombe, graphs, scatter, growth, time series, time, random walks, cycles, stochastic, difference, air passengers, inflation, phillips curve, stock market\n\n\nAn Introduction to Statistical Learning with Applications in R, Springer, 2013\n\n\nAnalysis of Questionnaire Data with R, CRC Press, 2012, \ndescription, response, summary statistics, plotting, graphs, relationships, variable, risk, odds, ratio, correlation, hierarchical, clustering, hypothesis, confidence interval, proportion, mean, percentage, means, correlation, groups, linear, logistic, poisson, regression, binary, multinomial, categorical, count, multilevel, predictors, interaction, missing data, bootstrap, random effects, multilevel, composite, distribution, multi-trait, multi-method, error, alpha, structural equation, factor analysis, memory card, univariate, bivariate, multidimensional, inference\n\n\nAnalyzing Baseball Data with R, CRC Press, 2014, \ndatabase, retrosheet, game, paly, pitch, graphics, run, win, expectancy, balls, strikes, career, simulation, streak, bat, mysql, package, mlbam, data\n\n\nArt of R Programming - A Tour of Statistical Software Design, No Starch Press, 2011, \nvector, matrix, array, data frame, programming, math, simulation, oop, input, output, graphics, debugging, speed, memory, code, language, parallel\n\n\nAutomated Data Collection with R A Practical Guide to Web Scraping and Text Mining, Wiley, 2015, \nweb scraping, text mining, html, primer, syntax, tag, attribute, xml, json, parsing, xpath, web document, extracting, http, protocol, libcurl, rcurl, ajax, javascript, dom, chrome, tools, sql, relational, database, dbms, regular expression, regex, string, stringr, toolbox, web, retrieval, api, oauth, robots, statistical text processing, tm, corpus, ngram, cleansing, support vector machines, random forest, maximum entropy, rtexttools, press release, processing, loop, plyr, progress feedback, error, exception, case studies, bills, network, ftp, server, twitter, rest, streaming, visualizing, mining, tweet, geographic distribution, data collection, website inspection, mapping, automating, gathering, storage, sentiment, collecting, analyzing\n\n\nBeginning SQL Server R Services, Analytics for Data Scientists, 1st Ed, Apress, 2016, \nsetup, installation, visual studio, project, scenario, scope, building model, sample, package, plotting, regression, case, wind speed by airport, average temperature by airport, configuring, connecting, reporting, account, url, database, portal, email, encryption, subscription, scaling, bi, report, imporring, generating, cases\n\n\nBig Data Analytics with R and Hadoop, Set up an integrated infrastructure of R and Hadoop to turn your data analytics into Big Data analytics, Packt, 2013, \ninstalling, linux, ubuntu, single node, multinode, hdfs, mapreduce, project, subproject, writing, programmes, scenario, shuffling, sorting, executing, limitations, solving, dataflow, examples, rhadoop, rhipe, streaming, analytics, preprocessing, visualizing, web page categorization, frequency of stock change, sale price of books, machine learning, regression, clustering, recommendation, algorithms, importing, exporting, package, mysql, rmysql, excel, mongodb, sqlite, postgresql, hive, hbase\n\n\nBuilding a Recommendation System with R, Learn the art of building robust and powerful recommendation engines using R, Packt, 2015, \ndata analysis, processing, simlar, euclidian, distance, cosine, pearson, dimensionality, reduction, principal component analysis, data mining, clustering, k-means, tree, ensembles, bagging, random forest, boosting, system, recommenderlab, datasets, matrix, exploration, preparation, item-based, flitering, user-based, evaluating, case, building, engine\n\n\nComputational Biology A Practical Introduction to BioData, A Practical Introduction to BioData Processing and Analysis with Linux, MySQL, and R, 2nd edition, Springer, 2013, \nlinux, shell, sed, awk, perl, mysql, r, operating system, windows, max os x, vmware, files, directories, moving, copying, renaming, attributes, web, remote, wget, curl, scp, ssh, rsync, text, cat, sorting, lines, scrolling, character, word, line, splitting, cut, paste, grep, finding, sparse, pico, vim, dos, pipe, alias, batch, job, command, wildcard, blast, clustalw, ftp, script, path, variables, input, output, echo, read, substitution, quoting, flow control, conditional, logic, notification, debugging, regular expression, sed, pattern, space, substitution, transliteration, deletion, insertion, change, printing, reading, writing, mysql, relational database, example, case\n\n\nComputational Finance with R, An Introductory Course with R, Springer, 2014, \nsecurities, bonds, stocks, options, derivatives, portfolios, investments, engineering, trading, price, discounted cash flow, arbitrage, risk, efficient, market, computational, time series, returns, distributions, density, stationary, autocovariance, forecasting, volatility, correlations, causalities, similarities, rank granger, nonparametric, grouping, clustering, graphs, models, trend, seasonality, arch, garch, nonlinear, semiparametric, neural networksds, support vector machines, tests, brownian motion, binomial tree, monte carlo, continuous, time process, wiener, ito, lemma, geometric, option pricing, black-scholes, trade, mining, value estimation, technical analysis, fundamental analysis, optimization, heuristic, combinatorial, annealing, genetic programming, ant colony, portfolio optimization, mean-variance, online finance, price search, online trading, portfolio selection\n  \n\n\nData Mining Algorithms Explained using R, Wiley, 2015\n\n\nData mining and business analytics with R, Wiley, 2013, \nregression, polynomial, nonparametric, model selection, parsimony, false discovery, lasso, logistic regression, binary classification, knn, naive bayes, multinomial logistic regression, discriminant analysis, fisher\ns, tree, chaid, baggin, boosting, random forests, support vector machines, neural networks, rattle, data mining, clustering, k-means, hierarchical clustering, market basket analysis, association, lift, dimension reduction, factorial, factor, principal component analysis, pca, multicollinear, partial least squares, text, sentiment, network\n\n\nData Mining and Statistics for Decision Making, Wiley (Tuff\u00e9ry), 2008\n\n\nData Mining with R - Learning with Case Studies, Torgo L., CRC Press, 2011, \nvisualization, summarization, unknown value, filling, missing value, prediction model, regression, tree, evaluation, stock market returns, time-dependent, reading files, csv, web, mysql, windows, linux, prediction, predictors, models, neural networks, support vector machines, splines, simulation, monte carlo, experimental comparison, trading, testing, fraudulent, transactions, dataset, data mining, precision, recall, lift, outlier, box lot, outlier, clustering, class imbalance, naive bayes, adaboost, classifying, anova, random forest, clustering, prediction, evaluation, knn\n\n\nDisplaying Time Series, Spatial, and Space-Time Data with R, CRC Press, 2014, \ntime series, horizontal axis, meteorological, scale, stacked, graphs, grouping, scatter, polylines, colors, positioning, panel, siar, unemployment, gross national income, co2, emission, spatial data, package, mapping, choropleth, raster, vector, physical map, openstreemap, googlemap, air quality, election, lang cover, space-time, spatiotemporal, level, exploratory, animation\n\n\nEvent History Analysis with R, CRC Press, 2012, \nsurvival, censoring, truncated, time scales, continuous time model, discrete time model, cox regression, proportinal hazards, log-rank test, baseline, explanatory variables, interactions, parameters, model, male mortality, poisson regression, pricewise constant hazards, time-varying covariates, communal covatiates, event times, risk sets, residuals, assumptions, period survival, parametric, proportianal hazards, accelerated failure, frailty, stratification, competing rick, probabilities, causality, matching, aalen, additive hazards, dynamic path, inference, symptotic, distribution\n\n\nExtending the Linear Model with R Generalized Linear, Mixed Effects and Nonparametric Regression Models, CRC Press, 2005, \nbinomial, count, regression, contingency, table, multinomial data, generalized linear model, gml, random effect, repeated measures, longitudinal, mixed effect, nonnormal, response, nonparametric, additive, tree, neural networks\n\n\nFoundations of Statistical Algorithms With References to R Packages, CRC Press, 2013, \nregression, iteration, optimization, univariate, multivariate, neural networks, constrained, linear programming, quadratic programming, evolutionary computing, simulated, annealing, maximum-likelihood, pls, em, algorithm, k-means, randomization, uniform distribution, recommended generator, bernouilli, binomial, hypergeometrical, poisson, waiting time, continuous, triangular, normal, mixture, exponential, lognormal, gibbs, rejection, metropolis-hasting, convergence, chain, mcmc, bugs, repetition, resampling, cross-validation, bootstrap, subsampling, hyperparameter, classification, evaluation, error, nonlinear, neural networks, latent variable, scalability, parallelization, parallel computing\n\n\nggplot2 - Elegant Graphics for Data Analysis, 2nd Edition, Springer, 2016, \ncolour, size, shape, aesthetic, facetting, geoms, smoother, boxplot, jittered, histogram, frequency, bar chart, time series, line, path, axes, labels, annotation, groups, surface, maps, vecotr, metadata, raster, area, uncertainty, overplotting, grammar, layers, scales, coordinate, legends, limits, positioning, linear coordinate, nonlinear coordinate, themes, elements, data analysis, tidy, spread, gather, separate, unite, filter, create variable, group-wise, summary, pipeline, trend, model, coefficient, programming, multiple component, function, functional programming\n\n\nGuide to Programming and Algorithms using R, Springer, 2013, \nloop, 1-norm, finding, nested, iteration, geometric, babylonian, recursion, recursive, fibonacci, factorial, solving, highest common factor, lowest common multiple, towers of hanoi, binary search, sequence generation, determinant, program, algorithms, inner product, order notation, infinity norm, matrix-vector, matrix-matrix, binary search, sequence generation, traveling salesman, binomial coefficients, accuracy, polynomial, horner\ns algorithm, sorting, bubble sort, insertion sort, quick sort, comparisons, linear systems of equations, triangular system, forward substitution, backward substitution, gaussian elimination, factorization, banded matrices, cholesky, gauss-jordan, determinant, inverting, file processing, investigating, modifying, multiple files, outputs, projects, traffic, words\n\n\nLattice Multivariate Data Visualization with R, Springer, 2008, \ntreillis, layout, grouped displays, annotation, captions, labels, legends, graphings, panel, univariate, density, histogram, q-q, normality, box-cox transformation, empirical cdf, box-and-whisker, violin, strip coercion, discrete, distribution, multiway table, bar chart, categorical, scatter, superposition, splom, scatter-plot, matrix, parallel coordinate, trivariate, 3d, surface, parameters, themes, devices, graphics, coordinate, axis, scales, labels, legends, data manipulation, subsetting, shingles, ordering, strips, tukey mean-difference, specialized, manipulating, inveractive additions, panel, function, box-and-whisker, corrgram, 3d, maps\n\n\nLe logiciel R, mai\u0302triser le langage, effectuer des analyses (bio)statistiquesLe logiciel R, mai\u0302triser le langage, effectuer des analyses (bio)statistiques (pdf)\n\n\nLe mod\u00e8le lin\u00e9aire g\u00e9n\u00e9ralis\u00e9 (scan), \ngeneralized linear model, logit, binary, odds, inference, test, deviance, coefficient, ols, explanatory power, residuals, multicollinearity, nonlinear, multinomial, probability, ordinal\n\n\nLearning Data Mining with R, Develop key skills and techniques with R to create and customize data mining algorithms, Packt, 2015\n\n\nLearning R for Geospatial Analysis, Leverage the power of R to elegantly manage crucial geospatial analysis tasks, Packt, 2014, \ntime series, vector, function, graphical, function, tables, csv, data frames, code, raster, matrix, array, subsetting, overlay, reclassification, points, lines, polygons, properties, geometrical, calculations, reprojection, projection, layers, geometries, joining, modifying, analyzing, merging, cropping, trimming, aggregating, resampling, filtering, clumping, topography, hillshade, slope, time, spatial, combining, rasterizing, conversion, interpolation, knn, idw, kriging, ggplot2, ggmap, plotting\n\n\nLearning RStudio for R Statistical Computing, Learn to effectively perform R development, statistical analysis and reporting with the most popular R IDE, Packt, 2012 \n\n\nMachine Learning with R, Learn how to use R to apply powerful machine learning methods and gain an insight into real-world applications, Packt, 2013\n\n\nMastering RStudio - Develop, Communicate, and Collaborate with R, Harness the power of RStudio to create web applications, R packages, markdown reports and pretty data visualizations, Packt, 2015\n\n\nMastering Social Media Mining with R, Extract valuable data from social media sites and make better business decisions using R, Packt, 2015, \ngraph, text, authentification, web, oauth, visualization, packages, opinion, sentiment, exploring, twitter, api, app, corpus, facebook, network, degree, betweenness, closeness, cluster, communities, trend, influencer, ctr, spam, stories, instagram, public media, hashtag, location, user, follower, user, comment, dataset, travel-related, follow, who, most, more, top, viral, popular, like, about, picture, github, active, metrics, exploratory, graphical, language, matcher, fork, issue, trend, repositories, repository, eda, correlation, regression, segmented, wikipedia, tumblr, quora, google maps, linkedin, blogger, foursquare, yelp\n\n\nMod\u00e9liser des interactions et des non-lin\u00e9arit\u00e9s (scan), \nmoderator, qualitative, dummy, groups, quantitative, multicollinearity, coefficient, nonlinear, change, piecewise, model, power, quadratic, polynomial\n\n\nParallel Computing for Data Science With Examples in R, C++ and CUDA (The R Series), CRC Press, 2015, \nspeed, obstacles, parallel, loop, scheduling, shared-memory, c, gpu, thrust, mapreduce, parallel, sorting, merging, matrix, subset, algebra\n\n\nParallel R, O\nReilly, 2012, \nsnow, cluster, k-means, workers, load, balancing, chunking, vectorizing, redux, function, environment, random number generation, rmpi, executing, queueing, troubleshooting, multicore, parallel, mapreduce, hadoop, rhadoop, rhipe, segue, revoscale, revoconnectr, cloudnumbers\n\n\nR and Data Mining, Examples and Case Studies, CC, 2013, \ncases\n\n\nR Compilation (see below)\n\n\nR for Data Science_ Import, Tidy, Transform, Visualize, and Model Data, O\nReilly, 2017, \nggplots2, workflow, transformation, dplyr, filter, arrange, select, add, group, mutate, summary, script, exploratory, question, variation, missing value, covariation, patterns, models, wrangle, tibbles, data frames, older code, readr, parsing, writing, tidy, spreading, gathering, separating, pull, missing value, case, nontidy, mutating, joins, stringr, string, regular expression, regex, pattern, factor, farcats, social survey, factor order, levels, dates, times, spans, zones, pipe, magrittr, function, conditional, arguments, return, vector, atomic recursive vector, iteration, purrr, loops, map, failure, modelr, visualizing, formulas, families, missing values, model, broom, gapminder, list-columns, markdown, output, document, notebook, presentation, dashboard, interactivity, website, format\n\n\nR for Marketing Research and Analytics, Springer, 2015\n\n\nR Graphics Cookbook, O\nReilly, 2013, \nexploring, bar graph, line, scatter, summarize, distribution, histogram, density, curve, frequency, polygon, boxplot, violin, dot, annotation, text, mathematical expression, segment, arrow, shaded, rectangle, highlighting, error bar, axes, x, y, range, continuous, categorical scaling, position, tick mark, appearance, logarithm, circular graph, date, time, themes, grid, facet, legend, position, order, title, appearance, label, facet, splitting, color, mapping, variable, discrete variable, palette, colorbling, region, correlation, function, subregion, network, heat, map, 3d, dendrogram, q-q, cumulative, mosaic, pie, map, choropleth, background, pdf, svg, output, file, presentation, shape, data frame, adding, deleting, renaming, reordering, subsetting, removing, changing, recoding, transforming, summarizing, converting, ggplot2\n\n\nR in Action, Data Analysis and Graphics with R, 2nd Edition, Manning, 2015\n\n\nR Packages Organize, Test, Document and Share Your Code, O\nReilly, 2015, \nmetadata, object documentation, vignette, testing, namespace, external data, compile, file, component, git, github, checking, releasing\n\n\nR\u00e9gression avec R, Springer, 2011\n\n\nRegression Models, Methods and Applications, Springer, 2013, \nlinear, mixed, nonparametric, additive, generalized, geoadditive, location, beyond mean, scale, shape, quantile, estimation, parameters, hypothesis, choice, variable, model, quality, criteria, diagnosis, weighted least squares, heteroscedasticity, autocorrelation, regularization, ridge, shrinkage, operator, geometric, partial regularization, boosting, componentwise boosting, bayesian, conjugate analysis, spike, slab, binary, maximum likelihood, fit, overdispersion, count, continuous, response, quasi-likelihood, beyesian generalized linear, bayesian inference, mcmc, boosting, categorical, unordered, ordinal, cumulative, sequential, mixed, longitudinal, cluseted, random, intercept, coefficient, slope, notation, conditional, stochastic, lmm, variance-covariance, covariance, glmm, penalized likelihood, empirical bayes estimation, application, nonparametric, polynomial splines, p-splines, smoothing, random walks, kriging, bivariate, spatial smoothing, structured addtive, random effects, boosting, star, case, quantile, matrix algebra, probability, calculus, statistical, inference, normal distribution, likelihood function, bayesian inference\n\n\nSocial Media Mining with R, Deploy cutting-edge sentiment analysis techniques to real-world social media data using R, Packt, 2014, \nbig data, mining, twitter, social media, opinion, sentiment, emotion, polarity, lexicon, naive bayes, response, case\n\n\nStatistical Analysis and Data Display, An Intermediate Course with Examples in R, 2nd Edition, Springer, 2009\n\n\nThe elements of statistical learning, Data Mining, Inference, and Prediction, 2nd Edition, Springer, 2011\n\n\nTiny Handbook of R, A SpringerBriefs in Statistics, Springer, 2011, \ncommand line, script, function, project, data structure, operation, vector, matrix, factor, index, reshape, stack, unstack, wide, long, merge, missing value, mapping, apply, function, writing, tables, graphs, hypothesis, linear, formula, fit, general linear, regression, diagnostics, testing, coefficient, prediction, stepwise, residualizing, anova, comparison\n\n\nWeb Application Development with R using Shiny, Harness the graphical and statistical power of R and rapidly develop interactive user interfaces using the superb Shiny package, Packt, 2013, \napplication, example, widget type, google analytics, reactive, web page, running, html, serer, interface, javascript, jquery, inputs, outputs, ui, interface, reactivity, get, graphics, sharing, running, github, git, zip, tar, glimmer, shiny server, browser\n\n\nR Compilation\n\n\n\n\nA Beginner\ns Guide to R [Zuur, Ieno \n Meesters 2009-07-02]\n\n\nA First Course in Statistical Programming with R [Braun \n Murdoch 2008-01-28]\n\n\nA Handbook of Statistical Analyses using R (2nd ed.) [Everitt \n Hothorn 2009-07-20]\n\n\nA Modern Approach to Regression with R [Sheather 2009-03-11]\n\n\nA Practical Guide to Ecological Modelling Using R as a Simulation Platform [Soetaert \n Herman 2008-11-21]\n\n\nAdaptive Design Theory and Implementation using SAS and R [Chang 2007-06-27]\n\n\nAdvances in Social Science Research Using R - Vinod H. - 2011\n\n\nAn Introduction to Analysis of Financial Data with R [Tsay 2012-10-29]\n\n\nAn Introduction to Bootstrap Methods with Applications to R [Chernick \n LaBudde 2011-11-01]\n\n\nAnalysis of Categorical Data with R [Bilder \n Loughin 2014-08-11]\n\n\nAnalysis of Correlated Data with SAS and R (3rd ed.) [Shoukri \n Chaudhary 2007-05-17]\n\n\nAnalysis of Financial Time Series\n\n\nAnalysis of Integrated and Cointegrated Time Series with R - Pfaff B. - 2008\n\n\nAnalysis of Phylogenetics and Evolution with R (2nd ed.) [Paradis 2011-11-09]\n\n\nAnalyzing Baseball Data with R [Marchi \n Albert 2013-10-29]\n\n\nAnalyzing Linguistic Data - A Practical Introduction to Statistics using R - Baayen R. - 2008\n\n\nAnalyzing Sensory Data with R [L\u00ea \n Worch 2014-10-09]\n\n\nAnalyzing Spatial Models of Choice and Judgment with R [Armstrong, Bakker, Carroll, Hare, Poole \n Rosenthal 2014-02-07]\n\n\nApplied Bayesian Statistics With R and OpenBUGS Examples [Cowles 2013-01-03]\n\n\nApplied Statistical Genetics with R - Foulkes A. - 2009\n\n\nApplied Statistics Using SPSS, STATISTICA, MATLAB and R - Marques J. - 2007\n\n\nBasic R for Finance [W\u00fcrtz, Lam, Ellis \n Chalabi 2010]\n\n\nBayesian Essentials with R (2nd ed.) [Marin \n Robert 2013-10-29]\n\n\nBayesian Networks in R With Applications in Systems Biology [Nagarajan, Scutari \n L\u00e8bre 2013-04-27]\n\n\nBayesian Networks With Examples in R [Scutari \n Denis 2014-06-20]\n\n\nBeginner\u2019s Guide to R - Zuur A. et al. - 2009\n\n\nBeginning Data Science with R [Pathak 2014-12-09]\n\n\nBeginning R - An Introduction to Statistical Programming - Pace L.\n\n\nBeginning R - The Statistical Programming Language - Gardener M. - 2012\n\n\nBehavioral Research Data Analysis with R - Li Y. et al. - 2012\n\n\nBioinformatics and Computational Biology Solutions using R and Bioconductor [Gentleman, Irizarry, Carey, Dudolt \n Huber 2005-08-31]\n\n\nBioinformatics with R Cookbook [Sinha 2014-07-23]\n\n\nBiostatistical Design and Analysis Using R - A Practical Guide - Logan M. - 2010\n\n\nBiostatistical Design and Analysis using R_ A Practical Guide [Logan 2010-05-10]\n\n\nBiostatistics with R An Introduction to Statistics through Biological Data [Shahbaba 2011-12-17]\n\n\nBusiness Analytics for Managers - Jank W. - 2011\n\n\nChemometrics with R Multivariate Data Analysis in the Natural Sciences and Life Sciences [Wehrens 2011-01-31]\n\n\nClinical Trial Data Analysis - Using R - Din Chen, Karl E. Peace - 2010\n\n\nCompeting Risks and Multistate Models with R - Beyersmann J. et al. - 2012\n\n\nComputational Actuarial Science with R [Charpentier 2014-08-26]\n\n\nComputational Finance An Introductory Course with R [Arratia 2014-05-09]\n\n\nComputational Statistics An Introduction to R [Sawitzki 2009-01-26]\n\n\nContingency Table Analysis Methods and Implementation using R [Kateri 2014-06-15]\n\n\nData Analysis and Graphics Using R - An Example-Based Approach, 3e - Maindonald J. et al. - 2010\n\n\nData Analysis using Regression and Multilevel_Hierarchical Models [Gelman \n Hill 2006-12-18]\n\n\nData Manipulation with R (2nd ed.) [Abedin \n Das 2015-03-31]\n\n\nData Manipulation with R - Spector P. - 2008\n\n\nData Mashups in R - Leipzig J. et al. - 2011\n\n\nData Mining with Rattle and R The Art of Excavating Data for Knowledge Discovery [Williams 2011-08-04]\n\n\nData Science in R A Case Studies Approach to Computational Reasoning and Problem Solving [Nolan \n Lang 2015-03-18]\n\n\nData Wrangling with R - Boehmke B. - 2016\n\n\nDoing Bayesian Data Analysis A Tutorial with R and BUGS [Kruschke 2010-11-10]\n\n\nDynamic Documents with R and knitr (2nd ed.) [Xie 2015-07-06]\n\n\nDynamic Linear Models with R - Petris G. et al. - 2009\n\n\nEnvStats An R Package for Environmental Statistics [Millard 2013-10-28]\n\n\nExploratory Multivariate Analysis by Example - Using R - Husson F. et al. - 2011\n\n\nExploring Everyday Things with R and Ruby - Sheong Chang S. - 2012\n\n\nFinancial Risk Modelling and Portfolio Optimization with R [Pfaff 2013-01-22]\n\n\nForest Analytics with R - An Introduction - Robinson A. et al. - 2011\n\n\nFoundational and Applied Statistics for Biologists using R [Aho 2013-12-17]\n\n\nFunctional and Phylogenetic Ecology in R [Swenson 2014-03-27]\n\n\nFunctional Data Analysis with R and MATLAB - Ramsay J. et al. - 2011\n\n\nGetting Started with RStudio - Verzani J. - 2011\n\n\nGraphical Models with R [H\u00f8jsgaard, Edwards \n Lauritzen 2012-02-23]\n\n\nGraphics for Statistics and Data Analysis with R [Keen 2010-04-26]\n\n\nGraphing Data with R An Introduction - 2015\n\n\nGuidebook to R Graphics using Microsoft Windows [Takezawa 2012-03-13]\n\n\nHandbook of Statistical Analyses Using R, 2e - Everitt B. et al. - 2010\n\n\nHands-On Programming with R_ Write Your Own Functions and Simulations [Grolemund 2014-08-02]\n\n\nHidden Markov Models for Time Series_ An Introduction using R [Zucchini \n MacDonald 2009-04-28]\n\n\nInstant R Starter [Teutonico 2013-04-23]\n\n\nInteractive and Dynamic Graphics for Data Analysis - With R and GGobi - Cook D. et al. - 2007\n\n\nIntroduction to Applied Multivariate Analysis with R - Everitt B. et al. -2011\n\n\nIntroduction to Data Analysis with R for Forensic Scientists [Curran 2010-07-30]\n\n\nIntroduction to Image Processing using R Learning by Examples [Frery \n Perciano 2013-01-31]\n\n\nIntroduction to Probability and Statistics Using R - Kerns G. - 2010\n\n\nIntroduction to Probability Simulation and Gibbs Sampling with R - Suess E. et al. - 2010\n\n\nIntroduction to Probability with R [Baclawski 2008-01-24]\n\n\nIntroduction to R for Quantitative Finance [Dar\u00f3czi, Cs\u00f3ka, Tulassay, Puhle, Havran, V\u00e1radi, Berlinger, Michaletzky \n Vidovics-Dancs 2013-11-22]\n\n\nIntroduction to Scientific Programming and Simulation using R (2nd ed.) [Jones, Maillardet \n Robinson 2014-06-12]\n\n\nIntroduction to Statistics through Resampling Methods and R (2nd ed.) [Good 2013-02-11]\n\n\nIntroductory Statistics with R, 2e - Dalgaard P. - 2008\n\n\nIntroductory Time Series with R - Cowpertwait P. et al. - 2009\n\n\nLatent Variable Modeling using R_ A Step-by-Step Guide [Beaujean 2014-05-08]\n\n\nLatent Variable Modeling with R [Finch \n French 2015-07-01]\n\n\nLearning Data Mining with R [Makhabel 2014-12-22]\n\n\nLearning Predictive Analytics with R- Eric Mayor - 2015\n\n\nLearning R A Step-by-Step Function Guide to Data Analysis [Cotton 2013-09-26]\n\n\nLinear Mixed-Effects Models using R_ A Step-by-Step Approach [Galecki \n Burzykowski 2013-02-05]\n\n\nLinear Models with R (2nd ed.) [Faraway 2014-07-01]_\n\n\nMachine Learning with R Cookbook [Chiu 2015-03-31]\n\n\nMachine Learning with R [Lantz 2013-10-25]\n\n\nMaking Your Case Using R for Program Evaluation [Auerbach \n Zeitlin 2015-07-06]\n\n\nMastering Predictive Analytics with R [Forte 2015-06-30]\n\n\nMastering Scientific Computing with R [Gerrard \n Johnson 2015-02-27]\n\n\nMathematical Statistics with Resampling and R - Chihara Laura M., Hesterberg Tim C. - 2011\n\n\nMATLAB Graphics and Data Visualization Cookbook 2012\n\n\nMaximum Likelihood Estimation and Inference_ With Examples in R, SAS and ADMB [Millar 2011-09-19]\n\n\nMixed Models_ Theory and Applications with R (2nd ed.) [Demidenko 2013-08-05]\n\n\nModern Actuarial Risk Theory - Using R - Kaas R. et al. - 2008\n\n\nModern Analysis of Customer Surveys With Applications using R [Kenett \n Salini 2012-01-30]\n\n\nModern Approach to Regression with R - Sheather S. - 2009\n\n\nModern Industrial Statistics_ With Applications in R, MINITAB and JMP (2nd ed.) [Kenett, Zacks \n Amberti 2014-01-28]\n\n\nModern Optimization with R [Cortez 2014-09-07]\n\n\nModern Regression Techniques Using R - Wright D. et al. - 2009\n\n\nModern Statistical Methods for Astronomy_ With R Applications [Feigelson \n Babu 2012-08-27]\n\n\nMorphometrics with R - Claude J. - 2008\n\n\nMultilevel Modeling using R [Finch, Bolin \n Kelley 2014-06-13]\n\n\nMultiple Comparisons using R [Bretz, Hothorn \n Westfall 2010-07-27]\n\n\nMultistate Analysis of Life Histories with R [Willekens 2014-09-12]\n\n\nMultivariate Generalized Linear Mixed Models using R [Berridge \n Crouchley 2011-04-25]\n\n\nMultivariate Methods of Representing Relations in R for Prioritization Purposes [Myers \n Patil 2012-03-24]\n\n\nMultivariate Nonparametric Regression and Visualization_ With R and Applications to Finance [Klemel\u00e4 2014-05-27]\n\n\nMultivariate Time Series Analysis_ With R and Financial Applications [Tsay 2013-12-09]\n\n\nNonlinear Regression with R - Ritz C. et al. - 2008\n\n\nNonparametric Hypothesis Testing_ Rank and Permutation Methods with Applications in R [Bonnini, Corain, Marozzi \n Salmaso 2014-09-15]\n\n\nNonparametric Statistical Methods using R [Kloke \n McKean 2014-10-09]\n\n\nNumerical Ecology with R - Borcard D. et al. - 2011\n\n\nPractical Data Science with R [Zumel \n Mount 2014-04-13]\n\n\nPractical Graph Mining with R [Samatova, Hendrix, Jenkins, Padmanabhan \n Chakraborty 2013-07-15]\n\n\nPractical Guide to Ecological Modelling - Using R as a Simulation Platform - Soetaert K. et al. - 2009\n\n\nPrimer of Ecology with R - Henry M. et al. - 2009\n\n\nPrimer to Analysis of Genomic Data using R [Gondro 2015-05-20]\n\n\nProbability With Applications and R [Dobrow 2013-11-04]\n\n\nProgramming Graphical User Interfaces in R - Lawrence M., Verzani J. - 2012\n\n\nQuantitative Trading with R Understanding Mathematical and Computational Tools from a Quant\ns Perspective [Georgakopoulos 2015-01-06]\n\n\nR Book - Crawley M. - 2007\n\n\nR by Example - Jim Albert, Maria Rizzo - 2012\n\n\nR Companion to Linear Statistical Models - Hay-Jahans C. - 2012\n\n\nR Cookbook - Proven Recipes for Data Analisys, Statistics, and Graphics - Teetor P. - 2011\n\n\nR Data Visualization Cookbook [Gohil 2015-01-29]\n\n\nR for Business Analytics [Ohri 2012-09-14]\n\n\nR for Cloud Computing An Approach for Data Scientists [Ohri 2014-11-15]\n\n\nR for Data Science [Toomey 2014-12-19]\n\n\nR for SAS and SPSS Users, 2e - Muenchen R - 2011\n\n\nR for Stata Users - Muenchen R. et al. - 2011\n\n\nR Graph Essentials [Lillis 2014-09-24]\n\n\nR Graphs Cookbook (2nd ed.) [Abedin \n Mittal 2014-10-20]\n\n\nR Graphs Cookbook - Mittal H. - 2011\n\n\nR Graphs Cookbook [Mittal 2011-01-14]\n\n\nR High Performance Programming [Lim \n Tjhi 2015-01-30]\n\n\nR in a Nutshell_ A Desktop Quick Reference (2nd ed.) [Adler 2012-10-19]\n\n\nR in Action - Kabacoff R. - 2011\n\n\nR in Action_ Data Analysis and Graphics with R [Kabacoff 2011-08-27]\n\n\nR Inferno - Burns P. - 2009\n\n\nR Machine Learning Essentials [Usuelli 2014-11-25]\n\n\nR Object-Oriented Programming [Black 2014-10-23]\n\n\nR Programming for Bioinformatics - Gentelman R. - 2009\n\n\nR Quick Syntax Reference [Tollefson 2014-04-25]\n\n\nR Recipes A Problem-Solution Approach [Pace 2014-12-19]\n\n\nR Statistical Application Development by Example Beginner\ns Guide [Tattar 2013-07-24]\n\n\nR Through Excel - Heiberger R. et al. - 2009\n\n\nReproducible Research with R and RStudio [Gandrud 2013-07-15]\n\n\nSAS and R Data Management, Statistical Analysis, and Graphics (2nd ed.) [Kleinman \n Horton 2014-07-17]\n\n\nSimulation and Inference for Stochastic Differential Equations_ With R Examples [Iacus 2008-05-05]\n\n\nSix Sigma with R - Statistical Engineering for Process Improvement - Cano E. et al. - 2012\n\n\nSocial Media Mining with R [Danneman \n Heimann 2014-03-24]\n\n\nSoftware for Data Analysis - Programming with R - Chambers J. - 2008\n\n\nSolving Differential Equations in R [Soetaert, Cash \n Mazzia 2012-06-07]\n\n\nStated Preference Methods using R [Aizaki, Nakatani \n Sato 2014-08-15]\n\n\nStatistical Analysis of Financial Data in R (2nd ed.) [Carmona 2013-12-14]\n\n\nStatistical Analysis of Network Data with R [Kolaczyk \n Cs\u00e1rdi 2014-05-23]\n\n\nStatistical Analysis of Questionnaires_ A Unified Approach Based on R and Stata [Bartolucci, Bacci \n Gnaldi 2015-08-07]\n\n\nStatistical Analysis with R - Beginner\ns Guide - Quick J. - 2010\n\n\nStatistical Bioinformatics with R - Mathur S. - 2010\n\n\nStatistical Data Analysis Explained - Applied Environmental Statistics with R - Reimann C. et al. - 2008\n\n\nStatistical Methods for Environmental Epidemiology with R_ A Case Study in Air Pollution and Health [Peng \n Dominici 2008-07-25]\n\n\nStatistical Tools for Nonlinear Regression_ A Practical Guide with S-PLUS and R Examples (2nd ed.) [Huet, Bouvier, Poursat \n Jolivet 2003-09-12]\n\n\nStatistics An Introduction using R (2nd ed.) [Crawley 2014-11-24]\n\n\nStatistics and Data Analysis for Financial Engineering With R Examples (2nd ed.) [Ruppert \n Matteson 2015-04-22]\n\n\nStatistics and Data Analysis for Microarrays using R and Bioconductor (2nd ed.) [Draghici 2016-06-15]\n\n\nStatistics and Data with R - An Applied Approach Through Examples - Cohen Y. et al.- 2008\n\n\nStatistics for Censored Environmental Data using Minitab and R (2nd ed.) [Helsel 2012-02-01]\n\n\nStatistics for Linguistics with R A Practical Introduction (2nd ed.) [Gries 2013-03-15]\n\n\nStatistics Using R with Biological Examples - Seefeld K. et al. - 2007\n\n\nText Analysis with R for Students of Literature [Jockers 2014-06-11]\n\n\nThe Essential R Reference [Gardener 2012-11-19]\n\n\nThe R Book (2nd ed.) [Crawley 2012-12-26]\n\n\nThe R Primer [Ekstr\u00f8m 2011-08-29]\n\n\nThe R Software_ Fundamentals of Programming and Statistical Analysis [de Micheaux, Drouilhet \n Liquet 2014-02-28]\n\n\nTime Series - Application to Finance with R and S-Plus, 2e - Chan N. - 2010\n\n\nTime Series Analysis - With Applications in R, 2e - Cryer J. et al. - 2008\n\n\nTime Series Analysis and Its Applications - With R Examples, 3e - Shumway R. et al. - 2010\n\n\nUnderstanding Statistics using R [Schumacker \n Tomek 2013-01-24]\n\n\nUsing R and RStudio for Data Management, Statistical Analysis, and Graphics (2nd ed.) [Horton \n Kleinman 2015-03-17]\n\n\nUsing R for Data Management, Statistical Analysis, and Graphics - Horton N. et al. - 2011\n\n\nUsing R for Numerical Analysis in Science and Engineering [Bloomfield 2014-04-24]\n\n\nUsing R for Statistics [Stowell 2014-06-24]\n\n\nWavelet Methods in Statistics with R - Nason G. - 2008\n\n\nXML and Web Technologies for Data Sciences with R [Nolan \n Lang 2014-01-27]\n\n\n\n\nRegex\n\n\nregular expressions, text mining\n\n\nCheat sheets\n\n\nBreaking the Ice with Regular Expressions (completed course, videos, notes, snapshots, manual), Code School, 2015, \nregex\n\n\nLes expressions r\u00e9guli\u00e8res par l\nexemple, HK, 2005, \nJavascript, Python, POSIX\n\n\nMastering Python Regular Expressions, Leverage regular expressions in Python even for the most complex features, Packt, 2014\n\n\nRegular Expressions Cookbook 2nd Edition, O\nReilly, 2012\n\n\nSQL\n\n\nBusiness Intelligence avec SQL Server, Maitrisez les concept et r\u00e9alisez un syst\u00e8me d\u00e9cisionnel, eni, 2014\n\n\nCheat sheets\n\n\nSoup to Bits, SQL (completed course, manual, 2 videos, notes), Code School, 2015, \numl, databases, udacity, github, article, mysql, install, cli, command, bash, create, table, data, entry, password, log in, case, weather, data, insurance, csv, gui, workbench, wamp, postgresql, install, cli, command, bash, gui, pgadmin, pgadmin iii\n\n\nSQL Analyzing Business Metrics (completed course, snapshots, notes), Codecademy, 2015\n\n\nLearn Java (completed course, notes, snapshots, codes, manual), Codecademy, 2015, \nmysql\n\n\nLearn SQL (completed course, snapshots, notes, codes, manual), Codecademy, 2015\n\n\nSQL Table Transformation (completed course, snapshots, notes), Codecademy, 2015\n\n\nLinuxCBT PostgreSQL Edition (videos, 17h), LinuxCBT, 2013\n\n\nPostgreSQL, Getting Started (videos, 2h), PluralSight, 2015\n\n\nPython Programming with PostgreSQL (videos, 5.5h)\n\n\nSQL Pocket Guide, O\nReilly, 2011\n\n\nSQL Server 2014, SQL, Transact SQL, Conception et realisation d\nune base de donn\u00e9es, avec exercices pratiques et corrig\u00e9s, eni, 2014\n\n\nApprendre SQL avec MySQL, Avec 40 exercices corrig\u00e9s, Eyrolles, 2006\n\n\nIntroduction to SQL JOINs (notes)\n\n\nLe langage MDX pour les requ\u00eates de cubes OLAP (notes)\n\n\nLearning PostgreSQL, Create, develop, and manage relational databases in real-world applications using PostgreSQL, Packt, 2015\n\n\nMDX tutorial, introduction to Multidimensional Expressions (videos), \ncube, olap, vba, excel, pivot, table, powerpivot, powerview, powerquery, powermap, business, analytics, big data, bi\n\n\nMySQL in a Nutshell, 2nd Edition, O\nReilly, 2008\n\n\nPostgreSQL Cookbook, Over 90 hands-on recipes to effectively manage, administer, and design solutions using PostgreSQL, Packt, 2015\n\n\nPostgreSQL for Data Architects, Discover how to design, develop, and maintain your database application effectively with PostgreSQL, Packt, 2015 \n\n\nProgrammer avec MySQL, SQL, transactions, PHP, Java, Optimisation, Avec 40 exercices corrig\u00e9s, 4e \u00e9dition, Eyrolles, 2015\n\n\nStorytelling, Visualization\n\n\nStorytelling with data, Wiley, 2015\n\n\nThe Wall Street Journal Guide to Information Graphics, The does and don\nts of presenting data, facts, and figures, W.W. Norton \n Company, 2013\n\n\nVisualize This, The FlowingData Guide to Design, Visualization, and Statistics, Wiley, 2011\n\n\nData Visualization with Python and JavaScript, O\nReilly, 2016\n\n\nExcel Sparklines (notes)\n\n\nInteractive Data Visualization for the Web, O\nReilly, 2013 \n\n\nInteractive Data Visualization, Foundations, Techniques, and Applications, 2nd Edition, CRC Press, 2015\n\n\nStorytelling with data (notes)\n\n\nThe Grammar of Graphics, 2nd Edition, Springer, 2005\n\n\nWhich chart or graph is right, Tableau (article), 2012\n\n\nTableau\n\n\nsoftware\n\n\nCases\n\n\nData Analysis Fundamentals with Tableau (videos, 5h), PluralSight, 2013\n\n\nEnterprise Business Intelligence with Tableau Server (videos, 1h30), PlusralSight, 2013\n\n\nCommunicating Data with Tableau.pdf\n\n\nGetting started (manual)\n\n\nLearning Tableau.pdf\n\n\nWhich chart or graph is right, Tableau (article), 2012\n\n\nTeamTreeHouse\n\n\nsoftware, programming, coding, script\n\n\nLearning Tracks:\n\n\n\n\nTTH Business\n\n\nTTH CSS\n\n\nTTH Databases\n\n\nTTH Development Tools\n\n\nTTH HTML\n\n\nTTH JavaScript\n\n\nTTH Python\n\n\nTTH Starting a Business Track\n\n\nTTH Web Design Track\n\n\n\n\nCourse topics:\n\n\n\n\nApp in Heroku\n\n\nBusiness, Google Analytics\n\n\nBusiness, SEO\n\n\nBusiness, build a company\n\n\nBusiness, freelance\n\n\nBusiness, market a business\n\n\nBusiness, social Media Strategy\n\n\nBusiness, soft skills\n\n\nBusiness, start a business\n\n\nBusiness, user needs\n\n\nBusiness, write a business plan\n\n\nCSS\n\n\nConsole\n\n\nD3.js\n\n\nDjango\n\n\nDynamic Site, Node.js\n\n\nFlask\n\n\nGeolocation\n\n\nGit\n\n\nGitHub\n\n\nHTML\n\n\nJavaScript\n\n\nMongoDB\n\n\nPython, databases\n\n\nPython, dates, times\n\n\nPython, decorators\n\n\nPython, image\n\n\nPython, setup\n\n\nPython, testing\n\n\nPython, write better code\n\n\nRegex\n\n\nScrum\n\n\nWebsite\n\n\n\n\nSetting Up a Local Python Environment (notes)\n\n\nTeamTreeHouse Course List (notes)\n\n\nUdemy\n\n\nPotential courses:\n\n\n\n\nA-B Testing\n\n\nApplied Multivariate Analysis with R\n\n\nCoding\n\n\nCustomer Choice Modeling\n\n\nData Analysis and Visualization\n\n\nData Science\n\n\nMachine Learning\n\n\nMongoDB\n\n\nMySQL\n\n\nPostgreSQL\n\n\nPython\n\n\nand many more\n\n\n\n\nUdemy courses (notes), \npython, mysql, postgresql, mongodb, tableau, r, text mining, machine learning, javascript, nlp, psychology\n\n\nvim\n\n\nCheat sheets\n\n\nMiscellanous\n\n\nvba, macros, excel, word, linux, r, packages, scrum\n\n\nLe livre du z\u00e9ro (s\u00e9rie: C#, C, Java, iPhone, C++, Word)\n\n\nAlgorithms in a Nutshell, 2nd Edition, O\nReilly, 2010\n\n\nAlgorithms in a Nutshell, O\nReilly, 2009\n\n\nLa chance et les jeux de hasard (notes)\n\n\nMacros et langage VBA (notes)\n\n\nWindev20 (notes)\n\n\nWord, habillage de figure (notes)", 
            "title": "Collection"
        }, 
        {
            "location": "/collection/#employment-education", 
            "text": "Articles (notes)  Cartes de visite (notes, examples)  Infographies (notes)  Resume templates (notes)  The Infographic Resume, How to Create a Visual Portfolio that Showcases Your Skills and Lands the Job (+ notes), McGraw-Hill Education, 2014,  workforce, technology, paper, morphing, job boards, databases, referrals, mismatch, gig, masses, linkedIn, power to the people, online portfolio, assets, explaining, pitching, social medias, work, visual flair, graphic designer, data visualization, shareability, success story, design, profile, checklist, interactive, slideshows, talent, checklists  A Game Plan for Success in Data Analytics (pdf, epub), Fru Nde,  hiring, demand, jobs, postings, industry, companies, roles, specialization, salaries, growth, data analysis, responsibilities, skill sets, tools, training, job description, excel, pivot tables, presentations, business intelligence, sql, warehouse, multidimensional, cube, programming,  programs, lifecycle, predictive analytics, changes  Cracking the Coding Interview, 150 Programming Interview Questions and Solutions, 4th Edition, CareerCup, 2010,  interview, behind the scenes, while stories, before, resume, behavioural, preparation, questions, mistakes, frequently asked questions, data structure, algorithms, programming languages, problem solving, knowledge  Getting a Big Data Job For Dummies, 1st Edition, 2015,  career path, public sector, academia, commercial organizations, corporate information technology, marketing departments, business units, big data firms, consulting companies, Groves, predictions, skills, attitudes, self-assessment, gaps, education, experience, milestones, timeline, risk, compliance, financial services, healthcare, government, retail, business analyst, data scientist, software developer, foundation, books, online tutorials, on-the-job training, goals, inventory, knowledge, tools, sql, nosql, hadoop, pig, hive, spark, business analytics, business intelligence, visualization, sentiment analysis, machine learning, consultant, specialist, startup, job landing, resume, interview, story, behaviour, motivation  Peter Days World Of Business, In Business Recruiting By Algorithm (podcast), 2016,  algorithms, online assessment, recruiting, business, interview, computer, filter, ranking, tracking, bias, workforce, blind recruiting, pool, diversity, the voice, blind audition, personality, questionnaires, psychology, sift, sieve, filter, keywords, saber software, dating websites, successful relationship, assessing, interview, expand, candidates, recruitment, robot, dating, core behaviours, templates, looking, similar, traits, compare, drivers, in modeling applicants, successful employees, training, models  Poste de Data Scientist (article),  resume  Technical Recruiter Cheat Sheet  Udacity, Ultimate Skill Checklist For Your First Data Analyst Job, Udacity, 2015,  skills, competencies, talent, data analyst, programming, statistics, experimental design, mathematics, machine learning, data wrangling, communication, visualization, intuition, learning  Data Analyst Nanodegree, Udacity,  statistics, databases, data wrangling, exploration, identification, fraud, machine learning, visualization, web, a/b testing  Machine Learning Engineer Nanodegree, Udacity,  exploration, prediction, identification, segmentation, training, learning, artificial intelligence, machine learning  Tech Entrepreneur Nanodegree, Udacity,  Product design, monetization, interactive prototype, marketing, distribution, plan, storytelling, pitch, submission  University, Microprograms (notes)", 
            "title": "Employment, Education"
        }, 
        {
            "location": "/collection/#industry", 
            "text": "Applied Business Analytics: Integrating Business Process, Big Data, and Advanced Analytics, Pearson, 2015,  business, analytics, financial management, customer management, hr management, internal operations, value, governance, quality, methodology, process, forecast, sales force, decision, feedback, identify opportunities, patterns, causality, events, workflows, customers, crm, competencies, ecosystem, exploratory, data preparation, cube  Big Data, Tirer parti des donn\u00e9es massives pour d\u00e9velopper l entreprise, Wiley, 2013,  walmart, transformation, perspectives, activity optimisation, data monetization, metrics, medias, target users, life cycle analysis, business intelligence, not why but what, decision-making, client management, value, visualization, satisfaction, segmentation, merchandising, user experience, ux, profitability, priority, client behaviour, efficiency, fraud, network, hadoop, mapreduce, hive, hbase, pig, etl  Enjeux et usages du big data : Technologies, m\u00e9thodes et mise en oeuvre, Hermes, 2013,  deluge, it systems, data volume, open data, velocity, variety, finance, banks, telecoms, utilities, business, public services, health, transports, distribution, sales, wholesales, medical, education, building, leisure, value, gains, data mining, methodology, machine learning, automated learning, statistical learning, social networks, a/b testing, crownsourcing, geomarketing, time series, visualization, competencies, hr, data scientists, qualities, it, profile, resume, manage a project, methodology  Infographies (notes)  Internet, ce qui nous \u00e9chappe, Yves Michel, 2015,  internet, how it works, changes, changes, black oil, moore s law, connected objects, disruptive innovation, silicon valley, platforms, economic war, bytes, society, behaviour, backlash, repositories, data centers  La R\u00e9volution Big Data, Dunod, 2015,  social networks, mobile, cloud, social business, volume, velocity, variety, repositories, hadoop, real time visualization, privacy, personal data, governance, business models, changes, marketing, sales, production, logistics, risks, frauds, hr, telecoms, energy, utilities, banks, public sector, health, open data, cognitive computing  Le Big Data, Que sais-je, 2015,  volume, sequencers, and, genetics, internet of things, connected objects, google flu, prediction, computer structure, architecture, software, inferential, decision, correlation, reason, causality, scientific research, users, crowdsourcing, business models, competencies  Traque Interdite (\u00e9mission web), R-C, 2015,  web personalization, categorization, five personality traits, algorithms, surveillance, massive, target users, demography, password, hacking, piracy, follow, mobile phone, protect, manipulation, bugs, facebook, tracking, data policy, cookies, chrome, firefox, twitter, publicity, instagram, personal data, bubble, anonymity, metadata, myths, video games, smart city, account, access, rights, linkedin, netflix, culture, personality, rick taking, wifi, friends, links, social networks, secure, apps, skynet, state, smartphone, revealing, online  Big Data - A Business and Legal Guide, Wiley, 2014,  executive, security, compliance, information, privacy, Big Data, laws, risk, license, antitrust, insurance, human resources, hr,    Big Data et machine learning, Manuel du data scientist, Dunod, 2015,  data explosion, economic cause, internet giants, web, price decrease, competitive advantage, demanding clients, value, eldorado, ecosystem, needs, cloud, value creation, resulting, velocity, variety, applications, competencies, hadoop, modeling, new tools, impacts, consequences, companies, clients, suppliers, legal, big brother, nosql, large-scale, repositories, data centers, documents, column-oriented, document-oriented, graph-oriented, future, algorithm, mapreduce, statistical analysis, joints, matrices, framework, data scientist, data collection, workflow, deluge, sources, formats, quality, exploratory, statistics, cleaning, enrich, transform, etl, programming, machine learning, understanding, predicting, models, overfitting, supervised, unsupervised, regression, classification, linear, nonlinear, parametric, nonparametric, incremental, geometric, probabilistic, linear regression, k-nn, knn, na\u00efve bayes, logistic regression, k-means, decision trees, random forest, support vector machines, examples, noise, visualization, graphs, complex, interactive, hadoop distributions, hadoop ecosystem  Big Data For Dummies, 2013,  And data management, waves, content management, structured data, unstructured data, distributed computing, parallel computing, redundant physical infrastructure, storage, servers, security, interfaces, databases, services, tools, warehouses, analytics, virtualization, network, processor, memory, storage, cloud, relational, nonrelational database, key-value, document, mongodb, couchdb, columnar, column-bases, hbase, graph, neo4j, special, postgis, opengio suite, polyglot, persistence, mapreduce, hadoop, distributed file system, hdfs, nodes, ecosystem, yarn, hbase, hive, pig, pig latin, sqoop, zookeeper, extraction,  extract, transformation, transform, loading, load, elt, operationalized, business intelligence, analytical, infrastructure, text, taxonomies, voice of the customer, social media, exploratory, codifying, integration, incorporation, traditional, validity, talent, return on investment, roi, roadmap, urgency, capacity, methodology, budget, skill sets, risk, governance, experiment, experimentation, streaming, improving business, needs, customer experience, best action, fraud, sources  Big Data MBA Driving Business Strategies with Data Science, Wiley, 1st Edition, 2015,  business mandate, model maturity, strategy, prioritization, user experience, customer engagement, employees, channels, analyst, characteristics, approaches, models, business, exploratory, algorithms, models, date lake, learning, data science, data scientist, techniques, score, monetization, metamorphosis, creative thinking, creativity  The Patient Revolution, How Big Data and Analytics Are Transforming the Health Care Experience, Wiley, 2015,  startup, experience is a teacher, healthcare, banking, retail, patient empowerment, health cloud, analytics, trends, behavioural, personal health, population, consumer, choices, incentive, adoption, costs, outcomes, providers, diagnoses, machine learning, artificial intelligence, natural language processing, nlp, modernization, workflows,  experiences  Big Data, A Revolution that will Transform who we Live, Work, and Think, 2013,  now, more, messy, correlation, date, value, implications, risks, control, next  Blockchain Revolution: How the Technology Behind Bitcoin Is Changing Money, Business, and the World, Portfolio, 2016,  protocol, ledger, digital age, internet, prosperity, platform, design, principles, integrity, power, incentive, security, privacy, rights, inclusion, financial services, banks, banking, reputation, credit score, prediction, governance, accounting, business model, apps, agent, autonomous, open, networked, hacking, people, computing, internet of things, disruption, smart, smart phone, smart things, smart cities, prosperity, remittances, humanitarian aid, ownership, state, government, empowerment, democracy, voting, politics, justice, engaging citizens, crowd sourcing, streaming, connecting, education, energy, paradigm, NASCAR operation, job killer, big brother, criminals, ecosystem, cautionary tale, regulation, framework, agenda, trust  La R\u00e9volution Big data, Les donn\u00e9es au coeur de la transformation de l entreprise, Dunod, 2014  Who s tracking you in public (article),  facial recognition, department stores, casinos, cruise ships, churches, crowd, facebook, face Prince, 3d modeling, surveillance, database, privacy, public", 
            "title": "Industry"
        }, 
        {
            "location": "/collection/#bash-powershell", 
            "text": "cli, command line, interface, directory, bash, shell, powershell, file, filename, terminal, folders, print, search, uppercase, lowercase, occurrence, text, file names, line, lines, sort, reverse, random, randomly, number, numbers, size, characters, programs, inputs, outputs, input, output, pause, time, touch, create, execute, modify, view, delete, run, process, ps, pstree, killall, graphic, load, list, top, bottom, end, kill, pid, sudo, superuser, do, group, user, permission, permissions, right, write, chain, pipe, archive, compress, decompress, zip, unzip, install, update, upgrade, configure, bc, arbitrary precision calculator language, format-control, date, time, copy, move, retrieve, find, locate, which, find within, grep, variable, conditional, if, else, loops, for, while, logs, import, csv, text, array, gui, email, error, reporting, html, database, mailbox, extract, formatting, jobs, debug, breakpoints  Cheat sheets  Learn the Command Line (completed course, notes, snapshots, codes, manual, exercises), Codecademy, 2015  Command Line Crash Course (completed course, notes, snapshots, codes, manual, exercises), Learn Python the Hard Way, 2015  The Linux Command Line (completed course, notes, snapshots, codes, manual, exercises), No Starch Press, living book!  Les commandes Fondamentales De Linux, CC, 2016  Linux Shortcuts and Commands (notes),   i/o, pipes, filter, permissions, job control, script, flow control, shortcuts, sanity, commands, operations, network, file compression, process control, administration commands, program installation, drive, partition, music-related, graphic-related, files, folders, directory  The Administrator Crash Course, Windows PowerShell v2, Realtime, 2010", 
            "title": "Bash, Powershell"
        }, 
        {
            "location": "/collection/#cloud-computing", 
            "text": "How to Become A Data Scientist Using Azure Machine Learning (course, video)  Spark Big Data Cluster Computing in Production, Wiley, 2016,  cloud, text, sequence, avro, parquet, cluster, standalone, yarn, mesos, tuning, partitioning, shuffling, serialization, cache, memory, shared, locality, security, acl, network, encryption, fault, execution  Cloud computing, s\u00e9curit\u00e9, gouvernance du SI hybride et panorama du march\u00e9, Dunod, 2016  Open Source Cloud Computing for Complete Beginners, Building and Scaling High-Performance Web Servers on the Amazon Cloud (tutorial)", 
            "title": "Cloud Computing"
        }, 
        {
            "location": "/collection/#data", 
            "text": "Data Source Handbook, O Reilly, 2011,  open data, public data, repositories, api, whois, bit.ly, compete, github, whitepages, facebook, twitter, boss, wikipedia, simplegeo, us census, crunchbase, zoominfo, maxming, books, films, music, products  Open Data, consommation, traitement, analyse et visualisation de la donn\u00e9e publique, eni, 2016,  open data, consumption, processing, analysis, visualization, public data", 
            "title": "Data"
        }, 
        {
            "location": "/collection/#datacamp", 
            "text": "Introduction to R (Beta) (completed course, slides),  variable, workspace, comments, scripts, data type, logical, numeric, character, double, complex, raw, coercion, coerce, as., create name, vectors, calculus, element, sum, subset, matrix, rbind, cbind, bind, rownames, colnames, colsums, rowsums, scalar, factor, categorical, order, levels, nominal, ordinal, order, list, extend, data frame, data.frame, sort, add column, row, sorting, graphics, plot numerical, plot categorical, hist, bins, barplot, boxplot, pairs, par, parameters, line type, lty, plot symbol, pch, multiple plots, mfrow, mfcol, grid, layout, stack  Intermediate R (completed course, slides, snapshots),  relational operators, greater, lower, equal, logical operators, and , or, not, conditional, if, else, else if, loops, while, infinite, break, for, list, function, print, function documentation, na.rm, built-in functions, arguments, args, create functions, optional argument, return, packages, install, load, search, apply, lapply, sapply, vapply, identical, abs, absolute, round, sum, mean, sequence, seq, repeat, rep, sort, string, str, is., as., append, reverse, rev, regular expressions, grepl, grep, sub, gsub, subset, time, date, sys.date, sys.time, posixct, lubridate, zoo, xts, time series packages  Importing Data into R (completed course, snapshots),  flat files, excel files, spss files, sas files, stata files, postgresql database, mysql database, sqlite database, web, utils, read.table, header, separator, stringasfactors,  read.csv, comma, dot, , semi-colon, tab, delimited, states_eu.csv, readr, data.table, read_delim, col_names, col_types, skip, n_max, n_min, fread, readxl, excel_sheets, gdata, xlsx, xlconnect, loadworkbook, getsheets, readworksheet, createsheet, writeworksheet, saveworkbook, haven, foreign, read_sas, read_stata, read_dta,  as_factor, read_spss, read.dta, convert.factors, read.spss, rmysql, dbconnect, dblisttables, dbreadtable, dbgetquery, dbfetch, dbdisconnect, rpostgresql, web, path, download.file, url, authentication, dest_path, httr, api, json, twitter, jsonlite, array, nesting, tojson, prettify, minify  Cleaning Data in R (completed course, snapshots),  explore, structure, class, dim, names, str, glimpse, summary, visualize, head, tail, print, hist, plot, tidy, dirty, wide datasets, long datasets, tidyr, key, value, gather column, spread column, separate, unite, convert, conversion, as., lubridate, date, tmd, mdy, hms, ymd_hms, string manipulation, stringr, str_trim, trim, str_pad, str_detect, str_replace, replace, tolower, toupper, lowercase, uppercase, missing, na, not available, empty, dot, infinite value, inf, not a number, nan, outlier, errors, extreme, unexpected  Writing Functions in R (completed course),  define, return value, object, scope, scoping, lookup, data structures, atomic, list, missing very, not available, na, subset, less code, clearer, snippet, temporary, name, naming, argument, order, good function, functional programming, domain, variable, common code, functions as arguments, map, map_dbl, map_lgl, map_int, map_chr, failure, safely, possibly, quietly, map2, iterate, pmap, invoke_map, mapping, map2_dbl, map2_lgl, pmap_dbl, walk, pipe, pipeline, robust, errors, hidden argument, getting, setting options  Data Manipulation in R with dplyr (completed course, snapshots),  pipe, pipeline, tbl, select, filter, arrange, mutate, summarize, summarise, tidyr, tidy, starts_with, ends_with, contains, matches, num_range, one_of, is.na, !is.na, min, sum, sd, max, mean, var, length, median, iqr, first, last, nth, n, n_distinct, % %, magrittr, group_by, data frame, data table, database, tbl_dt, tbl  Data Analysis in R, the data.table Way (completed course, snapshots, cheat sheet),  data table, dt, select row, i, column, j, compute, by, group, subset, chain, chaining, :=, set, setnames, setcolorder, index, indexing, key, mult, nomatch, two-column key, join, setkey  Data Visualization in R with ggvis (completed course, snapshots),  points, bars, maps, histograms, scatter, density, data, coordinate system, mark, properties, html, javascript, js, THE% %, magrittr, pipe, pipeline, :=, sets, maps, =, stroke, width, opacity, dash, fill, lines, paths, ribbons, smooths, prediction, model_predictions, later, layer_smooths,  dplyr, layer_paths, layer_lines, group_by, mapping, chart, charting, interactivity, multilayered, axes, axis, legends  Data Visualization with ggplot2 (1) (completed course, snapshots, videos),  graphical data analysis, design, communication, exploratory, explanatory, anscombe plot, fit, fitting, grammar of graphics, layers, data, aesthetics, statistics, geometries, geom_, facets, coordinates, themes, base plot, points, tidy, color, size, shape, attribute, alpha, line type, label, shape, axes, positions, identity, dodge, stack, fill, jitter, jitterdodge, scale, scale functions, limit, breaks, expand, labs, abline, area, bar, bin2d, blank, boxplot, contour, crossbar, density, density2d, dotplot, errorbar, errorbar, freqpoly, hex, histogram, hline, line, linerange, map, path, pointrange, polygon, quantile, raster, rect, vline, ribbon, rug, segment, smooth, step, text, tile, violin, eas, geom, pch, crosshairs, remarks, time series, linetype, size, qplot  Data Visualization with ggplot2 (2) (completed course, snapshots, videos),  statistics, coordinates, facets, themes, best practices, function, geom, geom_, stat_, fill,  bin, histogram, bar, freqpoly, smooth, boxplot, bindot, bin2d, binhex, contour, quantile, sum, boxplot, dotplot, bin2d, hex, contour, quantile, count, summary, confidence interval, qq, quantile-quantile, coord_, coord_cartesian, scale_x_continuous, xlim, coord_cartesian, aspect ratio, facets, edward tufte, tidy, theme layer, element_, text, line, rectangle inheritance, blank, recycling, save theme, reuse theme, discrete x-axis, derivative theme, built-in theme templates, ggthemes, theme update, theme_set, pitfalls, dynamic plot, error bar, pointrange, pie charts, stacked bar chart, haircol, horizontal, heat maps  Reporting with R Markdown (completed course, snapshots, videos),  reproducible, research, knitr, pandoc, shiny,  interactive, web-based, rstudio, html, css, pdf, word, beamer, slidy, ioslides, markdown::render, rmd, latex  Introduction to Machine Learning (completed course, snapshots, notes),  regression, shopping basket analysis, recommendation systems, decision-making, classification, clustering, cluster, k-means, supervised, unsupervised, model performance, error, accuracy, competition time, interpretability, limits, confusion matrix, precision, recall, true, false, real mean squared error, cluster similarity, between cluster sum of squares, inter-cluster distance, dunn s index, training, train, testing, test, predictive power, split the dataset, cross validation, n-fold, number of validation, bias, variance, quadratic data, overfitting, underfitting, decision tree, numerical, categorical, classify, choose, split, sticking criteria, information gain, pruning, k-nearest neighbours, k-nn, distance, euclidean, scaling, dummy, roc curve, receiver operator characteristic curve, probability, simple regression, linear regression, r-squared, multiple linear regression, adjusted, predictors, assumptions, non-parametric, kernel regression, regression trees, generalized regression, within cluster sums of squares, between cluster sums of squares, k, scree plot, choosing, intercluster, hierarchical clustering, dendrogram, pros and cons  Intro to Statistics with R (completed course, snapshots, videos):   Course One Introduction,  types of variables, nominal, ordinal, interval, ratio, histogram, distribution, bimodal, skewed, skewness, uniform, platykurtic, leptokurtic, scales, z-score, percentile rank, measure of central tendency, average, median, mode, variability, variance, standard deviation  Course Two Student s T-test,  z-test, t-test,  single sample, dependent, independent, cohen, cohen s d, confidence interval, t-value, upper bound, lower bound  Course Three Analysis of Variance (ANOVA),  anova, analysis of variance, continuous, independent t-test, dependent t-test, between groups, repeated measures, f-test, f-ratio, post-hoc, tukey s procedure, tukey, factorial anova, two independent variables, one dependent variable, main effect, interaction effect, simple effect, effect size, homogeneity of variance, normal distribution  Course Four Repeated Measures (ANOVA),  statistical power, order effects, counterbalancing, missing data, extra assumption, homogeneity of variance, homogeneity of covariance, sphericity, systematic, between, unsystematic, within, subjects, f-test, mean-squared, ms, post-hoc test, holm  Course Five Correlation and Regression,  correlation, r, sum of cross products, sp, covariance, variance, magnitude, causation, causality, sampling, sample, measurement, regression, r-squared, estimation, coefficients, assumptions, normal, linear, homoscedasticity, anscombe s quartet, anscombe, scatterplot, residuals  Course Six Multiple Regression,  regression, simple, multiple, predictors, r-squared, dummy, dummies, matrix algebra, data frame, variance, covariance, standard deviation, correlation, estimation, coefficients, unweighted coding, weighted coding  Course Seven Moderation and Mediation,  experimental manipulation, moderator, moderation, enhance, test for moderation, centering predictors, avoid multicollinearity, constant, mediation, partial, full, sobel test   Big Data Analysis with Revolution R Enterprise (completed course, snapshots, videos),  challenge, move, merge, manage, munge, large datasets, inspect, transform, create available, summarize, xdf, parallel, visualize, histogram, correlation, subset, linear model, nonlinear relationship, categorical, revoscaler, revolution, distributed environments, cluster, information, dayofweek, summary, quantile, crosstabs, facets, weights, scatterplot, log, data frame, import, min, max, apply, means, drop, predict, machine learning, split, train, test, split, cross validation, standard errors, covariance, logit, logistic regression, generalized least square, cluster, clustering, decision trees, regression tree, revotree, roc curve, classification tree, confusion matrix  DataCamp, Data Analysis and Statistical Inference (completed course, snapshots, videos, notes, manuals, codes, datasets),  import, plot, inspect, web, load, table, barplot, mosaicplot, mosaic, summarize, structure, nrow, head, tail, str, summary, boxplot, names, descriptive statistics, histogram, sample, simulation, distribution, population, sampling distribution, for loop, sample size, confidence intervals, standard error, dotplot of samples, inferential statistics, bootstrap, interference, confidence level, bootstrap method, parameter of interest, relationships between two variables, by, anova, continuous, categorical, margin of error, proportion, regression, correlation, plot, moneyball, abline, linearity, normal residuals, constant variability, qqnorm, qqline, boxplot, mosaicplot, kitter, adding, removing, residuals, openintro statistics manual  Data Exploration With Kaggle Scripts (completed course, snapshots, videos, codes, datasets),  portfolio, github, cases, map, mapping, spanish production of silver, chopsticks effectiveness, pigeon data, kaggle account, phd earnings, american community survey  DrivenData Water Pumps Challenge (case),  data mining, water table, train, test, ggplots, visualization, map, well, location, random forest  Exploring Polling Data in R (case),  visualization, pools, election  Having Fun with googleVis (case),  visualization, gapminder, interactive, graphs  How to work with Quandl in R (completed course, snapshots),  data connection, api, python, excel, web, r, database aggregator, library, stocks, finance, economics, census, public services, government, world organizations  Intro to Computational Finance with R (completed course, snapshots, notes, codes, datasets, manuals),  tseries, stock returns, zoo, performanceanalytics, econometrics, risk analysis, financial data, returns, plot, chart, matrix, histogram, boxplot, plot, qqnorm, return distribution, descriptive statistics, args, apply, annualized monthly estimates, graphical analysis, covariance matrix, mvtnorm, simulate, set.seed, sigma, joint probability, rho, probability, expected return model, global minimum variance portfolio, efficient portfolio, efficiency frontier, tangency portfolio, calculate returns, standard error of the variances, hypothesis test of mean, of the correlation, normality, asset returns, class, portfolio theory, t-bills, sharpe slope, the global minimum variance portfolio, efficiency portfolio, quantiles, densities, normal curve, value-at-risk, continuously compounded monthly returns, simple total returns, dividend yields, annual returns, portfolio shares, portfolio returns, price data, index, indices, subset, continuously compounded 1-month returns, monthly compounding, xts, time series, moving average, ma, autoregressive, ar  Kaggle R Tutorial on Machine Learning (completed course, snapshot, codes),  titanic, prediction, decision trees, interpret, predict, summit, overfitting, reengineering, survival rates, pruning, random forest  Plotly Tutorial Plotly and R (case)  R for the Intimidated (introductory course, videos)  R, Yelp and the Search for Good Indian Food (case),  data wrangling, web scraping, filter, sieve, sift, data mining, dplyr  Assessing Tank Production (case),  monte carlo, bootstrap  Credit Risk Modeling in R (completed course, IPython notebook, slides, datasets),  expected loss, probability, default, exposure, loss, crosstable, loan, mortgage, gmodel, interest, grade, ownership, annual income, age, histogram, outlier, missing, confusion matrix, model prediction, classification accuracy, sensitivity, specificity, logistic regression, logit, generalized linear model, glm, table, logarithmic, predicting, prediction, discriminative, train, training, test, testing, probabilistic regression, probit, log-log, loglog, cutoff, decision tree, gini-measure, gini, rpart, sample, dataset, set, glass matrix, pruning, plotcp, printcp, complexity parameter, cp, prune, plotcp, prp, bad rate, fixed acceptance rate, strategy curve, table, bank, roc, comparison, proc, area under curve, auc, model reduction, discriminant analysis, random forest, neural networks, support vector machine, survival analysis  Exploring Pitch Data with R (completed course, IPython notebook, slides, notes, images, manuals, datasets),  velocity, graphical skills, distribution, fastball, hitting, outcomes, game, date, subset, histogram, abline, vertical line, ifelse, tapply, plot, time series, overlap, jitter, multimodal, bimodal, mix, change, table, prop.table, ball-strike cout, pitch usage, expectancy, paste, concatenate, pitch location, strike zone, locational variable, horizontal, vertical, binning, grid, loop, plot, visual interpretation, bat, batting, batted, contact rate, ggplot2, wide, long, locgrid, layer  Introduction to Python   Machine Learning (completed course, IPython notebook, slides, notes, images, manuals, datasets)  Kaggle Python Tutorial on Machine Learning (completed course, IPython notebook, slides, notes, images, manuals, datasets)  Python 3 (including Importing Data, completed courses, IPython notebook, slides, notes, datesets),  data types, float, int, str, bool, list, subset, slice, change, add, remove, function, type, round, method, built-in, max, len, capitalize, replace, bit_length, index, count, index, append, package, library, pip, python3, numpy, array, pip3, 3d array, ndarray, mean, median, corrcoef, std, round, column_stack, print, concatenation, inline, offline, matplotlib, pyplot, plot, show, scatter, hist, xlabel, ylabel, title, yticks, xticks, dictionary, dictionaries, keys, pandas, tabular, table, data frame, dataframe, index, read_csv, select, bracket, row, column, label-based, loc, integer position-based, iloc, comparison, greater, lower, equal, true, false, boolean, and, or, not, conditional, if, else, elif, filter, compare, loop, while, for, enumerate, in, list, dictionary, array, 2d array, my_dict.items, nditer(my_array), iterrows, apply, random, rand, seed, randint, random walk, range, tails, append, recfromcsv, xscale, clean frame, clf, transpose, flat file, csv, txt, comma, tab, delimiters, semi-colon, numpy, np, vector, array, import, essential for scikit-learn, loadtxt, genfromtxt, datatypes, pandas, data frame, dataframe, scipy, excel, matlab, sas, stata, hdf5, pickled, serialize, bytestream, spreadsheet, pickle, pickle.load, pd.excelfile, parse, sas7bdat, file.to_data_frame, pd.read_stata, h5py.file, data.keys, key, keys, value, values, scipy.io, sci.io.loadmat, scipy.io.savemat, .mat, relational database, postgresql, mysql, sqlite, sql, sqlalchemy, create_engine, engine.connect, con.execute, engine_table_name, query, execute, pd.dataframe, fetch, fetchall, close, rs.keys, engine.connect, join, read_sql_query, web, urllib, urlopen, urlretrieve, request, read, read_csv, close,  pd.read_excel, requests, get, scrape, scraping, beautifulsoup, prettify, find_all, api, json, tweepy, authentification, oauthhandler, os, getcsd, listdir  Statistical Thinking in Python (Part 1) (completed course, IPython notebook, slides, notes, datasets)  Text Mining, Bag of Words (completed course, IPython notebook, slides, notes, datasets),  workflow, problem definition, specific goals, identify text, text organization, feature extraction, analysis, insight, recommendation, bag of words, semantic parsing, Documents, corpus, corpora, vcorpus, read.csv, cleaning, preprocessing, tolower, removepunctuation, removenumbers, stripwhitespace, removewords, word stemming, stemdocument, stem_words, stem_completion, turn document matrix, tdm, document time matrix, dtm, word frequency matrix, wfm, tm, rowsums, sort, colsums, term_frequency, qdap, freq_terms, word cloud, stop word, tm_map, function, commonality clouds, commonality.cloud, union, paste, vectorsource, vcorpus, as.matrix, comparison cloud, pyramid plot, pyramid.plot, subset, abs, cbind, data.frame, rownames, word network, word clustering,  hclust, dendrogram, unigram, bifram, trigram, ngram, term weight, single word count, tfldf, penalize word, metadata, readtabular, hr analytics  Tidy Data in Python (completed courses, IPython notebook, datasets)  Time Series in R The Power of xts and zoo (completed courses, IPython notebook, datasets),  extensible time, xts, zoo, matrix, date, seq, core, coredata, convert, as.xts, as.matrix, import, export, iso8601, date, time, intraday, interval, extraction, first, last, coredata, as.numeric, merge, index, inner join, outer, left, right, rbind, cbind, na.locf, na.approx, lag, difference, diff, endpoints, apply, lapply, split, do.call, convert, to.period, to.quarterly, rolling value, time zone, timezones, indexclass, indextz, indexformat, time, periodicity, to.yearly, nmonths, nquarters, nyears, timestamps, indexwday, unique  Visualization with Bokeh (completed courses, IPython notebook, datasets)", 
            "title": "DataCamp"
        }, 
        {
            "location": "/collection/#econometrics-spatial-gis", 
            "text": "A Primer for Spatial Econometrics with Applications in R, Palgrave, 2014  Applied Econometrics, Palgrave, 2011  Econometrics by Example, Palgrave Macmillan, 2014  \u00c9conom\u00e9trie non param\u00e9trique, Economica, 2008  Economics and History: Surveys in Cliometrics, Wiley-Blackwell, 2011  Introduction \u00e0 l \u00e9conom\u00e9trie, Puf, 2009  Mastering Metrics, Princeton, 2015  The Esri Guide to GIS Analysis, Volume 2: Spatial Measurements and Statistics, Esri Press, 2005  Visualizing Data Patterns, CRC Press, 2010,  map, visualization, graph, geographical, geointelligence, interactive  An Introduction to R for Spatial Analysis and Mapping, SAGE, 2015  Analyzing Financial Data and Implementing Financial Models using R, Springer, 2015  Applied Econometrics with R, Springer, 2008  Handbook of Applied Spatial Analysis, Springer, 2010  Introduction to Spatial Econometrics, CRC Press, 2009  Perspectives on Spatial Data Analysis, Springer, 2010  QGIS-2.14-PyQGISDeveloperCookbook-fr (manual)  QGIS-2.14-QGISTrainingManual-fr (manual)  QGIS-2.14-UserGuide-fr (manual)  Spatial Analysis, Statistics, Visualization, and Computational Methods, CRC Press, 2016  Spatial Statistics   Geostatistics, SAGE, 2013  Using Econometrics A Practical Guide, 6th edition, Pearson, 2014", 
            "title": "Econometrics, Spatial, GIS"
        }, 
        {
            "location": "/collection/#git", 
            "text": "Cheat sheets  Learn Git (completed course, notes, snapshots, manual), Codecademy, 2015  Git maitrisez la gestion de vos versions, eni, 2016  Pragmatic Guide to Git, 1st edition, Pragmatic Bookshelf, 2010  Pragmatic Version Control Using Git, Pragmatic Bookshelf, 2009  Pro Git, 2nd ed", 
            "title": "Git"
        }, 
        {
            "location": "/collection/#hadoop", 
            "text": "hadoop, vmware, conception, development, developer, training, skills, administration, big data, apps, building, projects  Big Data Concept et mise en oeuvre de Hadoop, eni, 2014  Cheatsheets  Projects in Hadoop and Big Data, Learn by Building Apps, Udemy, 2015  Udemy potential courses:   Become a certified hadoop developer training tutorial  introduction to big data hadoop map-reduce  Master apache hadoop infinite skills hadoop training  Master big data and hadoop administration  Master apache hadoop, infinite skills hadoop training  Master big data and hadoop administration  Master big data and Hadoop step-by-step from scratch  Projects in hadoop and big data, learn by building apps  and many more", 
            "title": "Hadoop"
        }, 
        {
            "location": "/collection/#html-css", 
            "text": "Cheat sheets  Deploy a Website (completed course, notes, snapshots, codes, manual), Codecademy, 2015,  static website generator  HTML   CSS Final Project (uncompleted course, notes, snapshots, codes, manual), Codecademy, 2015  Make a Website (completed course, notes, snapshots, codes, manual) , Codecademy, 2015,  static website generator  Make an Interactive Website (completed course, notes, snapshots, codes, manual) , Codecademy, 2016,  static website generator  Concevez votre site web avec PHP et MySQL, Simple IT, Livre du Z\u00e9ro, 2010,  html, css, dynamic, wamp, mamp, xampp, editor, variable, conditional, loops, function, table, url, form, transmit, transmission, password, session, cookie, read, write, open, close, files, sql, database, record, save, error, insert, update, delete, chat, comment, joint, image, regular expression, mvc architecture, member, script  R\u00e9alisez votre site web avec HTML5 et CSS3, Simple IT, Livre du Z\u00e9ro, 2011,  html, css, static, web page, markup, attributes, comments, paragraph, title, list, links, image, figure, style, class, id, selector, size, police, alignment, floating, italic, bold, underline, color, background, transparency, border, round, shadow, click, select, block, inline, dimension, margin, absolute, relative, design, content, table, form, video, audio, adaptative, responsive, query, dom, javascript, canvas, svg, sockets, dynamic, php, jee, asp, .net, domain, host, heading, footer, debug", 
            "title": "HTML, CSS"
        }, 
        {
            "location": "/collection/#importio", 
            "text": "", 
            "title": "Import.io"
        }, 
        {
            "location": "/collection/#internet-of-things", 
            "text": "wearable, electronics, design, prototype, wear, interactive, garments, hackable, magazine, raspberry pi, arduino, internet of things, connected object, domotic, quantify-self  Arduino, maitrisez sa programmation et ses cartes d interface (shields), 2e \u00e9dition, Dunod, 2014  Arduino 3, Premiers pas en informatique embarqu\u00e9e, 2014  Le guide de la maison et des objets connect\u00e9s, Eyrolles, 2016  Make: Wearable Electronics  Objects connect\u00e9s, la nouvelle r\u00e9volution num\u00e9rique, eni, 2016", 
            "title": "Internet of Things"
        }, 
        {
            "location": "/collection/#javascript", 
            "text": "Cheat sheets  JavaScript (completed course, notes, snapshots, codes, manual), Codecademy, 2016,  static website generator  JavaScript API (uncompleted course, notes, snapshots, codes, manual) , Codecademy, 2016,  api, soundcloud, youtube  JavaScript Final Project (uncompleted course, notes, snapshots, manual), Codecademy, 2016,  static website generator  D3.js in Action, Manning, 2015,  visualization, interactive, online, server-side, back-end, web, website, web page  Data Visualization with JavaScript, No Starch Press, 2015  Data Visualization with Python and JavaScript, O Reilly, 2016  JavaScript, Dunod, 2010  Web Development with Node and Express, O Reilly, 2014", 
            "title": "JavaScript"
        }, 
        {
            "location": "/collection/#latex-markdown", 
            "text": "mathematics, markdown, r, text, formulas, tex, packages, markup  Cheat sheets  LaTeX pour l impatient, 3e \u00e9dition, MiniMax, 2009  LaTeX appliqu\u00e9 aux sciences humaines, Atramenta, 2012  Pandoc - Demos,  pandoc, cli document converter, pdf, html, markdown,  md, latex, tex, word, doc, txt, rft  R\u00e9digez des documents de qualit\u00e9 avec LaTeX, Simple IT, Livre du Z\u00e9ro, 2010  The Not So Short Introduction to LaTeX, CC, 2015  Tout ce que vous avez toujours voulu savoir sur LaTeX sans jamais oser le demander, CC, 2018", 
            "title": "LaTeX, Markdown"
        }, 
        {
            "location": "/collection/#linux", 
            "text": "Cheat sheets  Linux (notes)  Linux in a Nutshell, 6th Edition, O Reilly, 2009  Linux, base d administration et de programmation (scans),  virtual, virtualization, server, samba, administration, debug, commands  Reprenez le contr\u00f4le \u00e0 l aide de linux, Simple IT, Livre du Z\u00e9ro, 2010  The Linux Command Line, 2nd Edition, CC, 2013", 
            "title": "Linux"
        }, 
        {
            "location": "/collection/#nlp-digital-humanities", 
            "text": "digital humanities, history, text mining, machine learning  Cheat sheets  Statistiques en sciences humaines avec R, PUL, 2013,  quantitative methods, social sciences, quantitative research, hypotheses, database, univariate, seminary, histogram, list, frequency table, text exploration, multivariate, relation, correlation, Association, regression, cohen, time series, residuals, error margin, function, significance, tests, power, inferential, partial correlation, real estate, voting, road death toll, logistic, principal component, correspondence analysis  A century of trends in adult human height (article), eLIFE, 2015,  public health  Computational Folklorists (article), ACM, 2012  Exploring Big Historical Data, Imperial College Press, 2015  NLTK Essentials, Build cool NLP and machine learning applications using NLTK and other Python libraries, Packt, 2015  Text Mining Infrastructure in R (article), Journal of Statistical Software, 2008", 
            "title": "NLP, Digital Humanities"
        }, 
        {
            "location": "/collection/#nosql", 
            "text": "Big Data and Reporting with MongoDB, PluralSight, 2014  Cheat sheets  The Magical Marvels of MongoDB (completed course, videos, notes, snapshots, manual), Code School, 2015,  install, run, launch, commands, cli, shell, format, robomongo, gui, driver, university, course, tutorial, pluralsight  Learn NoSQL Database From Scratch, MongoDB, Udemy, 2014  MongoDB Initiating the Next Step, Udemy, 2016  Bases de donn\u00e9es orient\u00e9es graphes avec Neo4j, Eyrolles, 2016  Les bases de donn\u00e9es NoSQL et le Big Data, 2e \u00e9dition, Eyrolles, 2016  MongoDB Data Modeling, Focus on data usage and better design schemas with the help of MongoDB, Packt, 2015  MongoDB Applied Design Patterns, O Reilly, 2013  NoSQL For Dummies, 2015", 
            "title": "NoSQL"
        }, 
        {
            "location": "/collection/#predictive-modeling-data-science-data-mining-marketing-analytics", 
            "text": "preferences, choices, market basket, economic data analysis, operation management, text analysis, sentiment analysis, sports analytics, spatial data analysis, brands and prices, linear models, regression, na\u00efve bayes, clustering, k-means, optimization, non-linear programming, algorithms, time series, forecasting, smoothing, monte carlo, outliers, data preprocessing, data analysis, exploratory, statistical analysis, statistics, k-nn, decision trees, neural networks, k-means, kohonen, association rules, missing data, charts, models, visualization, data mining, machine learning, words clouds, charts, keywords search, semantic search, svn, markov model, logistic regression, classification, curve fitting, scoring, supervises, unsupervised, cases, classification, distribution, relationship, random forest, pruning, prediction, misclassification, evaluation, errors, generalized linear regression, polynomial regression, piecewise-linear regression, least squares, correlation, distance, k-medians, k-medoids, k-centers, models, average, weighted average, kernel, svr, transformation, attribute, hypothesis, tests, learning, testing, support vector machine, missing data, segmentation, overfitting, fitting, underfitting, neighbours, confusion matrix, roc, Lift, text mining, bag of words, term frequency, topic analysis, co-occurence, redux, mapreduce, hadoop, parallel processing, nosql, sql, random, expectations, descriptive, inferential, ordinary least squares, topology, spatial plot, map, mapping, factor analysis, principal component analysis, correspondence analysis, association analysis, bayesian methods, bootstrapping, scoring, web mining, social media mining, mail mining, email mining, github mining, facebook mining, linkedin mining, twitter mining  SEE: Econometrics, Spatial, GIS; R, Statistics; PYTHON  Analytics for Insurance, The Real Business of Big Data, Wiley, 2016,  risk, management, underwriting, claim, marketing, property, liability, life, pensions, people,   Data Smart Excel: Using Data Science to Transform Information into Insight, Wiley, 2013,  excel  Marketing Analytics, A Practical Guide to Real Marketing Science (excerpts, BANQ), Wiley, 2015,  statistics, consumer, behaviour, strategy, regression, business case, segmentation, elasticity, test, control, lift, collinearity, logistic, market basket analysis, survival, lifetime, value, ltv, descriptive, predictive, simultaneous equations, segment, k0means, lca, rfm, behavioural, missing value, research, conjoint analysis, structural equation, sem, sample, a/b, testing, factorial, engagement  Statistical Methods in Customer Relationship Management (excerpts, BANQ), Wiley, 2012,  with sas, crm, acquisition, retention, churn, win-back, models, vector autoregressive, lifetime, tobit, multinomial, logit, hazard, survival, event, poisson, regression, binomial    Data science, Fondamentaux et \u00e9tudes de cas_R_Py, Machine Learning avec Python et R, Eyrolles, 2015  Data Science for Business, What You Need to Know About Data Mining and Data-Analytic Thinking, O Reilly, 2013  Marketing Analytics, Wiley, 2014,  excel, spreadsheet, pivot table, pricing, bundling, skimming, revenue, forecasting, regression, trend, winter s, moving average, neural networks, conjoint analysis, logistic, discrete choice, lifetime customer value, monte carlo, acquisition, retention, segmentation, cluster, tree, s curve, bass diffusion, copernican, duration, market basket, lift, rfm, scan*pro, retail space, sales, point of sales, pos, advertising, media, pay per click, online, principal compenent, pca, multidimentional scaling, mds, naive bayes, discriminant analysis, anova, internet, network, viral, text, mining  Marketing Data Science, Modeling Techniques in Predictive Analytics with R and Python (epub), Wiley, 2015,  market, consumer choice, customer, retention, acquisition, positioning, promiting, recommending, brands, prices, social networks, competitors, predicting, sales, database, regression, bayesian, data mining, machine learning, data visualization, text, sentiment analysis, time series, market response, sampling, www, social media, surveys, experiments, interviews, focus groups, field search cases  Practical Data Analysis, Transform, model, and visualize your data through hands-on projects, developped in open source tools, Packt, 2013,  python, mlpy, d3.js, mongodb, data source, open data, text files, excel files, sql databases, nosql databases, multimedia, web scraping, data scrubing, csv, json, xml, yaml, openrefine, d3.js, html, dom, css, javascript, svg, text classification, image retrieval, stock prices, predicting prices, modeling epidemiology, social graphs, social networks, facebook, gephi, gdf, json, twitter, oauth, twython, sentiment classification, nltk, mapreduce, ipython, wakari, infrastructure, ubuntu, windows, python3, numpy, scipy, mlpy, openrefine", 
            "title": "Predictive Modeling, Data Science, Data Mining, Marketing Analytics"
        }, 
        {
            "location": "/collection/#programmation-for-kids", 
            "text": "Cahier d activit\u00e9, Scratch pour les kids, Eyrolles, 2015  JavaScript for Kids, No Starch Press, 2014  \u00c0 l aventure avec Arduino !, Eyrolles, 2015  Apprendre \u00e0 coder en Python grace \u00e0 Minecraft, Eyrolles, 2014  BBC micro:bit, 27 Projects for Students, Level 1, 2016  JavaScript For Kids For Dummies 2015  Making Games with Python   Pygame, CC, 2012  MicroPython for ESP8266 Development Workshop, 2016  Python pour les kids, La programmation accessible \u00e0 tous ! D\u00e8s 10 ans, Eyrolles, 2013  Python pour les kids, D\u00e8s 10 ans, Eyrolles, 2016  Super Scratch Programming Adventure, No Starch Press, 2010  The Official ScratchJr Book Help Your Kids Learn to Code, No Starch Press, 2016  The R Student Companion, CRC Press, 2013,  script, function, graphs, input, output, loops, logic, control, mathematic functions, arithmetics, trigonometrics, matrix, system linear equation, graphs, probability, simulation, fitting", 
            "title": "Programmation for Kids"
        }, 
        {
            "location": "/collection/#project-management", 
            "text": "Learning Path, DevOps with Docker (course, video)  DevOps for Digital Leaders, Reignite Business with a Modern DevOps-Enabled Software Factory, 1st Ed, CA Press, 2016  Effective DevOps, O Reilly, 2015  Mastering IT Project Management (PCO et ITIL) (notes),  pco, pmi, pmp, itil, skills, process  Scrum (cheat sheet)", 
            "title": "Project Management"
        }, 
        {
            "location": "/collection/#python", 
            "text": "", 
            "title": "Python"
        }, 
        {
            "location": "/collection/#advanced-specialized", 
            "text": "programming, coding, script  Apprendre la programmation avec Python et Django, Eyrolles, 2012  Data Structures and Algorithms in python, Wiley, 2013  Data Wrangling With Python, Tips and Tools to Make Your Life Easier, O Reilly, 2016  Fluent Python, O Reilly, 2014  FreeCAD, Solid Modeling with the Power of Python, Packt, 2012  Fundamentals of Python From First Programs through Data Structures, CC, 2010  Hacking Secret Ciphers With Python, CC, 2013  Learning Data Mining with Python, Harness the power of Python to analyze, data and create insightful predictive models, Packt, 2015  Learning Python, 5th Edition, O Reilly, 2013  Mastering Pycharm, Use PyCharm with fluid efficiency, Packt, 2015  Matplotlib (manual), 2013  Programmation Python, conception et optimisation, 2e \u00e9dition, Eyrolles, 2009  Python Cookbook, 3rd Edition, O Reilly, 2013  Python Essential Reference, 4th Edition, Addison-Wesley, 2009  Python for Data Analysis, O Reilly, 2012  Python Geospatial Development, Learn to build sophisticated mapping applications from scratch using Python tools for geospatial development, 2nd Edition, Packt, 2013  Python Programming for Hackers and Reverse Engineers, No Starch Press, 2009  Real World Instrumentation with Python, O Reilly, 2010  Web Scraping with Python, O Reilly, 2015", 
            "title": "Advanced, Specialized"
        }, 
        {
            "location": "/collection/#entry-level", 
            "text": "Automate the Boring Stuff with Python, No Starch Press, 2015   Cheat sheets  Python (completed course, notes, snapshots, codes, manual), Codecademy, 2015  Python API (completed course, notes, snapshots, codes, manual), Codecademy, 2016,  api, dwolla, nhtsa, npr, wepay  Python Final Project (uncompleted course, notes, snapshots, manual), Codecademy, 2015,  markov chain, github  Python Projects (notes, codes), Codecademy, 2015,  bank account, battleship, calendar, grading, guess game, converter, game  Data Science and Machine Learning with Python (course, videos)  Data Science and Machine Learning with Python (videos, 9h), Udemy, 2015  Intro to Python for Data Science (completed course, notes, snapshots, manual), DataCamp, 2015,  python3  Learn Python the Hard Way (completed course, videos, notes, snapshots, codes, manual)  Managing Your Biological Data with Python (notes, script, images, files), CRC Press, 2014  Understanding Machine Learning with Python (course, videos), 2016  LPTHW Python +  Python Glossary  A Comprehensive Introduction to Python Programming and GUI Design Using Tkinter (article)  Amazon Web Services in Action, Manning, 2016  An introduction to GUI programming with Tkinter (presentation), SciNet, 2014  An Introduction to Tkinter (presentation), CC, 2008  Apprenez \u00e0 programmer en Python, Simple IT, Livre du Z\u00e9ro, 2011  Bayesian Methods for Hackers, Addison-Wesley, 2016,  bayesian, inference, pymc, python, a/b testing, cases, distribution, algorithms, plot, graphs, statistics, probability, loss function, machine learning  Building Machine Learning Systems with Python, Master the art of machine learning with Python and build effective machine learning systems with this intensive hands-on guide, Packt, 2013  Building Machine Learning Systems with Python, Get more from your data through creating practical machine learning systems with Python, 2nd Edition, Packt, 2015,  iris, classifying, scikit-learn, clustering, bag of words, k-means, topic modelling, data mining, knn, logistic, regression, sentiment analysis, naive bayes, classifier, tweets, clean, word,  cross-validation, penalize, regulatize, lasso, elasticnet, text, predictions, recommendations, basket analysis, classification, music, computer vision, dimension reduction, big data, amazon web service  Data Science from Scratch First Principles with Python, O Reilly, 2015,  visualization, linear algebra, statistics, probability, hypothesis, inference, gradient descent, fetch, read, file, scraping, web, api, twitter, exploring, machine learning, k-nearest neighbours, knn, naive bayes, regression, logistic, decision tree, neural networks, clustering, natural language processing, network analysis, recommender system, databases, sql, mapreduce  Data Wrangling with Python Tips and Tools to Make Your Life Easier, O Reilly, 2016,  excel, spreadsheet, pdf, storing, clean, format, outilier, bad, duplicates, match, regex, standardizing, scripting, exploration, presenting, reporting, web, scraping, screen, spiders, api, automation, scaling    Flask By Example, Unleash the full potential of the Flask web framework by creating simple yet powerful web applications, Packt, 2016  Flask Framework Cookbook, Over 80 hands-on recipes to help you create small-to-large web applications using Flask, Packt, 2014  Flask Web Development, Developing Web Applications with Python, O Reilly, 2014  Introducing Data Science, Big Data, Machine Learning and more, using Python tools, Manning, 2016,  big data, project management, machine learning, large data, steps, nosql, graph databases, text mining, text analytics, data visualization, user  Introduction to Computation and Programming Using Python (theory), Revised, MIT Press, 2013,  testing, debugging, exceptions, assertions, classes, oop, algorithmic complexity, algorithms, plotting, stochastic, programs, probability, statistics, random walks, data visualization, monte carlo, experimental data, dynamic programming, machine learning  Introduction to Machine Learning with Python, O Reilly, 2017,  scikit-learn, jupyter, ipython, knn, regression naive bayes, decision trees, ensembles, support vector machines, neural networks, decision function, train, test, scale, pca, principal component, non-negative matrix factorization, nmf, manifold learning with t-sne, clustering, k-means, binning, trees, polynomials, nonlinear, regression, models, forms, statistics, cross-validation, grid search, metrics, scoring, preprocessing, pipeline, text, strings, bag of words, ngram, topic modeling, token, document, stemming, lemmatization  Introduction to Python for Econometrics, Statistics and Data Analysis, CC, 2014  IPython Interactive Computing and Visualization Cookbook, Over 100 hands-on recipes to sharpen your skills in high-performance numerical computing and data science with Python, Packt, 2014  IPython Notebook Essentials, Compute scientific data and execute code interactively with NumPy and SciPy, Packt, 2014  Large Scale Machine Learning with Python, Learn to build powerful machine learning models quickly and deploy large-scale predictive applications, Packt, 2016,  scalability, ipython, scikit-learn, streaming, stochastic, learning, support vector machines, nonlinearity, hyperparameter, neural networks, deep learning, h2o, tensorflow, neural networks, keras, tree, classification, cart, boosting, xgboost, random forest, pca, principal component analysis, k-means, lda, cpu, memory, machine, distributed, hadoop, spark, virtual box, hdfs, yarn, spark, pyspark, machine learning, cluster, preprocessing,   Learning IPython for Interactive Computing and Data Visualization, Learn IPython for interactive Python programming, high-performance numerical computing, and data visualization, Packt, 2013  Machine Learning in Python, Essential Techniques for Predictive Analysis, Wiley, 2015,  machine learning, classification, visualization, predictive, balancing performance, big data, linear regression, penalize, predictive model, logistic, binary, multinomial, emsembles, tree, bootstrap, gradient boosting, random forest, bagging, binary classification  Mastering Machine Learning with scikit-learn, Apply effective learning algorithms to real-world problems using scikit-learn, Packt, 2014,  linear regression, polynomial, regularizatino, gradient descent, machine learning, scikit-learn, extraction, preprocessing, text, images, logistic regression, binary classifier, tree, clustering, k-means, pca, principal component analysis, perceptron, support vector machines, neural networks,   Mining the Social Web Data Mining Facebook, Twitter, LinkedIn, Google+, GitHub, O Reilly, 2013  Programmation en Python pour les math\u00e9matiques, 2e \u00e9dition, Dunod, 2011  Programming ArcGIS 10.1 with Python Cookbook, Over 75 recipes to help you automate geoprocessing tasks, create solutions, and solve problems for ArcGIS with Python, Packt, 2013,  geoprocessing script, ArcPy, map, documents, layers, referencing, data frame, layers, tables, symbology, finding, fixing, broken data, find, replace, automating, production, printing, exporting, map book, geoprocessing, retrieving, tool, creating tools, querying, data, table, listing, gis, describing, customizing, interface, add-ins, error handling, troubleshooting, python, script    Python 3 niveau avanc\u00e9, CC, 2016  Python Data Visualization Cookbook, Over 70 recipes, based on the principal concepts of data visualization, to get you started with popular Python libraries, 2nd edition, Packt, 2015,  importing, exporting, reading, cleaning, generating random, smoothing noise, plotting, customizing plot, 3d, visualiztion, charts, images, maps, capchat, logarithm, spectogram, vector flow, colormaps, cross correlation, autocorrelation, matplotlib, drawing, latex, api, plotly, cloud  Python For Data Analysis, O Reilly, 2013,  ipython, numpy, pandas, loading, storage, formats, data, wrangling, cleaning, transforming, merging, reshaping, plotting, visualizing, visulization, aggregation, operations, time series, financial, economic, data, app, manipulation, advanced, sorting, matrix, broadcasting  Python for Data Science For Dummies, 2015  Python for Education, Learning Maths   Science using Python and writing them in LATEX, CC, 2010  Python Machine Learning, Unlock deeper insights into machine learning with this vital guide to cutting-edge predictive analytics, Packt, 2015,  algorithms, training, classification, neural networks, classifier, scikit-learn, logistic,  regression, support vector machines, kernel, nonlinear problems, tree, knn, missing data, categorical data, train, test, regularization, dimensionality reduction, discriminant analysis, nonlinear mapping, hyperparameter, tuning, cross-validation, pipelines, debugging, metrics, emsembles, bagging, bootstrap, sentiment analysis, bag of words, online algorithm, sqlite, flash, into web app, flask, web application, displaying, predicting, ols, regularize, polynomial,  clustering, k-means, tree, neural networks, handwritten, backpropagation, gradient, parallelizing  Python Pocket Reference, 5th Edition, O Reilly, 2014  Python Real World Machine Learning, Learn to solve challenging data science problems by building powerful machine learning models using Python, Packt, 2016,  preprocessing, label, endoding, linear, regression, model persitence, ridge, polynomial, price, demand, classifier, regression, logistic, naive bayes, train, test, cross-validation, confusion matrix, performance, curves, support vector machines, hyperparameters, predictor, clustering, k-means, vector, cluster, algorithm, recommendation, pipeline, euclidean distance, pearson distance, movie recommendation, stemming, lemmatization, chunking, bad of words, text, gender, sentiment, topic modeling, speech recognition, audio, frequency, music, markov model, speech recognizer, time series, conditional random fields, images, detecting, biometric, face, recognition, deep neural networks, visualizing, pca, principal component analysis, k-means, deep learning, bolzmann, denoising autoendoders, convolutional, cnn, text, engineering, cleaning, tagging, nltk, correlation, lasso, recursive feature elimination, genetic model, restful, api, twitter, ensembles, random forests, xgboost, stacking, lasagne, tensorflow, scalability, python, scikit-learn, h2o, gensim, theano, sknn, theanets, keras, streaming, stochastic, deep learning, cart, boosting, gradient boosting, virtual box, hadoop, hdfs, yarn, spark  Python Testing with unittest, nose, pytest, Leanpub, 2014  Real Python, An introduction to Python through practical examples, CC, 2013  scikit-learn Cookbook, Over 50 recipes to incorporate scikit-learn into every step of the data science pipeline, from feature extraction to model building and model evaluation, Packt, 2014,  workflow, source, sample, categorical, label, missing value, pipelines, pca, principal component analysis, factor analysis, nonlinear dimensionality, truncated, support machines, gradient descent, fitting, linear model, regression, ridge, sparsity, regularize, lars, classification, logistic, bayesian, boosting, k-means, centroids, cluster, knn, tree, random forests, support vector machines, multiclass, classification, lda, qda, stochastic, naive bayes, propagation, cross-validation, grid search, dummy estimator  Scipy, Lectures Notes, CC, living book  Learn Web Scraping With Python In A Day, Acodemy, 2015", 
            "title": "Entry-level"
        }, 
        {
            "location": "/collection/#r-statistics", 
            "text": "", 
            "title": "R, Statistics"
        }, 
        {
            "location": "/collection/#advanced-specialized_1", 
            "text": "A User s Guide to Network Analysis in R, 1st edition, Springer, 2015  An Introduction to R for Spatial Analysis and Mapping, SAGE, 2015  Basic Data Analysis for Time Series with R, Wiley, 2014  Biostatistique : une approche intuitive (BANQ), De Boeck, 2013,  tests, hypothesis, p-value, proportion confidence interval, survival analysis, censored, poisson, error, bias, continuous, categorical, spread, gauss, log-normal, geometric, significance, tests, type i, type ii, equivalence, comparison, group, outlier, khi-squared, propective, experimental, relative risk, survey, two mean, groups, correlation, regression, model, nonlinear, logistic, anova, nonparametric, sensitivity, roc, bayes, sample, sampling, cases  Comprendre et r\u00e9aliser les tests statistiques \u00e0 l aide de R : manuel de biostatistique (BANQ), De Boeck, 2014,  r, descriptive statistics, random variable, data table, scientific methodology, survey, outliers, tests, probability distribution, binomial, multinomial, pascal, negative binomial, geometric, hypergeometric, poisson, laplace-gauss, normal, exponential, gamma, chi-squared, fisher-snedecor, student, mann-whitney, wilcoxon, hypothesis, type i, type ii, bias, error, proportion comparison, conformity, homogeneity, g, mantel-haenszel, mac nemar, mean comparison, welch, anova, median comparison, mann-whitney-wilcoxon, kruskal-wallis, variance comparison, ansari-bradley, bartlett, fligner-killeen, correlation comparison, pearson, spearman, kendall, distribution comparison, kolmogorov-smirnov, shapiro-wilk, regression, survival, covariance, central limit  Learning Bayesian Models with R, Become an expert in Bayesian machine learning methods using R and apply them to solve real-world Big Data problems, Packt, 2015   Making your Case, Using R for Program Evaluation (BANQ, c7, c8, c8, ssdanalysis.com), Oxford University Press, 2015,  test, variance, var.test, t, t.test, cohen.d, dchange, aov, wilcox, wilcox.test, kruskal, kruskal.test, mcnemar, mcnemar.test, wald, wald.test, nonconstant variance, ncvtest, log-lin, logistic regression", 
            "title": "Advanced, Specialized"
        }, 
        {
            "location": "/collection/#entry-level_1", 
            "text": "Cheat sheets  Data Mining Applications with R, Elsevier, 2013,  power grid, hadoop, property value, web, random forest, data mining, classification, images, crime analyses, football mining, internet dns, traffic, optimization, bayesian classifiers, text mining, topic modeling, social network,  recommender systems, response modeling, direct marketing, insurance customer profile, predicting bank loan default, customer preference analysis  Discovering Knowledge in Data, An Introduction to Data Mining, 2nd edition, Wiley, 2014,  larose  Doing Bayesian Data Analysis, 2nd edition, AP, 2014,  credibility, model, parameters, bayesian, r, probability, bayes s rule, inferring, binomial, probability, markov chain, monte carlo, jags, hierarchival model, model comparison, hypothesis, testing, sample size, stan, generalized linear model, metric, predicted, predictor, dichotomous, qualitative, nominal, ordinal, count, tools  Guidebook to R Graphics Using Microsoft Windows, Wiley, 2012  Introducing Survival and Event History Analysis, SAGE, 2011,  survival, event, history, r, data, exploration, descriptive statistics, data structures, nonparametric, kaplan-meier estimator,  \nIntroduction to Stochastic Process with R, Wiley, 2016,  markov chain, stationary, periodicity, ergodic, time, absorbing, branching, probability, extinction, monte carlo, gibbs sampler, sampler, eigenvalue, card shuffling, poisson process, arrival, interarrival, thinning, superposition, uniform distribution, spatial poisson, nonhomogeneous, parting, continuous-time, brownian motion, gaussian process, transformation, properties, variations, applications, ito integral, discrete random variable, joint distribution, continuous random variable, common probability distributions, moment-generating functino, matrix algebra  Introduction to Stochastic Process with R, Wiley, 2016,  markov chain, branching, monte carlo, poisson process, continuous-time markov chain, brownian motion, stochastic calculus, probability, algebra  Learning RStudio (notes)  Modeling Techniques in Predictive Analytics, Business Problems and Solutions with R, Pearson, 2014  Mod\u00e9lisation pr\u00e9dictive et apprentissage statistique avec R (Tuff\u00e9ry, aper\u00e7u), Technip, 2015  Modern Multivariate Statistical Techniques, Regression, Classification, and Manifold Learning (extracts), Springer, 2008  Monte Carlo Simulation and Resampling Methods for Social Science with R, SAGE, 2013,  probability, random number generation, statistical stimulation, linear model, generalized linear model, simulation, testing theory, resampling  Multivariate generalized linear mixed models using R, CRC Press, 2011,  generalized linear models, continuous, interval, scale, logistic, logit, probit, likelihood, binary data, ordinal data, ordered logit, ordered categories, poisson regression models, count data, mixed models, continuous, interval, scale data, two-level logistic, general two-level logistic, intraclass correlation, two-level ordered logit model, likelihood, tow-level poisson model, three-level, duration, event history, left censoring, right censoring, time-varying, competing risk, single-level, two-level, three-level, renewal models, stayer, non-susceptible, endpoint, mover-stayer, handling initial condition, state dependence, random effect, initial contition problems, condition analysis, woordridge conditinal model, joint analysis, random effect, link function, dummy, sabrer, r  Nonlinear Parameter Optimization Using R Tools, Wiley, 2014,  optimization, algorithms, one-parameter, root-finding, minimization, optimize, nonlinear least squares, nonlinear equations, function, calculation, derivatives, bound, constraints, masks, scaling, centering, mathematical programming, global optimization, reparameterization, solution, kkt, tuning, terminating, neos, nlopt, bugs, differential equation, miscellaneous, generalized nonlinear model, systems of equations, noisy objective functino, moving forward  R Graphics, 2nd edition, CRC Press, 2011  Spatial and Spatio-temporal Bayesian Models with R-INLA, Wiley, 2015,  bayesian methods, computing, regression, hierarchical, nonlinear regression, random walk, household and income, generalized linear model, cd4 counts, aids, cancer, mortality, spatial modeling, gmrf, mapping, mym, suicide, disease, zero-inflated model, geostatistical model, spde, spatial prediction, spatio-temporal, bivariate model, joint model, gaussian distribution, non-gaussian distribution, semicontinuous model, rainfall, spatio-temporal dynamic model, besag, space-time model  Spatial Regression Model, SAGE, 2011,  interaction, social science, spatial dependence, map, spatial association, correlation, proximity, spatial model, spatially lagged dependent variable, maximum likelihood, equilibrium effect, spatial dependence, italy, different weights matrices, ols, dummy variable, spatial error, maximum likelihood, democracy, development, dyadic trade flow, connectivities, inference  Statistical Hypothesis Testing with SAS and R, Wiley, 2014,  normal distribution, tests, mean, t-test, z-test, variance, sample, f-test, binomial distribution, proportion, sample, k-sample, poisson distribution, exponential distribution, correlation, nonparametric, location, wilcoxon, krustal-wallis, scale difference, siegel-tukey, ansari-bradley, goodness-of-fit, edf, kolmogorov-smirnov, anderson-darling, cramer-von mises, shapiro-wilk, jarque-bera, distribution, randomness, wald-wolfowitz, run, von neumann, von newmann rank, contingency, fisher, pearson, likelihood, cohen, kappa, large sample, outliers, grubb, david-hartley-pearson, regression, slope, intercept, anova, bartlett, levene  Statistiques et probabilit\u00e9s appliqu\u00e9es, puf, 2008,  descriptive, frequency, centre, average, median, pourcentile, variance, distribution, range, absolute, parameters, probability, events, conditional, independence, random variable, discrete, binomial, continuous, normal, gaussian, test, poisson distribution, exponential distribution, function, sampling, confidence interval, hypothesis, difference, regression  Text Mining and Visualization, Case Studies Using Open-Source ToolsCRC Press (RapidMiner, Knime, Python, R), 2016,  rapidminer, text analytics, corpus, token, repository, mining, visualization, documents, rank-frequency, sequential window, zipf-mandelbrot, knime, preprocessing, frequencies, transformation, data table, social media, network mining, slashdot, python, mongodb, sparse matrix, character encoding, web scraping, cleansing, visualization, exploration, classification, clustering, pca, principal compenent analysis, sentiment, mining search, logs, r  An Introduction to R (pdf)  An Introduction to R for Quantitative Economics, Springer, 2015,  rstudio, crude oil price, supply, demand, fish, function, derivative, elasticity, linear, log-log, cobb,douglas, matrix, statistics, regression, simulation, normal, uniform, binomial, central limit theorem, t-test, logit, anscombe, graphs, scatter, growth, time series, time, random walks, cycles, stochastic, difference, air passengers, inflation, phillips curve, stock market  An Introduction to Statistical Learning with Applications in R, Springer, 2013  Analysis of Questionnaire Data with R, CRC Press, 2012,  description, response, summary statistics, plotting, graphs, relationships, variable, risk, odds, ratio, correlation, hierarchical, clustering, hypothesis, confidence interval, proportion, mean, percentage, means, correlation, groups, linear, logistic, poisson, regression, binary, multinomial, categorical, count, multilevel, predictors, interaction, missing data, bootstrap, random effects, multilevel, composite, distribution, multi-trait, multi-method, error, alpha, structural equation, factor analysis, memory card, univariate, bivariate, multidimensional, inference  Analyzing Baseball Data with R, CRC Press, 2014,  database, retrosheet, game, paly, pitch, graphics, run, win, expectancy, balls, strikes, career, simulation, streak, bat, mysql, package, mlbam, data  Art of R Programming - A Tour of Statistical Software Design, No Starch Press, 2011,  vector, matrix, array, data frame, programming, math, simulation, oop, input, output, graphics, debugging, speed, memory, code, language, parallel  Automated Data Collection with R A Practical Guide to Web Scraping and Text Mining, Wiley, 2015,  web scraping, text mining, html, primer, syntax, tag, attribute, xml, json, parsing, xpath, web document, extracting, http, protocol, libcurl, rcurl, ajax, javascript, dom, chrome, tools, sql, relational, database, dbms, regular expression, regex, string, stringr, toolbox, web, retrieval, api, oauth, robots, statistical text processing, tm, corpus, ngram, cleansing, support vector machines, random forest, maximum entropy, rtexttools, press release, processing, loop, plyr, progress feedback, error, exception, case studies, bills, network, ftp, server, twitter, rest, streaming, visualizing, mining, tweet, geographic distribution, data collection, website inspection, mapping, automating, gathering, storage, sentiment, collecting, analyzing  Beginning SQL Server R Services, Analytics for Data Scientists, 1st Ed, Apress, 2016,  setup, installation, visual studio, project, scenario, scope, building model, sample, package, plotting, regression, case, wind speed by airport, average temperature by airport, configuring, connecting, reporting, account, url, database, portal, email, encryption, subscription, scaling, bi, report, imporring, generating, cases  Big Data Analytics with R and Hadoop, Set up an integrated infrastructure of R and Hadoop to turn your data analytics into Big Data analytics, Packt, 2013,  installing, linux, ubuntu, single node, multinode, hdfs, mapreduce, project, subproject, writing, programmes, scenario, shuffling, sorting, executing, limitations, solving, dataflow, examples, rhadoop, rhipe, streaming, analytics, preprocessing, visualizing, web page categorization, frequency of stock change, sale price of books, machine learning, regression, clustering, recommendation, algorithms, importing, exporting, package, mysql, rmysql, excel, mongodb, sqlite, postgresql, hive, hbase  Building a Recommendation System with R, Learn the art of building robust and powerful recommendation engines using R, Packt, 2015,  data analysis, processing, simlar, euclidian, distance, cosine, pearson, dimensionality, reduction, principal component analysis, data mining, clustering, k-means, tree, ensembles, bagging, random forest, boosting, system, recommenderlab, datasets, matrix, exploration, preparation, item-based, flitering, user-based, evaluating, case, building, engine  Computational Biology A Practical Introduction to BioData, A Practical Introduction to BioData Processing and Analysis with Linux, MySQL, and R, 2nd edition, Springer, 2013,  linux, shell, sed, awk, perl, mysql, r, operating system, windows, max os x, vmware, files, directories, moving, copying, renaming, attributes, web, remote, wget, curl, scp, ssh, rsync, text, cat, sorting, lines, scrolling, character, word, line, splitting, cut, paste, grep, finding, sparse, pico, vim, dos, pipe, alias, batch, job, command, wildcard, blast, clustalw, ftp, script, path, variables, input, output, echo, read, substitution, quoting, flow control, conditional, logic, notification, debugging, regular expression, sed, pattern, space, substitution, transliteration, deletion, insertion, change, printing, reading, writing, mysql, relational database, example, case  Computational Finance with R, An Introductory Course with R, Springer, 2014,  securities, bonds, stocks, options, derivatives, portfolios, investments, engineering, trading, price, discounted cash flow, arbitrage, risk, efficient, market, computational, time series, returns, distributions, density, stationary, autocovariance, forecasting, volatility, correlations, causalities, similarities, rank granger, nonparametric, grouping, clustering, graphs, models, trend, seasonality, arch, garch, nonlinear, semiparametric, neural networksds, support vector machines, tests, brownian motion, binomial tree, monte carlo, continuous, time process, wiener, ito, lemma, geometric, option pricing, black-scholes, trade, mining, value estimation, technical analysis, fundamental analysis, optimization, heuristic, combinatorial, annealing, genetic programming, ant colony, portfolio optimization, mean-variance, online finance, price search, online trading, portfolio selection     Data Mining Algorithms Explained using R, Wiley, 2015  Data mining and business analytics with R, Wiley, 2013,  regression, polynomial, nonparametric, model selection, parsimony, false discovery, lasso, logistic regression, binary classification, knn, naive bayes, multinomial logistic regression, discriminant analysis, fisher s, tree, chaid, baggin, boosting, random forests, support vector machines, neural networks, rattle, data mining, clustering, k-means, hierarchical clustering, market basket analysis, association, lift, dimension reduction, factorial, factor, principal component analysis, pca, multicollinear, partial least squares, text, sentiment, network  Data Mining and Statistics for Decision Making, Wiley (Tuff\u00e9ry), 2008  Data Mining with R - Learning with Case Studies, Torgo L., CRC Press, 2011,  visualization, summarization, unknown value, filling, missing value, prediction model, regression, tree, evaluation, stock market returns, time-dependent, reading files, csv, web, mysql, windows, linux, prediction, predictors, models, neural networks, support vector machines, splines, simulation, monte carlo, experimental comparison, trading, testing, fraudulent, transactions, dataset, data mining, precision, recall, lift, outlier, box lot, outlier, clustering, class imbalance, naive bayes, adaboost, classifying, anova, random forest, clustering, prediction, evaluation, knn  Displaying Time Series, Spatial, and Space-Time Data with R, CRC Press, 2014,  time series, horizontal axis, meteorological, scale, stacked, graphs, grouping, scatter, polylines, colors, positioning, panel, siar, unemployment, gross national income, co2, emission, spatial data, package, mapping, choropleth, raster, vector, physical map, openstreemap, googlemap, air quality, election, lang cover, space-time, spatiotemporal, level, exploratory, animation  Event History Analysis with R, CRC Press, 2012,  survival, censoring, truncated, time scales, continuous time model, discrete time model, cox regression, proportinal hazards, log-rank test, baseline, explanatory variables, interactions, parameters, model, male mortality, poisson regression, pricewise constant hazards, time-varying covariates, communal covatiates, event times, risk sets, residuals, assumptions, period survival, parametric, proportianal hazards, accelerated failure, frailty, stratification, competing rick, probabilities, causality, matching, aalen, additive hazards, dynamic path, inference, symptotic, distribution  Extending the Linear Model with R Generalized Linear, Mixed Effects and Nonparametric Regression Models, CRC Press, 2005,  binomial, count, regression, contingency, table, multinomial data, generalized linear model, gml, random effect, repeated measures, longitudinal, mixed effect, nonnormal, response, nonparametric, additive, tree, neural networks  Foundations of Statistical Algorithms With References to R Packages, CRC Press, 2013,  regression, iteration, optimization, univariate, multivariate, neural networks, constrained, linear programming, quadratic programming, evolutionary computing, simulated, annealing, maximum-likelihood, pls, em, algorithm, k-means, randomization, uniform distribution, recommended generator, bernouilli, binomial, hypergeometrical, poisson, waiting time, continuous, triangular, normal, mixture, exponential, lognormal, gibbs, rejection, metropolis-hasting, convergence, chain, mcmc, bugs, repetition, resampling, cross-validation, bootstrap, subsampling, hyperparameter, classification, evaluation, error, nonlinear, neural networks, latent variable, scalability, parallelization, parallel computing  ggplot2 - Elegant Graphics for Data Analysis, 2nd Edition, Springer, 2016,  colour, size, shape, aesthetic, facetting, geoms, smoother, boxplot, jittered, histogram, frequency, bar chart, time series, line, path, axes, labels, annotation, groups, surface, maps, vecotr, metadata, raster, area, uncertainty, overplotting, grammar, layers, scales, coordinate, legends, limits, positioning, linear coordinate, nonlinear coordinate, themes, elements, data analysis, tidy, spread, gather, separate, unite, filter, create variable, group-wise, summary, pipeline, trend, model, coefficient, programming, multiple component, function, functional programming  Guide to Programming and Algorithms using R, Springer, 2013,  loop, 1-norm, finding, nested, iteration, geometric, babylonian, recursion, recursive, fibonacci, factorial, solving, highest common factor, lowest common multiple, towers of hanoi, binary search, sequence generation, determinant, program, algorithms, inner product, order notation, infinity norm, matrix-vector, matrix-matrix, binary search, sequence generation, traveling salesman, binomial coefficients, accuracy, polynomial, horner s algorithm, sorting, bubble sort, insertion sort, quick sort, comparisons, linear systems of equations, triangular system, forward substitution, backward substitution, gaussian elimination, factorization, banded matrices, cholesky, gauss-jordan, determinant, inverting, file processing, investigating, modifying, multiple files, outputs, projects, traffic, words  Lattice Multivariate Data Visualization with R, Springer, 2008,  treillis, layout, grouped displays, annotation, captions, labels, legends, graphings, panel, univariate, density, histogram, q-q, normality, box-cox transformation, empirical cdf, box-and-whisker, violin, strip coercion, discrete, distribution, multiway table, bar chart, categorical, scatter, superposition, splom, scatter-plot, matrix, parallel coordinate, trivariate, 3d, surface, parameters, themes, devices, graphics, coordinate, axis, scales, labels, legends, data manipulation, subsetting, shingles, ordering, strips, tukey mean-difference, specialized, manipulating, inveractive additions, panel, function, box-and-whisker, corrgram, 3d, maps  Le logiciel R, mai\u0302triser le langage, effectuer des analyses (bio)statistiquesLe logiciel R, mai\u0302triser le langage, effectuer des analyses (bio)statistiques (pdf)  Le mod\u00e8le lin\u00e9aire g\u00e9n\u00e9ralis\u00e9 (scan),  generalized linear model, logit, binary, odds, inference, test, deviance, coefficient, ols, explanatory power, residuals, multicollinearity, nonlinear, multinomial, probability, ordinal  Learning Data Mining with R, Develop key skills and techniques with R to create and customize data mining algorithms, Packt, 2015  Learning R for Geospatial Analysis, Leverage the power of R to elegantly manage crucial geospatial analysis tasks, Packt, 2014,  time series, vector, function, graphical, function, tables, csv, data frames, code, raster, matrix, array, subsetting, overlay, reclassification, points, lines, polygons, properties, geometrical, calculations, reprojection, projection, layers, geometries, joining, modifying, analyzing, merging, cropping, trimming, aggregating, resampling, filtering, clumping, topography, hillshade, slope, time, spatial, combining, rasterizing, conversion, interpolation, knn, idw, kriging, ggplot2, ggmap, plotting  Learning RStudio for R Statistical Computing, Learn to effectively perform R development, statistical analysis and reporting with the most popular R IDE, Packt, 2012   Machine Learning with R, Learn how to use R to apply powerful machine learning methods and gain an insight into real-world applications, Packt, 2013  Mastering RStudio - Develop, Communicate, and Collaborate with R, Harness the power of RStudio to create web applications, R packages, markdown reports and pretty data visualizations, Packt, 2015  Mastering Social Media Mining with R, Extract valuable data from social media sites and make better business decisions using R, Packt, 2015,  graph, text, authentification, web, oauth, visualization, packages, opinion, sentiment, exploring, twitter, api, app, corpus, facebook, network, degree, betweenness, closeness, cluster, communities, trend, influencer, ctr, spam, stories, instagram, public media, hashtag, location, user, follower, user, comment, dataset, travel-related, follow, who, most, more, top, viral, popular, like, about, picture, github, active, metrics, exploratory, graphical, language, matcher, fork, issue, trend, repositories, repository, eda, correlation, regression, segmented, wikipedia, tumblr, quora, google maps, linkedin, blogger, foursquare, yelp  Mod\u00e9liser des interactions et des non-lin\u00e9arit\u00e9s (scan),  moderator, qualitative, dummy, groups, quantitative, multicollinearity, coefficient, nonlinear, change, piecewise, model, power, quadratic, polynomial  Parallel Computing for Data Science With Examples in R, C++ and CUDA (The R Series), CRC Press, 2015,  speed, obstacles, parallel, loop, scheduling, shared-memory, c, gpu, thrust, mapreduce, parallel, sorting, merging, matrix, subset, algebra  Parallel R, O Reilly, 2012,  snow, cluster, k-means, workers, load, balancing, chunking, vectorizing, redux, function, environment, random number generation, rmpi, executing, queueing, troubleshooting, multicore, parallel, mapreduce, hadoop, rhadoop, rhipe, segue, revoscale, revoconnectr, cloudnumbers  R and Data Mining, Examples and Case Studies, CC, 2013,  cases  R Compilation (see below)  R for Data Science_ Import, Tidy, Transform, Visualize, and Model Data, O Reilly, 2017,  ggplots2, workflow, transformation, dplyr, filter, arrange, select, add, group, mutate, summary, script, exploratory, question, variation, missing value, covariation, patterns, models, wrangle, tibbles, data frames, older code, readr, parsing, writing, tidy, spreading, gathering, separating, pull, missing value, case, nontidy, mutating, joins, stringr, string, regular expression, regex, pattern, factor, farcats, social survey, factor order, levels, dates, times, spans, zones, pipe, magrittr, function, conditional, arguments, return, vector, atomic recursive vector, iteration, purrr, loops, map, failure, modelr, visualizing, formulas, families, missing values, model, broom, gapminder, list-columns, markdown, output, document, notebook, presentation, dashboard, interactivity, website, format  R for Marketing Research and Analytics, Springer, 2015  R Graphics Cookbook, O Reilly, 2013,  exploring, bar graph, line, scatter, summarize, distribution, histogram, density, curve, frequency, polygon, boxplot, violin, dot, annotation, text, mathematical expression, segment, arrow, shaded, rectangle, highlighting, error bar, axes, x, y, range, continuous, categorical scaling, position, tick mark, appearance, logarithm, circular graph, date, time, themes, grid, facet, legend, position, order, title, appearance, label, facet, splitting, color, mapping, variable, discrete variable, palette, colorbling, region, correlation, function, subregion, network, heat, map, 3d, dendrogram, q-q, cumulative, mosaic, pie, map, choropleth, background, pdf, svg, output, file, presentation, shape, data frame, adding, deleting, renaming, reordering, subsetting, removing, changing, recoding, transforming, summarizing, converting, ggplot2  R in Action, Data Analysis and Graphics with R, 2nd Edition, Manning, 2015  R Packages Organize, Test, Document and Share Your Code, O Reilly, 2015,  metadata, object documentation, vignette, testing, namespace, external data, compile, file, component, git, github, checking, releasing  R\u00e9gression avec R, Springer, 2011  Regression Models, Methods and Applications, Springer, 2013,  linear, mixed, nonparametric, additive, generalized, geoadditive, location, beyond mean, scale, shape, quantile, estimation, parameters, hypothesis, choice, variable, model, quality, criteria, diagnosis, weighted least squares, heteroscedasticity, autocorrelation, regularization, ridge, shrinkage, operator, geometric, partial regularization, boosting, componentwise boosting, bayesian, conjugate analysis, spike, slab, binary, maximum likelihood, fit, overdispersion, count, continuous, response, quasi-likelihood, beyesian generalized linear, bayesian inference, mcmc, boosting, categorical, unordered, ordinal, cumulative, sequential, mixed, longitudinal, cluseted, random, intercept, coefficient, slope, notation, conditional, stochastic, lmm, variance-covariance, covariance, glmm, penalized likelihood, empirical bayes estimation, application, nonparametric, polynomial splines, p-splines, smoothing, random walks, kriging, bivariate, spatial smoothing, structured addtive, random effects, boosting, star, case, quantile, matrix algebra, probability, calculus, statistical, inference, normal distribution, likelihood function, bayesian inference  Social Media Mining with R, Deploy cutting-edge sentiment analysis techniques to real-world social media data using R, Packt, 2014,  big data, mining, twitter, social media, opinion, sentiment, emotion, polarity, lexicon, naive bayes, response, case  Statistical Analysis and Data Display, An Intermediate Course with Examples in R, 2nd Edition, Springer, 2009  The elements of statistical learning, Data Mining, Inference, and Prediction, 2nd Edition, Springer, 2011  Tiny Handbook of R, A SpringerBriefs in Statistics, Springer, 2011,  command line, script, function, project, data structure, operation, vector, matrix, factor, index, reshape, stack, unstack, wide, long, merge, missing value, mapping, apply, function, writing, tables, graphs, hypothesis, linear, formula, fit, general linear, regression, diagnostics, testing, coefficient, prediction, stepwise, residualizing, anova, comparison  Web Application Development with R using Shiny, Harness the graphical and statistical power of R and rapidly develop interactive user interfaces using the superb Shiny package, Packt, 2013,  application, example, widget type, google analytics, reactive, web page, running, html, serer, interface, javascript, jquery, inputs, outputs, ui, interface, reactivity, get, graphics, sharing, running, github, git, zip, tar, glimmer, shiny server, browser", 
            "title": "Entry-level"
        }, 
        {
            "location": "/collection/#r-compilation", 
            "text": "A Beginner s Guide to R [Zuur, Ieno   Meesters 2009-07-02]  A First Course in Statistical Programming with R [Braun   Murdoch 2008-01-28]  A Handbook of Statistical Analyses using R (2nd ed.) [Everitt   Hothorn 2009-07-20]  A Modern Approach to Regression with R [Sheather 2009-03-11]  A Practical Guide to Ecological Modelling Using R as a Simulation Platform [Soetaert   Herman 2008-11-21]  Adaptive Design Theory and Implementation using SAS and R [Chang 2007-06-27]  Advances in Social Science Research Using R - Vinod H. - 2011  An Introduction to Analysis of Financial Data with R [Tsay 2012-10-29]  An Introduction to Bootstrap Methods with Applications to R [Chernick   LaBudde 2011-11-01]  Analysis of Categorical Data with R [Bilder   Loughin 2014-08-11]  Analysis of Correlated Data with SAS and R (3rd ed.) [Shoukri   Chaudhary 2007-05-17]  Analysis of Financial Time Series  Analysis of Integrated and Cointegrated Time Series with R - Pfaff B. - 2008  Analysis of Phylogenetics and Evolution with R (2nd ed.) [Paradis 2011-11-09]  Analyzing Baseball Data with R [Marchi   Albert 2013-10-29]  Analyzing Linguistic Data - A Practical Introduction to Statistics using R - Baayen R. - 2008  Analyzing Sensory Data with R [L\u00ea   Worch 2014-10-09]  Analyzing Spatial Models of Choice and Judgment with R [Armstrong, Bakker, Carroll, Hare, Poole   Rosenthal 2014-02-07]  Applied Bayesian Statistics With R and OpenBUGS Examples [Cowles 2013-01-03]  Applied Statistical Genetics with R - Foulkes A. - 2009  Applied Statistics Using SPSS, STATISTICA, MATLAB and R - Marques J. - 2007  Basic R for Finance [W\u00fcrtz, Lam, Ellis   Chalabi 2010]  Bayesian Essentials with R (2nd ed.) [Marin   Robert 2013-10-29]  Bayesian Networks in R With Applications in Systems Biology [Nagarajan, Scutari   L\u00e8bre 2013-04-27]  Bayesian Networks With Examples in R [Scutari   Denis 2014-06-20]  Beginner\u2019s Guide to R - Zuur A. et al. - 2009  Beginning Data Science with R [Pathak 2014-12-09]  Beginning R - An Introduction to Statistical Programming - Pace L.  Beginning R - The Statistical Programming Language - Gardener M. - 2012  Behavioral Research Data Analysis with R - Li Y. et al. - 2012  Bioinformatics and Computational Biology Solutions using R and Bioconductor [Gentleman, Irizarry, Carey, Dudolt   Huber 2005-08-31]  Bioinformatics with R Cookbook [Sinha 2014-07-23]  Biostatistical Design and Analysis Using R - A Practical Guide - Logan M. - 2010  Biostatistical Design and Analysis using R_ A Practical Guide [Logan 2010-05-10]  Biostatistics with R An Introduction to Statistics through Biological Data [Shahbaba 2011-12-17]  Business Analytics for Managers - Jank W. - 2011  Chemometrics with R Multivariate Data Analysis in the Natural Sciences and Life Sciences [Wehrens 2011-01-31]  Clinical Trial Data Analysis - Using R - Din Chen, Karl E. Peace - 2010  Competing Risks and Multistate Models with R - Beyersmann J. et al. - 2012  Computational Actuarial Science with R [Charpentier 2014-08-26]  Computational Finance An Introductory Course with R [Arratia 2014-05-09]  Computational Statistics An Introduction to R [Sawitzki 2009-01-26]  Contingency Table Analysis Methods and Implementation using R [Kateri 2014-06-15]  Data Analysis and Graphics Using R - An Example-Based Approach, 3e - Maindonald J. et al. - 2010  Data Analysis using Regression and Multilevel_Hierarchical Models [Gelman   Hill 2006-12-18]  Data Manipulation with R (2nd ed.) [Abedin   Das 2015-03-31]  Data Manipulation with R - Spector P. - 2008  Data Mashups in R - Leipzig J. et al. - 2011  Data Mining with Rattle and R The Art of Excavating Data for Knowledge Discovery [Williams 2011-08-04]  Data Science in R A Case Studies Approach to Computational Reasoning and Problem Solving [Nolan   Lang 2015-03-18]  Data Wrangling with R - Boehmke B. - 2016  Doing Bayesian Data Analysis A Tutorial with R and BUGS [Kruschke 2010-11-10]  Dynamic Documents with R and knitr (2nd ed.) [Xie 2015-07-06]  Dynamic Linear Models with R - Petris G. et al. - 2009  EnvStats An R Package for Environmental Statistics [Millard 2013-10-28]  Exploratory Multivariate Analysis by Example - Using R - Husson F. et al. - 2011  Exploring Everyday Things with R and Ruby - Sheong Chang S. - 2012  Financial Risk Modelling and Portfolio Optimization with R [Pfaff 2013-01-22]  Forest Analytics with R - An Introduction - Robinson A. et al. - 2011  Foundational and Applied Statistics for Biologists using R [Aho 2013-12-17]  Functional and Phylogenetic Ecology in R [Swenson 2014-03-27]  Functional Data Analysis with R and MATLAB - Ramsay J. et al. - 2011  Getting Started with RStudio - Verzani J. - 2011  Graphical Models with R [H\u00f8jsgaard, Edwards   Lauritzen 2012-02-23]  Graphics for Statistics and Data Analysis with R [Keen 2010-04-26]  Graphing Data with R An Introduction - 2015  Guidebook to R Graphics using Microsoft Windows [Takezawa 2012-03-13]  Handbook of Statistical Analyses Using R, 2e - Everitt B. et al. - 2010  Hands-On Programming with R_ Write Your Own Functions and Simulations [Grolemund 2014-08-02]  Hidden Markov Models for Time Series_ An Introduction using R [Zucchini   MacDonald 2009-04-28]  Instant R Starter [Teutonico 2013-04-23]  Interactive and Dynamic Graphics for Data Analysis - With R and GGobi - Cook D. et al. - 2007  Introduction to Applied Multivariate Analysis with R - Everitt B. et al. -2011  Introduction to Data Analysis with R for Forensic Scientists [Curran 2010-07-30]  Introduction to Image Processing using R Learning by Examples [Frery   Perciano 2013-01-31]  Introduction to Probability and Statistics Using R - Kerns G. - 2010  Introduction to Probability Simulation and Gibbs Sampling with R - Suess E. et al. - 2010  Introduction to Probability with R [Baclawski 2008-01-24]  Introduction to R for Quantitative Finance [Dar\u00f3czi, Cs\u00f3ka, Tulassay, Puhle, Havran, V\u00e1radi, Berlinger, Michaletzky   Vidovics-Dancs 2013-11-22]  Introduction to Scientific Programming and Simulation using R (2nd ed.) [Jones, Maillardet   Robinson 2014-06-12]  Introduction to Statistics through Resampling Methods and R (2nd ed.) [Good 2013-02-11]  Introductory Statistics with R, 2e - Dalgaard P. - 2008  Introductory Time Series with R - Cowpertwait P. et al. - 2009  Latent Variable Modeling using R_ A Step-by-Step Guide [Beaujean 2014-05-08]  Latent Variable Modeling with R [Finch   French 2015-07-01]  Learning Data Mining with R [Makhabel 2014-12-22]  Learning Predictive Analytics with R- Eric Mayor - 2015  Learning R A Step-by-Step Function Guide to Data Analysis [Cotton 2013-09-26]  Linear Mixed-Effects Models using R_ A Step-by-Step Approach [Galecki   Burzykowski 2013-02-05]  Linear Models with R (2nd ed.) [Faraway 2014-07-01]_  Machine Learning with R Cookbook [Chiu 2015-03-31]  Machine Learning with R [Lantz 2013-10-25]  Making Your Case Using R for Program Evaluation [Auerbach   Zeitlin 2015-07-06]  Mastering Predictive Analytics with R [Forte 2015-06-30]  Mastering Scientific Computing with R [Gerrard   Johnson 2015-02-27]  Mathematical Statistics with Resampling and R - Chihara Laura M., Hesterberg Tim C. - 2011  MATLAB Graphics and Data Visualization Cookbook 2012  Maximum Likelihood Estimation and Inference_ With Examples in R, SAS and ADMB [Millar 2011-09-19]  Mixed Models_ Theory and Applications with R (2nd ed.) [Demidenko 2013-08-05]  Modern Actuarial Risk Theory - Using R - Kaas R. et al. - 2008  Modern Analysis of Customer Surveys With Applications using R [Kenett   Salini 2012-01-30]  Modern Approach to Regression with R - Sheather S. - 2009  Modern Industrial Statistics_ With Applications in R, MINITAB and JMP (2nd ed.) [Kenett, Zacks   Amberti 2014-01-28]  Modern Optimization with R [Cortez 2014-09-07]  Modern Regression Techniques Using R - Wright D. et al. - 2009  Modern Statistical Methods for Astronomy_ With R Applications [Feigelson   Babu 2012-08-27]  Morphometrics with R - Claude J. - 2008  Multilevel Modeling using R [Finch, Bolin   Kelley 2014-06-13]  Multiple Comparisons using R [Bretz, Hothorn   Westfall 2010-07-27]  Multistate Analysis of Life Histories with R [Willekens 2014-09-12]  Multivariate Generalized Linear Mixed Models using R [Berridge   Crouchley 2011-04-25]  Multivariate Methods of Representing Relations in R for Prioritization Purposes [Myers   Patil 2012-03-24]  Multivariate Nonparametric Regression and Visualization_ With R and Applications to Finance [Klemel\u00e4 2014-05-27]  Multivariate Time Series Analysis_ With R and Financial Applications [Tsay 2013-12-09]  Nonlinear Regression with R - Ritz C. et al. - 2008  Nonparametric Hypothesis Testing_ Rank and Permutation Methods with Applications in R [Bonnini, Corain, Marozzi   Salmaso 2014-09-15]  Nonparametric Statistical Methods using R [Kloke   McKean 2014-10-09]  Numerical Ecology with R - Borcard D. et al. - 2011  Practical Data Science with R [Zumel   Mount 2014-04-13]  Practical Graph Mining with R [Samatova, Hendrix, Jenkins, Padmanabhan   Chakraborty 2013-07-15]  Practical Guide to Ecological Modelling - Using R as a Simulation Platform - Soetaert K. et al. - 2009  Primer of Ecology with R - Henry M. et al. - 2009  Primer to Analysis of Genomic Data using R [Gondro 2015-05-20]  Probability With Applications and R [Dobrow 2013-11-04]  Programming Graphical User Interfaces in R - Lawrence M., Verzani J. - 2012  Quantitative Trading with R Understanding Mathematical and Computational Tools from a Quant s Perspective [Georgakopoulos 2015-01-06]  R Book - Crawley M. - 2007  R by Example - Jim Albert, Maria Rizzo - 2012  R Companion to Linear Statistical Models - Hay-Jahans C. - 2012  R Cookbook - Proven Recipes for Data Analisys, Statistics, and Graphics - Teetor P. - 2011  R Data Visualization Cookbook [Gohil 2015-01-29]  R for Business Analytics [Ohri 2012-09-14]  R for Cloud Computing An Approach for Data Scientists [Ohri 2014-11-15]  R for Data Science [Toomey 2014-12-19]  R for SAS and SPSS Users, 2e - Muenchen R - 2011  R for Stata Users - Muenchen R. et al. - 2011  R Graph Essentials [Lillis 2014-09-24]  R Graphs Cookbook (2nd ed.) [Abedin   Mittal 2014-10-20]  R Graphs Cookbook - Mittal H. - 2011  R Graphs Cookbook [Mittal 2011-01-14]  R High Performance Programming [Lim   Tjhi 2015-01-30]  R in a Nutshell_ A Desktop Quick Reference (2nd ed.) [Adler 2012-10-19]  R in Action - Kabacoff R. - 2011  R in Action_ Data Analysis and Graphics with R [Kabacoff 2011-08-27]  R Inferno - Burns P. - 2009  R Machine Learning Essentials [Usuelli 2014-11-25]  R Object-Oriented Programming [Black 2014-10-23]  R Programming for Bioinformatics - Gentelman R. - 2009  R Quick Syntax Reference [Tollefson 2014-04-25]  R Recipes A Problem-Solution Approach [Pace 2014-12-19]  R Statistical Application Development by Example Beginner s Guide [Tattar 2013-07-24]  R Through Excel - Heiberger R. et al. - 2009  Reproducible Research with R and RStudio [Gandrud 2013-07-15]  SAS and R Data Management, Statistical Analysis, and Graphics (2nd ed.) [Kleinman   Horton 2014-07-17]  Simulation and Inference for Stochastic Differential Equations_ With R Examples [Iacus 2008-05-05]  Six Sigma with R - Statistical Engineering for Process Improvement - Cano E. et al. - 2012  Social Media Mining with R [Danneman   Heimann 2014-03-24]  Software for Data Analysis - Programming with R - Chambers J. - 2008  Solving Differential Equations in R [Soetaert, Cash   Mazzia 2012-06-07]  Stated Preference Methods using R [Aizaki, Nakatani   Sato 2014-08-15]  Statistical Analysis of Financial Data in R (2nd ed.) [Carmona 2013-12-14]  Statistical Analysis of Network Data with R [Kolaczyk   Cs\u00e1rdi 2014-05-23]  Statistical Analysis of Questionnaires_ A Unified Approach Based on R and Stata [Bartolucci, Bacci   Gnaldi 2015-08-07]  Statistical Analysis with R - Beginner s Guide - Quick J. - 2010  Statistical Bioinformatics with R - Mathur S. - 2010  Statistical Data Analysis Explained - Applied Environmental Statistics with R - Reimann C. et al. - 2008  Statistical Methods for Environmental Epidemiology with R_ A Case Study in Air Pollution and Health [Peng   Dominici 2008-07-25]  Statistical Tools for Nonlinear Regression_ A Practical Guide with S-PLUS and R Examples (2nd ed.) [Huet, Bouvier, Poursat   Jolivet 2003-09-12]  Statistics An Introduction using R (2nd ed.) [Crawley 2014-11-24]  Statistics and Data Analysis for Financial Engineering With R Examples (2nd ed.) [Ruppert   Matteson 2015-04-22]  Statistics and Data Analysis for Microarrays using R and Bioconductor (2nd ed.) [Draghici 2016-06-15]  Statistics and Data with R - An Applied Approach Through Examples - Cohen Y. et al.- 2008  Statistics for Censored Environmental Data using Minitab and R (2nd ed.) [Helsel 2012-02-01]  Statistics for Linguistics with R A Practical Introduction (2nd ed.) [Gries 2013-03-15]  Statistics Using R with Biological Examples - Seefeld K. et al. - 2007  Text Analysis with R for Students of Literature [Jockers 2014-06-11]  The Essential R Reference [Gardener 2012-11-19]  The R Book (2nd ed.) [Crawley 2012-12-26]  The R Primer [Ekstr\u00f8m 2011-08-29]  The R Software_ Fundamentals of Programming and Statistical Analysis [de Micheaux, Drouilhet   Liquet 2014-02-28]  Time Series - Application to Finance with R and S-Plus, 2e - Chan N. - 2010  Time Series Analysis - With Applications in R, 2e - Cryer J. et al. - 2008  Time Series Analysis and Its Applications - With R Examples, 3e - Shumway R. et al. - 2010  Understanding Statistics using R [Schumacker   Tomek 2013-01-24]  Using R and RStudio for Data Management, Statistical Analysis, and Graphics (2nd ed.) [Horton   Kleinman 2015-03-17]  Using R for Data Management, Statistical Analysis, and Graphics - Horton N. et al. - 2011  Using R for Numerical Analysis in Science and Engineering [Bloomfield 2014-04-24]  Using R for Statistics [Stowell 2014-06-24]  Wavelet Methods in Statistics with R - Nason G. - 2008  XML and Web Technologies for Data Sciences with R [Nolan   Lang 2014-01-27]", 
            "title": "R Compilation"
        }, 
        {
            "location": "/collection/#regex", 
            "text": "regular expressions, text mining  Cheat sheets  Breaking the Ice with Regular Expressions (completed course, videos, notes, snapshots, manual), Code School, 2015,  regex  Les expressions r\u00e9guli\u00e8res par l exemple, HK, 2005,  Javascript, Python, POSIX  Mastering Python Regular Expressions, Leverage regular expressions in Python even for the most complex features, Packt, 2014  Regular Expressions Cookbook 2nd Edition, O Reilly, 2012", 
            "title": "Regex"
        }, 
        {
            "location": "/collection/#sql", 
            "text": "Business Intelligence avec SQL Server, Maitrisez les concept et r\u00e9alisez un syst\u00e8me d\u00e9cisionnel, eni, 2014  Cheat sheets  Soup to Bits, SQL (completed course, manual, 2 videos, notes), Code School, 2015,  uml, databases, udacity, github, article, mysql, install, cli, command, bash, create, table, data, entry, password, log in, case, weather, data, insurance, csv, gui, workbench, wamp, postgresql, install, cli, command, bash, gui, pgadmin, pgadmin iii  SQL Analyzing Business Metrics (completed course, snapshots, notes), Codecademy, 2015  Learn Java (completed course, notes, snapshots, codes, manual), Codecademy, 2015,  mysql  Learn SQL (completed course, snapshots, notes, codes, manual), Codecademy, 2015  SQL Table Transformation (completed course, snapshots, notes), Codecademy, 2015  LinuxCBT PostgreSQL Edition (videos, 17h), LinuxCBT, 2013  PostgreSQL, Getting Started (videos, 2h), PluralSight, 2015  Python Programming with PostgreSQL (videos, 5.5h)  SQL Pocket Guide, O Reilly, 2011  SQL Server 2014, SQL, Transact SQL, Conception et realisation d une base de donn\u00e9es, avec exercices pratiques et corrig\u00e9s, eni, 2014  Apprendre SQL avec MySQL, Avec 40 exercices corrig\u00e9s, Eyrolles, 2006  Introduction to SQL JOINs (notes)  Le langage MDX pour les requ\u00eates de cubes OLAP (notes)  Learning PostgreSQL, Create, develop, and manage relational databases in real-world applications using PostgreSQL, Packt, 2015  MDX tutorial, introduction to Multidimensional Expressions (videos),  cube, olap, vba, excel, pivot, table, powerpivot, powerview, powerquery, powermap, business, analytics, big data, bi  MySQL in a Nutshell, 2nd Edition, O Reilly, 2008  PostgreSQL Cookbook, Over 90 hands-on recipes to effectively manage, administer, and design solutions using PostgreSQL, Packt, 2015  PostgreSQL for Data Architects, Discover how to design, develop, and maintain your database application effectively with PostgreSQL, Packt, 2015   Programmer avec MySQL, SQL, transactions, PHP, Java, Optimisation, Avec 40 exercices corrig\u00e9s, 4e \u00e9dition, Eyrolles, 2015", 
            "title": "SQL"
        }, 
        {
            "location": "/collection/#storytelling-visualization", 
            "text": "Storytelling with data, Wiley, 2015  The Wall Street Journal Guide to Information Graphics, The does and don ts of presenting data, facts, and figures, W.W. Norton   Company, 2013  Visualize This, The FlowingData Guide to Design, Visualization, and Statistics, Wiley, 2011  Data Visualization with Python and JavaScript, O Reilly, 2016  Excel Sparklines (notes)  Interactive Data Visualization for the Web, O Reilly, 2013   Interactive Data Visualization, Foundations, Techniques, and Applications, 2nd Edition, CRC Press, 2015  Storytelling with data (notes)  The Grammar of Graphics, 2nd Edition, Springer, 2005  Which chart or graph is right, Tableau (article), 2012", 
            "title": "Storytelling, Visualization"
        }, 
        {
            "location": "/collection/#tableau", 
            "text": "software  Cases  Data Analysis Fundamentals with Tableau (videos, 5h), PluralSight, 2013  Enterprise Business Intelligence with Tableau Server (videos, 1h30), PlusralSight, 2013  Communicating Data with Tableau.pdf  Getting started (manual)  Learning Tableau.pdf  Which chart or graph is right, Tableau (article), 2012", 
            "title": "Tableau"
        }, 
        {
            "location": "/collection/#teamtreehouse", 
            "text": "software, programming, coding, script  Learning Tracks:   TTH Business  TTH CSS  TTH Databases  TTH Development Tools  TTH HTML  TTH JavaScript  TTH Python  TTH Starting a Business Track  TTH Web Design Track   Course topics:   App in Heroku  Business, Google Analytics  Business, SEO  Business, build a company  Business, freelance  Business, market a business  Business, social Media Strategy  Business, soft skills  Business, start a business  Business, user needs  Business, write a business plan  CSS  Console  D3.js  Django  Dynamic Site, Node.js  Flask  Geolocation  Git  GitHub  HTML  JavaScript  MongoDB  Python, databases  Python, dates, times  Python, decorators  Python, image  Python, setup  Python, testing  Python, write better code  Regex  Scrum  Website   Setting Up a Local Python Environment (notes)  TeamTreeHouse Course List (notes)", 
            "title": "TeamTreeHouse"
        }, 
        {
            "location": "/collection/#udemy", 
            "text": "Potential courses:   A-B Testing  Applied Multivariate Analysis with R  Coding  Customer Choice Modeling  Data Analysis and Visualization  Data Science  Machine Learning  MongoDB  MySQL  PostgreSQL  Python  and many more   Udemy courses (notes),  python, mysql, postgresql, mongodb, tableau, r, text mining, machine learning, javascript, nlp, psychology", 
            "title": "Udemy"
        }, 
        {
            "location": "/collection/#vim", 
            "text": "Cheat sheets", 
            "title": "vim"
        }, 
        {
            "location": "/collection/#miscellanous", 
            "text": "vba, macros, excel, word, linux, r, packages, scrum  Le livre du z\u00e9ro (s\u00e9rie: C#, C, Java, iPhone, C++, Word)  Algorithms in a Nutshell, 2nd Edition, O Reilly, 2010  Algorithms in a Nutshell, O Reilly, 2009  La chance et les jeux de hasard (notes)  Macros et langage VBA (notes)  Windev20 (notes)  Word, habillage de figure (notes)", 
            "title": "Miscellanous"
        }, 
        {
            "location": "/Digital Humanities/", 
            "text": "Digital Humanities\n\n\nForeword\n\n\nNotes. From:\n\n\n\n\nThe Historian\ns Macroscope\n\n\nBook exploring big historical data.\n\n\n\n\n\n\nDigital Humanities\n\n\nStandford Humanities Center.\n\n\n\n\n\n\n\n\n\n\nCONTENT\n\n\nDigital Humanities\n\n\nBlogs\n\n\nCases\n\n\nCourses\n\n\nData and Models\n\n\nDatasets and Projects\n\n\nNetworks\n\n\nTools, Data Mining and Analyzing\n\n\nTools, Mining the Web\n\n\nTools, Referencing\n\n\nTools, Scanning\n\n\nTools, Visualization\n\n\n\n\n\n\n\n\n\n\n\n\nBlogs\n\n\n\n\nHistoryonics\n\n\nDay of Archaeology\n\n\nThe Day of Archaeology is an event where archaeologists write about their activities on a group blog.\n\n\nCurrently there are over 1000 posts on the blog; a lot to read in one sitting. Rather than closely read each post, we can do a distant reading to get some insights into the corpus. Distant reading refers to efforts to understand texts through quantitative analysis and visualisation. \n\n\n\n\n\n\nGlobal Perspective on Digital History\n\n\nThe Programming Historian\n\n\nLessons, projects, research, blog. \n\n\nThe Programming Historian offers novice-friendly, peer-reviewed tutorials that help humanists learn a wide range of digital tools, techniques, and workflows to facilitate their research.\n\n\n\n\n\n\nITHAKA S+R\n\n\nProvides research and strategic guidance to help the academic community navigate economic and technological change.\n\n\n\n\n\n\n\n\nCases\n\n\n\n\nBig Data + Old History\n\n\nVideo; dig into documents without reading them.\n\n\n\n\n\n\nOld Bailey\n\n\nThe proceedings of the Old Bailey, 1674-1913. A fully searchable edition of the largest body of texts detailing the lives of non-elite people ever published, containing 197,745 criminal trials held at London\ns central criminal court.\n\n\nWhite paper; Data Mining with Criminal Intent\n\n\n\n\n\n\n\n\nCourses\n\n\n\n\nPython Programming for the Humanities\n\n\nThe Programming Historian Lessons\n\n\nAPI.\n\n\nData Management.\n\n\nData Manipulation.\n\n\nDistant Reading.\n\n\nGetting Ready to Program.\n\n\nLinked Open Data.\n\n\nMapping and GIS.\n\n\nNetwork Analysis.\n\n\nDigital Exhibits and Augmented Reality.\n\n\nWeb Scraping.\n\n\nIntroduction to Python (series).\n\n\n\n\n\n\n\n\nData and Models\n\n\n\n\nOpen Data\n\n\n\n\nDatasets and Projects\n\n\n\n\nLe Programme de recherche en d\u00e9mographie historique\n\n\nDonn\u00e9es g\u00e9n\u00e9alogiques de la Nouvelle France au Qu\u00e9bec contemporain.\n\n\n\n\n\n\nORBIS\n\n\nThe Stanford Geospatial Network Model of the Roman World.\n\n\nCould become a boardgame (or an app).\n\n\n\n\n\n\nPelagios\n\n\nPelagios Commons provides online resources and a community forum for using open data methods to link and explore historical places.\n\n\nMaps\n\n\nBlog\n\n\n\n\n\n\nLOTR Project\n\n\nLotrProject is dedicated to bringing J.R.R. Tolkien\ns works to life through various creative web projects (genealogy, interactive maps, timelines, and statistics).\n\n\n\n\n\n\nComputational Folkloristics\n\n\nMapping folktales and linking themes.\n\n\nMagazine\n\n\nArticle; Big Folklore: A Special Issue on Computational Folkloristics\n\n\n\n\n\n\nDataverse\n\n\nDataverse collects data on social-scientific, health, and environmental data for the world as a whole for the past four or five centuries. \n\n\n\n\n\n\nCLIWOC\n\n\nClimatological Database for the World\ns Oceans 1750-1850.\n\n\nWikipedia\n\n\nRoyal Netherlands Meteorological Institute\n\n\n\n\n\n\n\n\nNetworks\n\n\n\n\nAccess Linked Open Data\n\n\nRDF databases, graph databases, and how researchers can access these data though the query language called SPARQL. \n\n\nRDF represents information in a series of three-part \nstatements\n that comprise a subject, predicate, and an object.  \n\n\n\n\n\n\nNetwork visualizations\n\n\nData extraction and network visualization of historical sources.\n\n\n\n\n\n\nUCINETS\n\n\nUCINET 6 is a software package for the analysis of social network data.\n\n\n\n\n\n\nPajek\n\n\nAnalysis and visualization of large networks. \n\n\nAnalyse des r\u00e9seaux : une introduction \u00e0 Pajek\n\n\n\n\n\n\nNetwork Workbench\n\n\nA Large-Scale Network Analysis, Modeling and Visualization Toolkit for Biomedical, Social Science and Physics Research.   \n\n\n\n\n\n\nSci2\n\n\nThe Science of Science (Sci2) Tool is a modular toolset specifically designed for the study of science.\n\n\nIt supports the temporal, geospatial, topical, and network analysis and visualization of scholarly datasets at the micro (individual), meso (local), and macro (global) levels.   \n\n\n\n\n\n\nNodeXL\n\n\nNetwork overview, discovery and exploration for Excel.\n\n\n\n\n\n\nGephi\n\n\nVisualization and exploration software for all kinds of graphs and networks.\n\n\nViewer\n\n\nExporter\n\n\nWeb Export\n\n\n\n\n\n\n\n\nTools, Data Mining and Analyzing\n\n\n\n\nGoogle Ngram\n\n\nMine Google Books.\n\n\nOnline and an API is available.\n\n\n\n\n\n\nR and Python\n\n\nPackages for text mining, text analysis, NLP (natural language processing).\n\n\nTopic Modeling Tool\n\n\n\n\n\n\nUnix\n\n\nMining text and qualitative data with Unix.\n\n\nDownload, trim, dig into dir and subdir, find patterns, count files, lines, words, save into a new file or a subdir, count instances and other stats like concordances, use pipelines, create lists with  unix commands, etc.\n\n\n\n\n\n\nOpenRefine\n\n\nClean data, remove duplicate records, separate multiple values contained in the same field, analyse the distribution of values throughout a data set, group together different representations of the same reality, etc.\n\n\n\n\n\n\nAntConc\n\n\nCorpus analysis.\n\n\nCreate/download a corpus of texts, conduct a keyword-in-context search, identify patterns surrounding a particular word, use more specific search queries, look at statistically significant differences between corpora, make multi-modal comparisons using corpus lingiustic methods. \n\n\nMany other tools on \nLaurence Anthony\ns Website\n:\n\n\nAntConc : a freeware corpus analysis toolkit for concordancing and text analysis.\n\n\nAntPConc : a freeware \nparallel\n corpus analysis toolkit for concordancing and text analysis using UTF-8 encoded text files.\n\n\nAntWordProfiler : a freeware tool for profiling the vocabulary level and complexity of texts.\n\n\nAntFileConverter : a freeware tool to convert PDF and Word (DOCX) files into plain text for use in corpus tools like AntConc.\n\n\nAntMover : a freeware text structure (moves, outline, flow) analysis program.\n\n\nAntCLAWSGUI : a front-end interface to the CLAWS tagger developed at Lancaster University,\n\n\nEncodeAnt : a freeware tool for detecting and converting character encodings.\n\n\nFireAnt :  a freeware social media and data analysis toolkit (developed in collaboration with Claire Hardaker of Lancaster University).\n\n\nProtAnt : a freeware prototypical text analysis tool (developed in collaboration with Paul Baker of Lancaster University).\n\n\nSarAnt : a freeware batch search and replace tool.\n\n\nSegmentAnt : a freeware Japanese and Chinese segmenter (segmentation/tokenizing tool).\n\n\nTagAnt : a freeware Part-Of-Speech (POS) tagger built on TreeTagger (developed by Helmut Schmid).\n\n\nVariAnt : A freeware spelling VariAnt analysis program.\n\n\n\n\n\n\n\n\n\n\nBeautiful Soup\n\n\nPython module.\n\n\nText parsing.\n\n\n\n\n\n\nTaPOR\n\n\nGateway to the tools for sophisticated text analysis and retrieval.\n\n\n\n\n\n\nPaper Machines\n\n\nVisualize thousands of texts.\n\n\nPlugin for the Zotero bibliographic management software.\n\n\n\n\n\n\nVoyant\n\n\nOnline tool for concordances, wordles, stats, graphics. \n\n\nCan be set on a local server.\n\n\nDocumentation\n\n\n\n\n\n\nOverview\n\n\nSearch, visualize, and review your documents. \n\n\nUp to hundreds of thousands of them, in any format.\n\n\n\n\n\n\nMALLET\n\n\nNLP toolkit and machine learning.\n\n\nTopic Analysis, keywords, bags of words. \n\n\nTopic modeling tool takes a single text (or corpus) and looks for patterns in the use of words; it is an attempt to inject semantic meaning into vocabulary.\n\n\nTopic models represent a family of computer programs that extract topics from texts. A topic to the computer is a list of words that occur in statistically meaningful ways. A text can be an email, a blog post, a book chapter, a journal article, a diary entry \u2013 that is, any kind of unstructured text.\n\n\nDocumentation\n\n\nArticle; Getting Started with Topic Modeling and MALLET\n\n\n\n\n\n\nStanford Topic Modeling Toolbox\n\n\nThe Stanford Topic Modeling Toolbox (TMT) brings topic modeling tools to social scientists and others who wish to perform analysis on datasets that have a substantial textual component:\n\n\nImport and manipulate text from cells in Excel and other spreadsheets.\n\n\nTrain topic models (LDA, Labeled LDA, and PLDA new) to create summaries of the text.\n\n\nSelect parameters (such as the number of topics) via a data-driven process.\n\n\nGenerate rich Excel-compatible outputs for tracking word usage across topics, time, and other groupings of data.\n\n\n\n\n\n\n\n\n\n\nRegexr\n\n\nOnline tool for processing regular expression or regex.\n\n\n\n\n\n\nRegex can also be done\n\n\nTutorial on how to process regular expressions in Notepad++, on TextWrangler or other text/code editors (Vim, Emacs, etc.).\n\n\n\n\n\n\n\n\nTools, Mining the Web\n\n\n\n\nRetrieving Web Archive:\n\n\nInternet Archives, Wayback Machine\n\n\nTime Travel\n\n\n\n\n\n\nMining the Internet Archive Collection\n\n\ninternetarchive\n Python package.\n\n\n\n\n\n\nWget\n\n\nAutomated downloading with Wget. \n\n\nPull data from the web.\n\n\n\n\n\n\nQuery\n\n\nDownloading many records using Python.\n\n\nHow to check if a directory exists and create it if necessary\n\n\n\n\n\n\nFigshare\n\n\nWeb scraping. \n\n\n\n\n\n\nOutwit\n\n\nFind, grab and organize all kinds of data and media from online sources.  \n\n\n\n\n\n\nXPath\n\n\nWeb scraping, screen scraping, data parsing and other related things.\n\n\nAbout XPath\n\n\n\n\n\n\nImportio\n\n\nExtract web data the easy way.    \n\n\n\n\n\n\nTabula\n\n\nExtract data from PDF.    \n\n\n\n\n\n\n\n\nTools, Referencing\n\n\n\n\nZotero\n\n\nStandalone and add-in to Mozilla Firefox. \n\n\nAdd-in to text processors (including LaTeX and Markdown editors). \n\n\nDig into journals and books primary sources.\n\n\nCollect, organize, cite, and share your research sources.\n\n\n\n\n\n\nJSOR\n\n\nJournals, primary sources, and now BOOKS.\n\n\n\n\n\n\n\n\nTools, Scanning\n\n\n\n\nOCR scanner\n\n\nDigitization material documents (from books, letters to maps).\n\n\n\n\n\n\nThe MNIST Database\n\n\nThe MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples (used in machine learning to process NLP). \n\n\nIt is a subset of a larger set available from NIST. \n\n\nThe digits have been size-normalized and centered in a fixed-size image. \n\n\n\n\n\n\n\n\nTools, Visualization\n\n\n\n\nSparklines are mini-graphics. Add-in to Excel.\n\n\nGIS: the best open-source database for mapping and GIS is PostgreSQL.\n\n\nGoogle Maps and Google Earth\n\n\nGoogle My Maps and Google Earth provide an easy way to start creating digital maps. With a Google Account you can create and edit personal maps by clicking on My Places.\n\n\nIn My Maps you can choose between several different base maps (including the standard satellite, terrain, or standard maps) and add points, lines and polygons. It is also possible to import data from a spreadsheet, if you have columns with geographical information (i.e. longitudes and latitudes or place names). This automates a formerly complex task known as geocoding. Not only is this one of the easiest ways to begin plotting your historical data on a map, but it also has the power of Google\u2019s search engine. As you read about unfamiliar places in historical documents, journal articles or books, you can search for them using Google Maps. It is then possible to mark numerous locations and explore how they relate to each other geographically. Your personal maps are saved by Google (in their cloud), meaning you can access them from any computer with an internet connection. You can keep them private or embed them in your website or blog. Finally, you can export your points, lines, and polygons as KML files and open them in Google Earth or Quantum GIS.\n\n\n\n\n\n\nQGIS\n\n\nAdding layers to a map.\n\n\nInstalling QGIS 2.0 and Adding Layers\n\n\nCreating New Vector Layers\n\n\nGeoreferencing\n\n\nA Free and Open Source Geographic Information System \n\n\n\n\n\n\nOmeka\n\n\nDisplays items, collections, like in museums, libraries, archives with narratives.\n\n\n\n\n\n\nAugmented Reality\n\n\nOverlaying of digital content (images, video, text, sound, etc.) onto physical objects or locations, and it is typically experienced by looking through the camera lens of an electronic device such as a smartphone, tablet, or optical head-mounted display (e.g. Microsoft Hololens).  \n\n\nAlthough AR is a cutting-edge, complex technology, there are a number of user-friendly platforms that allow people with no previous coding experience to create compelling augmented reality experiences.\n\n\n\n\n\n\nD3.js\n\n\nJavaScript visualization for the web; server-side.\n\n\nD3 Examples\n\n\nA freelancer\ns gallery\n\n\n\n\n\n\nBokeh\n\n\nGallery\n\n\n\n\n\n\nTop 7 Free Infographics Tools \n Online Makers in 2016\n\n\nSHANTI INTERACTIVE\n\n\nSuite of tools that make it easy to create highly interactive web-based visualizations, videos, and maps. They are freely available from the University of Virginia\ns Sciences, Humanities \n Arts Network of Technological Initiatives (SHANTI).\n\n\nQmedia provides new ways to use video for instructional and scholarly purposes. The viewer interacts with the whole screen and sees a wide array of web-based resources and offers an immersive experience that adds context.\n\n\nSHIVA takes a new approach that makes it easy to add graphical and data-driven visualizations to websites. Elements such as data, charts, network graphs, maps, image montages, and timelines are easily created.\n\n\nMapScholar is an online platform for geospatial visualization funded by the NEH. It enables humanities and social science scholars to create digital \u201catlases\u201d featuring high-resolution images of historic maps.\n\n\nVisualEyes is web-based authoring tool for historic visualization funded by the NEH to weave images, maps, charts, video and data into highly interactive and compelling dynamic visualizations.\n\n\nVisualEyes5 is a HTML5 version of the VisualEyes authoring tool for historic visualization to weave images, maps, charts, video and data into highly interactive and compelling dynamic visualizations.", 
            "title": "Digital Humanities"
        }, 
        {
            "location": "/Digital Humanities/#blogs", 
            "text": "Historyonics  Day of Archaeology  The Day of Archaeology is an event where archaeologists write about their activities on a group blog.  Currently there are over 1000 posts on the blog; a lot to read in one sitting. Rather than closely read each post, we can do a distant reading to get some insights into the corpus. Distant reading refers to efforts to understand texts through quantitative analysis and visualisation.     Global Perspective on Digital History  The Programming Historian  Lessons, projects, research, blog.   The Programming Historian offers novice-friendly, peer-reviewed tutorials that help humanists learn a wide range of digital tools, techniques, and workflows to facilitate their research.    ITHAKA S+R  Provides research and strategic guidance to help the academic community navigate economic and technological change.", 
            "title": "Blogs"
        }, 
        {
            "location": "/Digital Humanities/#cases", 
            "text": "Big Data + Old History  Video; dig into documents without reading them.    Old Bailey  The proceedings of the Old Bailey, 1674-1913. A fully searchable edition of the largest body of texts detailing the lives of non-elite people ever published, containing 197,745 criminal trials held at London s central criminal court.  White paper; Data Mining with Criminal Intent", 
            "title": "Cases"
        }, 
        {
            "location": "/Digital Humanities/#courses", 
            "text": "Python Programming for the Humanities  The Programming Historian Lessons  API.  Data Management.  Data Manipulation.  Distant Reading.  Getting Ready to Program.  Linked Open Data.  Mapping and GIS.  Network Analysis.  Digital Exhibits and Augmented Reality.  Web Scraping.  Introduction to Python (series).", 
            "title": "Courses"
        }, 
        {
            "location": "/Digital Humanities/#data-and-models", 
            "text": "Open Data", 
            "title": "Data and Models"
        }, 
        {
            "location": "/Digital Humanities/#datasets-and-projects", 
            "text": "Le Programme de recherche en d\u00e9mographie historique  Donn\u00e9es g\u00e9n\u00e9alogiques de la Nouvelle France au Qu\u00e9bec contemporain.    ORBIS  The Stanford Geospatial Network Model of the Roman World.  Could become a boardgame (or an app).    Pelagios  Pelagios Commons provides online resources and a community forum for using open data methods to link and explore historical places.  Maps  Blog    LOTR Project  LotrProject is dedicated to bringing J.R.R. Tolkien s works to life through various creative web projects (genealogy, interactive maps, timelines, and statistics).    Computational Folkloristics  Mapping folktales and linking themes.  Magazine  Article; Big Folklore: A Special Issue on Computational Folkloristics    Dataverse  Dataverse collects data on social-scientific, health, and environmental data for the world as a whole for the past four or five centuries.     CLIWOC  Climatological Database for the World s Oceans 1750-1850.  Wikipedia  Royal Netherlands Meteorological Institute", 
            "title": "Datasets and Projects"
        }, 
        {
            "location": "/Digital Humanities/#networks", 
            "text": "Access Linked Open Data  RDF databases, graph databases, and how researchers can access these data though the query language called SPARQL.   RDF represents information in a series of three-part  statements  that comprise a subject, predicate, and an object.      Network visualizations  Data extraction and network visualization of historical sources.    UCINETS  UCINET 6 is a software package for the analysis of social network data.    Pajek  Analysis and visualization of large networks.   Analyse des r\u00e9seaux : une introduction \u00e0 Pajek    Network Workbench  A Large-Scale Network Analysis, Modeling and Visualization Toolkit for Biomedical, Social Science and Physics Research.       Sci2  The Science of Science (Sci2) Tool is a modular toolset specifically designed for the study of science.  It supports the temporal, geospatial, topical, and network analysis and visualization of scholarly datasets at the micro (individual), meso (local), and macro (global) levels.       NodeXL  Network overview, discovery and exploration for Excel.    Gephi  Visualization and exploration software for all kinds of graphs and networks.  Viewer  Exporter  Web Export", 
            "title": "Networks"
        }, 
        {
            "location": "/Digital Humanities/#tools-data-mining-and-analyzing", 
            "text": "Google Ngram  Mine Google Books.  Online and an API is available.    R and Python  Packages for text mining, text analysis, NLP (natural language processing).  Topic Modeling Tool    Unix  Mining text and qualitative data with Unix.  Download, trim, dig into dir and subdir, find patterns, count files, lines, words, save into a new file or a subdir, count instances and other stats like concordances, use pipelines, create lists with  unix commands, etc.    OpenRefine  Clean data, remove duplicate records, separate multiple values contained in the same field, analyse the distribution of values throughout a data set, group together different representations of the same reality, etc.    AntConc  Corpus analysis.  Create/download a corpus of texts, conduct a keyword-in-context search, identify patterns surrounding a particular word, use more specific search queries, look at statistically significant differences between corpora, make multi-modal comparisons using corpus lingiustic methods.   Many other tools on  Laurence Anthony s Website :  AntConc : a freeware corpus analysis toolkit for concordancing and text analysis.  AntPConc : a freeware  parallel  corpus analysis toolkit for concordancing and text analysis using UTF-8 encoded text files.  AntWordProfiler : a freeware tool for profiling the vocabulary level and complexity of texts.  AntFileConverter : a freeware tool to convert PDF and Word (DOCX) files into plain text for use in corpus tools like AntConc.  AntMover : a freeware text structure (moves, outline, flow) analysis program.  AntCLAWSGUI : a front-end interface to the CLAWS tagger developed at Lancaster University,  EncodeAnt : a freeware tool for detecting and converting character encodings.  FireAnt :  a freeware social media and data analysis toolkit (developed in collaboration with Claire Hardaker of Lancaster University).  ProtAnt : a freeware prototypical text analysis tool (developed in collaboration with Paul Baker of Lancaster University).  SarAnt : a freeware batch search and replace tool.  SegmentAnt : a freeware Japanese and Chinese segmenter (segmentation/tokenizing tool).  TagAnt : a freeware Part-Of-Speech (POS) tagger built on TreeTagger (developed by Helmut Schmid).  VariAnt : A freeware spelling VariAnt analysis program.      Beautiful Soup  Python module.  Text parsing.    TaPOR  Gateway to the tools for sophisticated text analysis and retrieval.    Paper Machines  Visualize thousands of texts.  Plugin for the Zotero bibliographic management software.    Voyant  Online tool for concordances, wordles, stats, graphics.   Can be set on a local server.  Documentation    Overview  Search, visualize, and review your documents.   Up to hundreds of thousands of them, in any format.    MALLET  NLP toolkit and machine learning.  Topic Analysis, keywords, bags of words.   Topic modeling tool takes a single text (or corpus) and looks for patterns in the use of words; it is an attempt to inject semantic meaning into vocabulary.  Topic models represent a family of computer programs that extract topics from texts. A topic to the computer is a list of words that occur in statistically meaningful ways. A text can be an email, a blog post, a book chapter, a journal article, a diary entry \u2013 that is, any kind of unstructured text.  Documentation  Article; Getting Started with Topic Modeling and MALLET    Stanford Topic Modeling Toolbox  The Stanford Topic Modeling Toolbox (TMT) brings topic modeling tools to social scientists and others who wish to perform analysis on datasets that have a substantial textual component:  Import and manipulate text from cells in Excel and other spreadsheets.  Train topic models (LDA, Labeled LDA, and PLDA new) to create summaries of the text.  Select parameters (such as the number of topics) via a data-driven process.  Generate rich Excel-compatible outputs for tracking word usage across topics, time, and other groupings of data.      Regexr  Online tool for processing regular expression or regex.    Regex can also be done  Tutorial on how to process regular expressions in Notepad++, on TextWrangler or other text/code editors (Vim, Emacs, etc.).", 
            "title": "Tools, Data Mining and Analyzing"
        }, 
        {
            "location": "/Digital Humanities/#tools-mining-the-web", 
            "text": "Retrieving Web Archive:  Internet Archives, Wayback Machine  Time Travel    Mining the Internet Archive Collection  internetarchive  Python package.    Wget  Automated downloading with Wget.   Pull data from the web.    Query  Downloading many records using Python.  How to check if a directory exists and create it if necessary    Figshare  Web scraping.     Outwit  Find, grab and organize all kinds of data and media from online sources.      XPath  Web scraping, screen scraping, data parsing and other related things.  About XPath    Importio  Extract web data the easy way.        Tabula  Extract data from PDF.", 
            "title": "Tools, Mining the Web"
        }, 
        {
            "location": "/Digital Humanities/#tools-referencing", 
            "text": "Zotero  Standalone and add-in to Mozilla Firefox.   Add-in to text processors (including LaTeX and Markdown editors).   Dig into journals and books primary sources.  Collect, organize, cite, and share your research sources.    JSOR  Journals, primary sources, and now BOOKS.", 
            "title": "Tools, Referencing"
        }, 
        {
            "location": "/Digital Humanities/#tools-scanning", 
            "text": "OCR scanner  Digitization material documents (from books, letters to maps).    The MNIST Database  The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples (used in machine learning to process NLP).   It is a subset of a larger set available from NIST.   The digits have been size-normalized and centered in a fixed-size image.", 
            "title": "Tools, Scanning"
        }, 
        {
            "location": "/Digital Humanities/#tools-visualization", 
            "text": "Sparklines are mini-graphics. Add-in to Excel.  GIS: the best open-source database for mapping and GIS is PostgreSQL.  Google Maps and Google Earth  Google My Maps and Google Earth provide an easy way to start creating digital maps. With a Google Account you can create and edit personal maps by clicking on My Places.  In My Maps you can choose between several different base maps (including the standard satellite, terrain, or standard maps) and add points, lines and polygons. It is also possible to import data from a spreadsheet, if you have columns with geographical information (i.e. longitudes and latitudes or place names). This automates a formerly complex task known as geocoding. Not only is this one of the easiest ways to begin plotting your historical data on a map, but it also has the power of Google\u2019s search engine. As you read about unfamiliar places in historical documents, journal articles or books, you can search for them using Google Maps. It is then possible to mark numerous locations and explore how they relate to each other geographically. Your personal maps are saved by Google (in their cloud), meaning you can access them from any computer with an internet connection. You can keep them private or embed them in your website or blog. Finally, you can export your points, lines, and polygons as KML files and open them in Google Earth or Quantum GIS.    QGIS  Adding layers to a map.  Installing QGIS 2.0 and Adding Layers  Creating New Vector Layers  Georeferencing  A Free and Open Source Geographic Information System     Omeka  Displays items, collections, like in museums, libraries, archives with narratives.    Augmented Reality  Overlaying of digital content (images, video, text, sound, etc.) onto physical objects or locations, and it is typically experienced by looking through the camera lens of an electronic device such as a smartphone, tablet, or optical head-mounted display (e.g. Microsoft Hololens).    Although AR is a cutting-edge, complex technology, there are a number of user-friendly platforms that allow people with no previous coding experience to create compelling augmented reality experiences.    D3.js  JavaScript visualization for the web; server-side.  D3 Examples  A freelancer s gallery    Bokeh  Gallery    Top 7 Free Infographics Tools   Online Makers in 2016  SHANTI INTERACTIVE  Suite of tools that make it easy to create highly interactive web-based visualizations, videos, and maps. They are freely available from the University of Virginia s Sciences, Humanities   Arts Network of Technological Initiatives (SHANTI).  Qmedia provides new ways to use video for instructional and scholarly purposes. The viewer interacts with the whole screen and sees a wide array of web-based resources and offers an immersive experience that adds context.  SHIVA takes a new approach that makes it easy to add graphical and data-driven visualizations to websites. Elements such as data, charts, network graphs, maps, image montages, and timelines are easily created.  MapScholar is an online platform for geospatial visualization funded by the NEH. It enables humanities and social science scholars to create digital \u201catlases\u201d featuring high-resolution images of historic maps.  VisualEyes is web-based authoring tool for historic visualization funded by the NEH to weave images, maps, charts, video and data into highly interactive and compelling dynamic visualizations.  VisualEyes5 is a HTML5 version of the VisualEyes authoring tool for historic visualization to weave images, maps, charts, video and data into highly interactive and compelling dynamic visualizations.", 
            "title": "Tools, Visualization"
        }, 
        {
            "location": "/Learn Git/", 
            "text": "Learn and Master Git\n\n\nForeword\n\n\nNotes.\n\n\n\n\nCONTENT\n\n\nLearn and Master Git\n\n\nCrash Course\n\n\nCourses\n\n\nGit\n\n\nReferences\n\n\n\n\n\n\n\n\n\n\n\n\nCrash Course\n\n\n\n\nGot 15 minutes and want to learn Git ?\n\n\n\n\nCourses\n\n\n\n\nGit - SVN Crash Course\n\n\nIntroduction to Git Extensions\n\n\nThe Beginner\u2019s Guide for GIT Extensions: How to use GIT to clone repository from GitHub and make changes.\n\n\nManaging a repository\n\n\nGenerating SSH Keys as one time activity.\n\n\nHow to Clone a Repository ?\n\n\nHow to open a repository ?\n\n\nHow to track the changes using Git Extensions ?\n\n\nHow to perform Commit \n Push ?\n\n\n\n\n\n\n\n\n\n\nGit, Quick Guide\n\n\nBy Tutorialspoint.\n\n\n\n\n\n\n\n\nGit\n\n\n\n\nGit, Official Website\n\n\nGood resources for learning Git and GitHub\n\n\n\n\nReferences\n\n\n\n\nGit Reference\n\n\nSimple documentation webpage.\n\n\n\n\n\n\nGitHub Guides (Official)\n\n\nUnderstanding the GitHub Flow\n\n\nHello World\n\n\nContributing to Open Source on GitHub\n\n\nGetting Started with GitHub Pages\n\n\nGetting your project on GitHub\n\n\nForking Projects\n\n\nBe Social\n\n\nMaking You Code Citable\n\n\nMastering Issues\n\n\nMastering Markdown\n\n\nDocumenting your projects on GitHub", 
            "title": "Learn Git"
        }, 
        {
            "location": "/Learn Git/#crash-course", 
            "text": "Got 15 minutes and want to learn Git ?", 
            "title": "Crash Course"
        }, 
        {
            "location": "/Learn Git/#courses", 
            "text": "Git - SVN Crash Course  Introduction to Git Extensions  The Beginner\u2019s Guide for GIT Extensions: How to use GIT to clone repository from GitHub and make changes.  Managing a repository  Generating SSH Keys as one time activity.  How to Clone a Repository ?  How to open a repository ?  How to track the changes using Git Extensions ?  How to perform Commit   Push ?      Git, Quick Guide  By Tutorialspoint.", 
            "title": "Courses"
        }, 
        {
            "location": "/Learn Git/#git", 
            "text": "Git, Official Website  Good resources for learning Git and GitHub", 
            "title": "Git"
        }, 
        {
            "location": "/Learn Git/#references", 
            "text": "Git Reference  Simple documentation webpage.    GitHub Guides (Official)  Understanding the GitHub Flow  Hello World  Contributing to Open Source on GitHub  Getting Started with GitHub Pages  Getting your project on GitHub  Forking Projects  Be Social  Making You Code Citable  Mastering Issues  Mastering Markdown  Documenting your projects on GitHub", 
            "title": "References"
        }, 
        {
            "location": "/Resources for Data Science/", 
            "text": "Resources for Data Science\n\n\nForeword\n\n\nNotes.\n\n\n\n\nCONTENT\n\n\nResources for Data Science\n\n\nData (Open)\n\n\nBoardgame Data\n\n\n\n\n\n\nData Mining, Data Wrangling, and Data Munging\n\n\nData Visualization and Storytelling\n\n\nGIS \n Mapping\n\n\nGUI \n Interfaces\n\n\nImage Processing\n\n\nInfographics\n\n\nKids and Coding\n\n\nNLP\n\n\nOnline Programs, Courses, Lessons, and Tutorials\n\n\nOnline Reporting \n Publishing\n\n\nParallel and Distributed Computing (Big Data)\n\n\nPython Web Frameworks\n\n\nStatistics, Statistical \n Machine Learning\n\n\nVirtual Console and Virtual Coding\n\n\nVisualizing and Inspecting the Code\n\n\n\n\n\n\n\n\n\n\n\n\nData (Open)\n\n\n\n\nDonn\u00e9es Qu\u00e9bec\n\n\nOpen Data Canada\n\n\nOpen Data US\n\n\nMondo\n\n\nData hub.\n\n\n\n\n\n\nThe Dataverse Project\n\n\nOpen source research data repository software.\n\n\n\n\n\n\nGapminder\n\n\nQuandl\n\n\nHigh-quality financial and economic data in many formats.\n\n\n\n\n\n\nFind the Data\n\n\nFindTheData is a reference site that uses Graphiq\u2019s semantic technology to deliver deep insights via data-driven articles, visualizations and research tools.\n\n\n\n\n\n\nKnoema\n\n\nRecherche intelligente avec toutes les statistiques entre vos mains\n\n\n\n\n\n\n\n\nBoardgame Data\n\n\n\n\nBGG Data Mining, Data Science Group\n\n\nBGG Geek Tools\n\n\n\n\nData Mining, Data Wrangling, and Data Munging\n\n\n\n\nR and Data Mining\n\n\nLinks.\n\n\n\n\n\n\nAn Introduction to Data Mining\n\n\nIntro course.\n\n\n\n\n\n\nimport.io\n\n\nExtract web data the easy way.\n\n\n\n\n\n\nThe Data Mining Page\n\n\nLinks.\n\n\n\n\n\n\nScraPy\n\n\nScrape web sites to get information off them. \n\n\nAn open source and collaborative framework for extracting the data you need from websites. In a fast, simple, yet extensible way.\n\n\n\n\n\n\n\n\nData Visualization and Storytelling\n\n\n\n\nData Visualization with JavaScript\n\n\nD3.js Gallery\n\n\n\n\n\n\nR ggplot2 package\n\n\nR ggvis package\n\n\nR shiny package\n\n\nDemo\n\n\nGallery\n\n\n\n\n\n\nGephi\n\n\nVisualization and exploration software for all kinds of graphs and networks. Gephi is open-source and free.\n\n\n\n\n\n\nKaty B\u00f6rner\n\n\nDesislava Hristova\n\n\nLAB1100.com\n\n\nIndependent research and software development firm. We built high quality applications driven by research questions, educational challenges and cultural encounters.\n\n\nNODEGOAT\n\n\n\n\n\n\nParaView\n\n\nOpen-source, multi-platform data analysis and visualization application. \n\n\n\n\n\n\nPeriscopic\n\n\nTechnology to visualize solutions that engage the public and deliver messages of action. \n\n\n\n\n\n\nplotly\n\n\nPlatform for agile business intelligence and data science.\n\n\n\n\n\n\nMicroStrategy.com\n\n\nBuild dashboards.\n\n\n\n\n\n\nSAP Lumira\n\n\nTake control and connect to multiple data sources, big and small, such as SAP HANA, SAP Business Warehouse, Excel spreadsheets and more.\n\n\n\n\n\n\nWeave\n\n\nWeb-based analysis and visualization environment designed to enable visualization of any available data by anyone for any purpose.\n\n\n\n\n\n\nBlocks (examples)\n\n\nD3.js gallery.\n\n\n\n\n\n\nBokeh\n\n\nPython package for interactive and web-based data visualization.\n\n\nCallbacks\n\n\n\n\n\n\nPanda3D\n\n\nPanda3D is a game engine, a framework for 3D rendering and game development for Python and C++ programs. Panda3D is Open Source and free for any purpose, including commercial ventures, thanks to its liberal license.\n\n\n\n\n\n\nRen\nPy\n\n\nRen\nPy is a visual novel engine used by hundreds of creators from around the world that helps you use words, images, and sounds to tell interactive stories that run on computers and mobile devices.\n\n\n\n\n\n\n\n\nGIS \n Mapping\n\n\n\n\nA Free and Open Source Geographic Information System \n\n\nCARTO\n\n\nCARTO is an open, powerful, and intuitive platform for discovering and predicting the key insights underlying the location data in our world.\n\n\n\n\n\n\n\n\nGUI \n Interfaces\n\n\n\n\nKivy\n\n\nOpen source Python library for rapid development of applications that make use of innovative user interfaces, such as multi-touch apps.\n\n\n\n\n\n\n\n\nImage Processing\n\n\n\n\nSimpleCV\n\n\nComputer Vision platform using Python. \n\n\nMaking your computer see things in the real world.\n\n\nSimpleCV is an open source framework for building computer vision applications. With it, you get access to several high-powered computer vision libraries, such as OpenCV, without having to first learn about bit depths, file formats, color spaces, buffer management, eigenvalues, or matrix versus bitmap storage.\n\n\nBook : Practical Computer Vision, O\u2019Reilly.\n\n\n\n\n\n\n\n\nInfographics\n\n\n\n\nPiktochart\n\n\nInfographic maker.\n\n\n\n\n\n\nCanvas\n\n\nEasy, drag-and-drop infographic creator.\n\n\n\n\n\n\nVizualize.me\n\n\nCreate your infographic resume for free.\n\n\n\n\n\n\nGoogle Charts\n\n\nGoogle chart tools are powerful, simple to use, and free;  rich gallery of interactive charts and data tools. \n\n\n\n\n\n\neasel.ly\n\n\nCr\u00e9er et partager des id\u00e9es de visuels .\n\n\n\n\n\n\ninfogr.am\n\n\nCreate and publish beautiful visualizations of your data. Interactive, responsive and engaging.\n\n\n\n\n\n\nVenngage\n\n\nverything you need to create and publish infographics is right here.\n\n\n\n\n\n\n\n\nKids and Coding\n\n\n\n\nCodecombat\n\n\nMulti-language.\n\n\nEducative.\n\n\nPython and web languages for creating games.\n\n\n\n\n\n\nLifelong Kindergarten\n\n\nMIT.\n\n\n\n\n\n\nScratch\n\n\nIntroduction to programming.\n\n\n\n\n\n\nRobotique\n\n\nFestival.\n\n\n\n\n\n\nHabiloM\u00e9dias\n\n\nDigital literacy.\n\n\n\n\n\n\nClassCraft\n\n\nFor schools.\n\n\n\n\n\n\n\n\nNLP\n\n\n\n\nNLTK\n\n\nNatural Language Tool Kit for analyzing written text and writing things like spam filters and chat bots. NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n\n\n\n\n\n\n\n\nOnline Programs, Courses, Lessons, and Tutorials\n\n\n\n\nThinkful\n\n\nPrograms: web development bootcamp, frontend, backend (Flask Python-based frameworks), mobile, design, UX, data science, individual, corporate.\n\n\n\n\n\n\nFreecodecamp\n\n\n4 programs: web development, front-end, back-end, full stack, data visualization.\n\n\n\n\n\n\nCode School\n\n\nPaths and courses: web and mobile development, Django Python-based frameworks, git, bash, regex, R, golang.\n\n\n\n\n\n\nCodecademy\n\n\nPaths and courses: web and mobile development, git, SQL, Java, Python.\n\n\n\n\n\n\nCoursera\n\n\nMOOC.\n\n\n\n\n\n\nSkilledup\n\n\neLearning for business.\n\n\n\n\n\n\nedX\n\n\nMOOC.\n\n\n\n\n\n\nvideo2brain\n\n\neLearning.\n\n\n\n\n\n\nnopaymba\n\n\neLearning.\n\n\n\n\n\n\nUdemy\n\n\neLearning.\n\n\n\n\n\n\nWiley Online Training\n\n\neLearning. \n\n\nCertifications.\n\n\n\n\n\n\nSpringboard\n\n\nPrograms: data science, data analytics, UX design.\n\n\n\n\n\n\nUdacity\n\n\nPrograms and courses: vr, self-driving car, predictive analytics, Android and iOS development, machine learning, programming, front-end developper, full-stack developer, data analysis.\n\n\n\n\n\n\nSQL Teaching\n\n\nSQL lessons.\n\n\n\n\n\n\nw3scholls\n\n\nTutorials: web development.\n\n\n\n\n\n\nMongoDB University\n\n\nCourses.\n\n\n\n\n\n\n\n\nStatistics.com\n\n\n\n\nPrograms and courses. \n\n\nCerfications.\n\n\n\n\n\n\n\n\nTutorialspoint\n\n\n\n\nAll topics.\n\n\n\n\n\n\nTizag\n\n\nCoding.\n\n\n\n\n\n\nLearnshell\n\n\n10 languages.\n\n\n\n\n\n\nLearn Python and\n\n\n8 other languages.\n\n\n\n\n\n\nList of Bootcamps\n\n\nList of Bootcamps\n\n\nGeneral Assembly\n\n\n\n\n\n\n\n\nOnline Reporting \n Publishing\n\n\n\n\nRPubs\n\n\nWrite R Markdown documents in RStudio. Share them on RPubs.\n\n\n\n\n\n\nbl.ocks.org\n\n\nSimple viewer for sharing code examples hosted on GitHub Gist.  \n\n\n\n\n\n\nGist GitHub\n\n\nCode snippets.\n\n\n\n\n\n\nBitbucket\n.\n\n\nGitHub\n.\n\n\nGitLab\n\n\nSourceforge\n\n\n\n\nParallel and Distributed Computing (Big Data)\n\n\n\n\nBases de donn\u00e9es documentaires et distribu\u00e9es\n\n\nParallel computing, NoSQL, XML, MapReduce, distributed computing, indexation, Eleasticsearch, JSON, CouchBase, Pig, Spark, MongoDB, replication, scalability, cloud, virtual machine, markov chaine, page rank, Solr, CouchDB, XQuery, \n\n\n\n\n\n\nCloudera\n\n\nHadoop ecosystem distribution.\n\n\n\n\n\n\nHortonworks\n\n\nHadoop ecosystem distribution.\n\n\n\n\n\n\n\n\nPython Web Frameworks\n\n\n\n\nDjango vs Flask vs Pyramid: Choosing a Python Web Framework\n\n\n\n\nStatistics, Statistical \n Machine Learning\n\n\n\n\nCours de programmation sous R\n\n\nLinks,\n\n\n\n\n\n\nStatistical Textbook\n\n\nBasic and advanced concepts.\n\n\n\n\n\n\nAn Introduction to Statistical Learning with Applications in R\n\n\nCases, codes, datasets.\n\n\n\n\n\n\nStatistical Consultants\n\n\nBlog, datasets, cases, codes.\n\n\n\n\n\n\nIntroduction to Stats and Data Analysis\n\n\nVideo courses.\n\n\n\n\n\n\nScikit\n\n\nSciKit-Learn for machine learning applications. \n\n\nSimple and efficient tools for data mining and data analysis.  \n\n\nAccessible to everybody, and reusable in various contexts. \n\n\nBuilt on NumPy, SciPy, and matplotlib. \n\n\nOpen source, commercially usable. BSD license. \n\n\n\n\n\n\nR Documentation\n\n\nSearch all CRAN, BioConductor and Github packages.\n\n\n\n\n\n\n\n\nVirtual Console and Virtual Coding\n\n\n\n\nrepl.it\n\n\nEverything you need to teach coding in your classroom.\n\n\nCode in the cloud and interactive environment. \n\n\n30 languages.\n\n\n\n\n\n\nCodeanywhere\n\n\nCross platform cloud IDE.\n\n\n\n\n\n\nR-Fiddle\n\n\nEnvironment to write, run and share R-code right inside your browser. \n\n\nIt even offers the option to include packages. \n\n\n\n\n\n\ndataiku\n\n\nA collaborative data science platform\n\n\n\n\n\n\nHeroku\n\n\nPlatform for building with modern architectures.\n\n\nInnovating quickly and scaling precisely to meet demand.\n\n\n\n\n\n\n\n\nVisualizing and Inspecting the Code\n\n\n\n\nPython Tutor\n\n\nOnline; Python, Java, JavaScript, TypeScript, Ruby, C, C++.\n\n\n\n\n\n\nPyDesk Visualizer\n\n\nPyDesk Visualizer is the desktop based Python Visualizer. It helps you to visualize Python Code. So one can easily understand execution of program.\n\n\n\n\n\n\nbl.ocks.org\n\n\nSimple viewer for sharing code examples hosted on GitHub Gist.", 
            "title": "Resources for Data Science"
        }, 
        {
            "location": "/Resources for Data Science/#data-open", 
            "text": "Donn\u00e9es Qu\u00e9bec  Open Data Canada  Open Data US  Mondo  Data hub.    The Dataverse Project  Open source research data repository software.    Gapminder  Quandl  High-quality financial and economic data in many formats.    Find the Data  FindTheData is a reference site that uses Graphiq\u2019s semantic technology to deliver deep insights via data-driven articles, visualizations and research tools.    Knoema  Recherche intelligente avec toutes les statistiques entre vos mains", 
            "title": "Data (Open)"
        }, 
        {
            "location": "/Resources for Data Science/#boardgame-data", 
            "text": "BGG Data Mining, Data Science Group  BGG Geek Tools", 
            "title": "Boardgame Data"
        }, 
        {
            "location": "/Resources for Data Science/#data-mining-data-wrangling-and-data-munging", 
            "text": "R and Data Mining  Links.    An Introduction to Data Mining  Intro course.    import.io  Extract web data the easy way.    The Data Mining Page  Links.    ScraPy  Scrape web sites to get information off them.   An open source and collaborative framework for extracting the data you need from websites. In a fast, simple, yet extensible way.", 
            "title": "Data Mining, Data Wrangling, and Data Munging"
        }, 
        {
            "location": "/Resources for Data Science/#data-visualization-and-storytelling", 
            "text": "Data Visualization with JavaScript  D3.js Gallery    R ggplot2 package  R ggvis package  R shiny package  Demo  Gallery    Gephi  Visualization and exploration software for all kinds of graphs and networks. Gephi is open-source and free.    Katy B\u00f6rner  Desislava Hristova  LAB1100.com  Independent research and software development firm. We built high quality applications driven by research questions, educational challenges and cultural encounters.  NODEGOAT    ParaView  Open-source, multi-platform data analysis and visualization application.     Periscopic  Technology to visualize solutions that engage the public and deliver messages of action.     plotly  Platform for agile business intelligence and data science.    MicroStrategy.com  Build dashboards.    SAP Lumira  Take control and connect to multiple data sources, big and small, such as SAP HANA, SAP Business Warehouse, Excel spreadsheets and more.    Weave  Web-based analysis and visualization environment designed to enable visualization of any available data by anyone for any purpose.    Blocks (examples)  D3.js gallery.    Bokeh  Python package for interactive and web-based data visualization.  Callbacks    Panda3D  Panda3D is a game engine, a framework for 3D rendering and game development for Python and C++ programs. Panda3D is Open Source and free for any purpose, including commercial ventures, thanks to its liberal license.    Ren Py  Ren Py is a visual novel engine used by hundreds of creators from around the world that helps you use words, images, and sounds to tell interactive stories that run on computers and mobile devices.", 
            "title": "Data Visualization and Storytelling"
        }, 
        {
            "location": "/Resources for Data Science/#gis-mapping", 
            "text": "A Free and Open Source Geographic Information System   CARTO  CARTO is an open, powerful, and intuitive platform for discovering and predicting the key insights underlying the location data in our world.", 
            "title": "GIS &amp; Mapping"
        }, 
        {
            "location": "/Resources for Data Science/#gui-interfaces", 
            "text": "Kivy  Open source Python library for rapid development of applications that make use of innovative user interfaces, such as multi-touch apps.", 
            "title": "GUI &amp; Interfaces"
        }, 
        {
            "location": "/Resources for Data Science/#image-processing", 
            "text": "SimpleCV  Computer Vision platform using Python.   Making your computer see things in the real world.  SimpleCV is an open source framework for building computer vision applications. With it, you get access to several high-powered computer vision libraries, such as OpenCV, without having to first learn about bit depths, file formats, color spaces, buffer management, eigenvalues, or matrix versus bitmap storage.  Book : Practical Computer Vision, O\u2019Reilly.", 
            "title": "Image Processing"
        }, 
        {
            "location": "/Resources for Data Science/#infographics", 
            "text": "Piktochart  Infographic maker.    Canvas  Easy, drag-and-drop infographic creator.    Vizualize.me  Create your infographic resume for free.    Google Charts  Google chart tools are powerful, simple to use, and free;  rich gallery of interactive charts and data tools.     easel.ly  Cr\u00e9er et partager des id\u00e9es de visuels .    infogr.am  Create and publish beautiful visualizations of your data. Interactive, responsive and engaging.    Venngage  verything you need to create and publish infographics is right here.", 
            "title": "Infographics"
        }, 
        {
            "location": "/Resources for Data Science/#kids-and-coding", 
            "text": "Codecombat  Multi-language.  Educative.  Python and web languages for creating games.    Lifelong Kindergarten  MIT.    Scratch  Introduction to programming.    Robotique  Festival.    HabiloM\u00e9dias  Digital literacy.    ClassCraft  For schools.", 
            "title": "Kids and Coding"
        }, 
        {
            "location": "/Resources for Data Science/#nlp", 
            "text": "NLTK  Natural Language Tool Kit for analyzing written text and writing things like spam filters and chat bots. NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.", 
            "title": "NLP"
        }, 
        {
            "location": "/Resources for Data Science/#online-programs-courses-lessons-and-tutorials", 
            "text": "Thinkful  Programs: web development bootcamp, frontend, backend (Flask Python-based frameworks), mobile, design, UX, data science, individual, corporate.    Freecodecamp  4 programs: web development, front-end, back-end, full stack, data visualization.    Code School  Paths and courses: web and mobile development, Django Python-based frameworks, git, bash, regex, R, golang.    Codecademy  Paths and courses: web and mobile development, git, SQL, Java, Python.    Coursera  MOOC.    Skilledup  eLearning for business.    edX  MOOC.    video2brain  eLearning.    nopaymba  eLearning.    Udemy  eLearning.    Wiley Online Training  eLearning.   Certifications.    Springboard  Programs: data science, data analytics, UX design.    Udacity  Programs and courses: vr, self-driving car, predictive analytics, Android and iOS development, machine learning, programming, front-end developper, full-stack developer, data analysis.    SQL Teaching  SQL lessons.    w3scholls  Tutorials: web development.    MongoDB University  Courses.     Statistics.com   Programs and courses.   Cerfications.     Tutorialspoint   All topics.    Tizag  Coding.    Learnshell  10 languages.    Learn Python and  8 other languages.    List of Bootcamps  List of Bootcamps  General Assembly", 
            "title": "Online Programs, Courses, Lessons, and Tutorials"
        }, 
        {
            "location": "/Resources for Data Science/#online-reporting-publishing", 
            "text": "RPubs  Write R Markdown documents in RStudio. Share them on RPubs.    bl.ocks.org  Simple viewer for sharing code examples hosted on GitHub Gist.      Gist GitHub  Code snippets.    Bitbucket .  GitHub .  GitLab  Sourceforge", 
            "title": "Online Reporting &amp; Publishing"
        }, 
        {
            "location": "/Resources for Data Science/#parallel-and-distributed-computing-big-data", 
            "text": "Bases de donn\u00e9es documentaires et distribu\u00e9es  Parallel computing, NoSQL, XML, MapReduce, distributed computing, indexation, Eleasticsearch, JSON, CouchBase, Pig, Spark, MongoDB, replication, scalability, cloud, virtual machine, markov chaine, page rank, Solr, CouchDB, XQuery,     Cloudera  Hadoop ecosystem distribution.    Hortonworks  Hadoop ecosystem distribution.", 
            "title": "Parallel and Distributed Computing (Big Data)"
        }, 
        {
            "location": "/Resources for Data Science/#python-web-frameworks", 
            "text": "Django vs Flask vs Pyramid: Choosing a Python Web Framework", 
            "title": "Python Web Frameworks"
        }, 
        {
            "location": "/Resources for Data Science/#statistics-statistical-machine-learning", 
            "text": "Cours de programmation sous R  Links,    Statistical Textbook  Basic and advanced concepts.    An Introduction to Statistical Learning with Applications in R  Cases, codes, datasets.    Statistical Consultants  Blog, datasets, cases, codes.    Introduction to Stats and Data Analysis  Video courses.    Scikit  SciKit-Learn for machine learning applications.   Simple and efficient tools for data mining and data analysis.    Accessible to everybody, and reusable in various contexts.   Built on NumPy, SciPy, and matplotlib.   Open source, commercially usable. BSD license.     R Documentation  Search all CRAN, BioConductor and Github packages.", 
            "title": "Statistics, Statistical &amp; Machine Learning"
        }, 
        {
            "location": "/Resources for Data Science/#virtual-console-and-virtual-coding", 
            "text": "repl.it  Everything you need to teach coding in your classroom.  Code in the cloud and interactive environment.   30 languages.    Codeanywhere  Cross platform cloud IDE.    R-Fiddle  Environment to write, run and share R-code right inside your browser.   It even offers the option to include packages.     dataiku  A collaborative data science platform    Heroku  Platform for building with modern architectures.  Innovating quickly and scaling precisely to meet demand.", 
            "title": "Virtual Console and Virtual Coding"
        }, 
        {
            "location": "/Resources for Data Science/#visualizing-and-inspecting-the-code", 
            "text": "Python Tutor  Online; Python, Java, JavaScript, TypeScript, Ruby, C, C++.    PyDesk Visualizer  PyDesk Visualizer is the desktop based Python Visualizer. It helps you to visualize Python Code. So one can easily understand execution of program.    bl.ocks.org  Simple viewer for sharing code examples hosted on GitHub Gist.", 
            "title": "Visualizing and Inspecting the Code"
        }, 
        {
            "location": "/Storytelling with Data/", 
            "text": "Storytelling with Data\n\n\nForeword\n\n\nNotes.\n\n\n\n\nCONTENT\n\n\nStorytelling with Data\n\n\n1. The importance of context\n\n\n2. Choosing an effective visual\n\n\n3. Clutter is your enemy !\n\n\n4. Focus your audience\u2019s attention\n\n\n5. Think like a designer\n\n\n7. Lessons in storytelling\n\n\n10. Final thoughts\n\n\nWrap up\n\n\n\n\n\n\n\n\n\n\n\n\n1. The importance of context\n\n\n\n\nExploratory vs explanatory graphics.\n\n\nExploratory is for the analyst.\n\n\nExploratory is for the audience.\n\n\n\n\n\n\nWho is the audience, who is the stakeholder?\n\n\nWhat do you need your audience to know or do?\n\n\nHow will you communicate to your audience? Live or written by email?\n\n\nWhat tone do you want your communication to set?\n\n\nWhat data is available that will help make your point?\n\n\nIf you had 3 minutes to tell a story, what would be the big idea : the unique point of view in one sentence?\n\n\nLet\ns now move to storyboarding.\n\n\n\n\n2. Choosing an effective visual\n\n\n\n\nGo with a simple text or a figure.\n\n\nAvailable charts: scatterplot, table, lines, slopegraph, vertical bars, horizontal bars, stacked vertical bars, stacked horizontal bars, waterfall, square areas.\n\n\nTable? Never in a live presentation (too dense).\n\n\nHeatmap? Never! A better alternative is a slopegraph.\n\n\nColor saturation is for visual contrast: put everything  in B\nW except the one thing you want to draw attention on.\n\n\nSpeak in relative, not in absolute: scatterplot, average, percentage, waterfall (change, increment), areas for comparing categories.\n\n\nAvoid clutter and 3D, avoid saturated series and secondary axes.\n\n\nPrefer a zero baseline and equal scales among several graphics.\n\n\nPrefer square forms over round forms.\n\n\nAvoid pie charts or donut charts.\n\n\nPrefer a horizontal bar chart.\n\n\n\n\n\n\n\n\n3. Clutter is your enemy !\n\n\n\n\nAvoid cognitive load: a simple chart, a few line, one image.\n\n\nAvoid borders (limited to the L axes), avoid gridlines and background shading. \n\n\nThe y-axis is often facultative.\n\n\nClean up axis labels; summarize labels (Jan vs January)\n\n\nBold and colors is for making the important stand out.\n\n\nLimit to 2 or 3 colors.\n\n\nAdd white space.\n\n\nAdd details to the important areas (annotate).\n\n\nAdd the legend on the graph, not outside.\n\n\n\n\n4. Focus your audience\u2019s attention\n\n\n\n\nUse preattentive attributes in text : bold, italics, separate spatially, underlined, more tone or color (B\nW), data stand out, numbered data, size.\n\n\nAvoid shades of red and green.\n\n\nUse preattentive attributes in visual : difference in orientation, shape, length, width, size, curvature, added marks, enclosure, hue, intensity, spatial position, motion\n\n\nPosition on the page follows the 1-2-3-4 loop.\n\n\nConsult:\n\n\nCultural Color Connotations, David McCandless.\n\n\nThe Visual Miscellaneum : A Colourful Guide to the World\u2019s Most Consequential Trivia.\n\n\n\n\n\n\n\n\n5. Think like a designer\n\n\n\n\nEliminate distractions.\n\n\nNot all data are equally important.\n\n\nWhen detail is not needed, summarize.\n\n\nWould eliminating \nthis\n change anything?\n\n\nPush necessary, but non-message-impacting items to the background.\n\n\nThe power of subcategories, subdivisions.\n\n\nLegible, clean, straightforward language.\n\n\nAction titles.\n\n\n\n\n7. Lessons in storytelling\n\n\n\n\nWrite a story with a schema (3 acts)\n\n\nWrite a narrative structure (hero\ns journey)\n\n\nUse the power of repetition: 3, 7, 12, 30, 40, 100, 500, 1000.\n\n\n\n\n10. Final thoughts\n\n\n\n\nLearn your tools well.\n\n\nGoogle Spreadsheet, Tableau, R, D3 (JavaScript), Processing, Python, etc.\n\n\n\n\n\n\nIterate.\n\n\nDevote time to storytelling with data.\n\n\nSeek inspiration throught good examples.\n\n\nFind your style.\n\n\nConsult websites.\n\n\nEagereyes, Robert Kosara\n\n\nFiveThirdyEight\u2019s Data Lab\n\n\nFlowing Data, Nathan Yau\n\n\nThe Functional Art, Alberto Cairo\n\n\nThe Gardian Data Blog\n\n\nHelpMeViz, Jon Schwabish\n\n\nJunk Carts, Kaiser Fung\n\n\nMake a Powerful Point, Gavin McMahon\n\n\nPerceptual Edge, Stephen Few\n\n\nVisualising Data, Andy Kirk\n\n\nVizWiz, Andy Kriebel\n\n\nStorytelling with data, Cole Nussbaumer Knaflic\n\n\nand many more.\n\n\n\n\n\n\n\n\nWrap up\n\n\n\n\nUnderstand the context.\n\n\nChoose an appropriate visual display.\n\n\nEliminate clutter.\n\n\nFocus attention where you want it.\n\n\nThink like a designer.\n\n\nTell a story.", 
            "title": "Storytelling with Data"
        }, 
        {
            "location": "/Storytelling with Data/#1-the-importance-of-context", 
            "text": "Exploratory vs explanatory graphics.  Exploratory is for the analyst.  Exploratory is for the audience.    Who is the audience, who is the stakeholder?  What do you need your audience to know or do?  How will you communicate to your audience? Live or written by email?  What tone do you want your communication to set?  What data is available that will help make your point?  If you had 3 minutes to tell a story, what would be the big idea : the unique point of view in one sentence?  Let s now move to storyboarding.", 
            "title": "1. The importance of context"
        }, 
        {
            "location": "/Storytelling with Data/#2-choosing-an-effective-visual", 
            "text": "Go with a simple text or a figure.  Available charts: scatterplot, table, lines, slopegraph, vertical bars, horizontal bars, stacked vertical bars, stacked horizontal bars, waterfall, square areas.  Table? Never in a live presentation (too dense).  Heatmap? Never! A better alternative is a slopegraph.  Color saturation is for visual contrast: put everything  in B W except the one thing you want to draw attention on.  Speak in relative, not in absolute: scatterplot, average, percentage, waterfall (change, increment), areas for comparing categories.  Avoid clutter and 3D, avoid saturated series and secondary axes.  Prefer a zero baseline and equal scales among several graphics.  Prefer square forms over round forms.  Avoid pie charts or donut charts.  Prefer a horizontal bar chart.", 
            "title": "2. Choosing an effective visual"
        }, 
        {
            "location": "/Storytelling with Data/#3-clutter-is-your-enemy", 
            "text": "Avoid cognitive load: a simple chart, a few line, one image.  Avoid borders (limited to the L axes), avoid gridlines and background shading.   The y-axis is often facultative.  Clean up axis labels; summarize labels (Jan vs January)  Bold and colors is for making the important stand out.  Limit to 2 or 3 colors.  Add white space.  Add details to the important areas (annotate).  Add the legend on the graph, not outside.", 
            "title": "3. Clutter is your enemy !"
        }, 
        {
            "location": "/Storytelling with Data/#4-focus-your-audiences-attention", 
            "text": "Use preattentive attributes in text : bold, italics, separate spatially, underlined, more tone or color (B W), data stand out, numbered data, size.  Avoid shades of red and green.  Use preattentive attributes in visual : difference in orientation, shape, length, width, size, curvature, added marks, enclosure, hue, intensity, spatial position, motion  Position on the page follows the 1-2-3-4 loop.  Consult:  Cultural Color Connotations, David McCandless.  The Visual Miscellaneum : A Colourful Guide to the World\u2019s Most Consequential Trivia.", 
            "title": "4. Focus your audience\u2019s attention"
        }, 
        {
            "location": "/Storytelling with Data/#5-think-like-a-designer", 
            "text": "Eliminate distractions.  Not all data are equally important.  When detail is not needed, summarize.  Would eliminating  this  change anything?  Push necessary, but non-message-impacting items to the background.  The power of subcategories, subdivisions.  Legible, clean, straightforward language.  Action titles.", 
            "title": "5. Think like a designer"
        }, 
        {
            "location": "/Storytelling with Data/#7-lessons-in-storytelling", 
            "text": "Write a story with a schema (3 acts)  Write a narrative structure (hero s journey)  Use the power of repetition: 3, 7, 12, 30, 40, 100, 500, 1000.", 
            "title": "7. Lessons in storytelling"
        }, 
        {
            "location": "/Storytelling with Data/#10-final-thoughts", 
            "text": "Learn your tools well.  Google Spreadsheet, Tableau, R, D3 (JavaScript), Processing, Python, etc.    Iterate.  Devote time to storytelling with data.  Seek inspiration throught good examples.  Find your style.  Consult websites.  Eagereyes, Robert Kosara  FiveThirdyEight\u2019s Data Lab  Flowing Data, Nathan Yau  The Functional Art, Alberto Cairo  The Gardian Data Blog  HelpMeViz, Jon Schwabish  Junk Carts, Kaiser Fung  Make a Powerful Point, Gavin McMahon  Perceptual Edge, Stephen Few  Visualising Data, Andy Kirk  VizWiz, Andy Kriebel  Storytelling with data, Cole Nussbaumer Knaflic  and many more.", 
            "title": "10. Final thoughts"
        }, 
        {
            "location": "/Storytelling with Data/#wrap-up", 
            "text": "Understand the context.  Choose an appropriate visual display.  Eliminate clutter.  Focus attention where you want it.  Think like a designer.  Tell a story.", 
            "title": "Wrap up"
        }, 
        {
            "location": "/Data_Storytelling/", 
            "text": "10 Kinds of Stories to Tell with Data\n\n\nForeword\n\n\nNotes.\n\n\n\n\nCONTENT\n\n\n10 Kinds of Stories to Tell with Data\n\n\nTime\n\n\nFocus\n\n\nDepth\n\n\nMethods\n\n\nAfterword\n\n\n\n\n\n\n\n\n\n\n\n\nTime\n\n\n\n\n\n\nReporting\n: perform descriptive analytics. Tell what happened.\n\n\n\n\n\n\nExplanatory survey\n: analyze what people or objects are up to. Ask people what they think about something. Conceive a statistical model; what factors drive others.\n\n\n\n\n\n\nPrediction\n: perform predictive analytics. Use historical data, add a statistical model, probabilities, and assumptions,  predict the future. Find out what customers are likely to buy. Assess how likely it is for an event to happen. Forecast economic conditions. \n\n\n\n\n\n\nFocus\n\n\n\n\n\n\nWhat\n: tell what happened with a \nfocus\n on one issue (\nReporting\n is not as focused).\n\n\n\n\n\n\nWhy\n: tell what underlying factors caused the outcome. \nFocus\n on the outcome.\n\n\n\n\n\n\nHow\n to address the issue: explore various ways to improve the situation. \nFocus\n on the situation.\n\n\n\n\n\n\nDepth\n\n\n\n\n\n\nCSI\n: run a small, ad hoc investigation. Find out why something is happening. Find out why some customer are dropping online transactions when they get to the postal code input form; what if some rural locations don\nt have postal codes?\n\n\n\n\n\n\nEureka\n: invest in long, analytically-driven searches for a solution to a complex problem. Discover the right way to refer and price potential buyers to real estate agents on a website. Since the project is core to the company\ns business model, it needs a corporate buy-in. The project involve several different analytical methods, false starts, dead-ends, discoveries.\n\n\n\n\n\n\nMethods\n\n\n\n\n\n\nCorrelations\n: find why the relationships among variables rise and fall at the same time. \n\n\n\n\n\n\nCausation\n: argue that one variable caused the other. Controlled experiment.\n\n\n\n\n\n\nAfterword\n\n\n\n\nThese 10 approaches are not mutually exclusive. \n\n\nBegin the report with the result and recommended outcome. Follow with the demonstration.\n\n\nFor the c-suite, keep technical terms to the minimum.\n\n\n\n\nSource: Harvard Business Review.", 
            "title": "10 Kinds of Stories to Tell with Data"
        }, 
        {
            "location": "/Data_Storytelling/#time", 
            "text": "Reporting : perform descriptive analytics. Tell what happened.    Explanatory survey : analyze what people or objects are up to. Ask people what they think about something. Conceive a statistical model; what factors drive others.    Prediction : perform predictive analytics. Use historical data, add a statistical model, probabilities, and assumptions,  predict the future. Find out what customers are likely to buy. Assess how likely it is for an event to happen. Forecast economic conditions.", 
            "title": "Time"
        }, 
        {
            "location": "/Data_Storytelling/#focus", 
            "text": "What : tell what happened with a  focus  on one issue ( Reporting  is not as focused).    Why : tell what underlying factors caused the outcome.  Focus  on the outcome.    How  to address the issue: explore various ways to improve the situation.  Focus  on the situation.", 
            "title": "Focus"
        }, 
        {
            "location": "/Data_Storytelling/#depth", 
            "text": "CSI : run a small, ad hoc investigation. Find out why something is happening. Find out why some customer are dropping online transactions when they get to the postal code input form; what if some rural locations don t have postal codes?    Eureka : invest in long, analytically-driven searches for a solution to a complex problem. Discover the right way to refer and price potential buyers to real estate agents on a website. Since the project is core to the company s business model, it needs a corporate buy-in. The project involve several different analytical methods, false starts, dead-ends, discoveries.", 
            "title": "Depth"
        }, 
        {
            "location": "/Data_Storytelling/#methods", 
            "text": "Correlations : find why the relationships among variables rise and fall at the same time.     Causation : argue that one variable caused the other. Controlled experiment.", 
            "title": "Methods"
        }, 
        {
            "location": "/Data_Storytelling/#afterword", 
            "text": "These 10 approaches are not mutually exclusive.   Begin the report with the result and recommended outcome. Follow with the demonstration.  For the c-suite, keep technical terms to the minimum.   Source: Harvard Business Review.", 
            "title": "Afterword"
        }, 
        {
            "location": "/Infographies/", 
            "text": "Infographies\n\n\nForeword\n\n\nNotes. An image is worth\n\n\n\n\nCONTENT\n\n\nInfographies\n\n\nBig \n Data \n Science\n\n\nData Science\n\n\n\n\n\n\n\n\n\n\n\n\nBig \n Data \n Science\n\n\nThe rise\n\n\n\n\nWhat?\n\n\n\n\nProcess\n\n\n\n\nData Science\n\n\nKnowledge base\n\n\n\n\n\n\n\n\n\n\n\n\nLearning\n\n\n\n\nTalent Big Picture\n\n\n\n\n\n\n\n\nData Scientist\n\n\nHybrid role\n\n\n\n\nCross Over to Data Science\n\n\n\n\n\n\n\n\nProgramming Languages\n\n\n\n\nData Software", 
            "title": "Infographies"
        }, 
        {
            "location": "/Infographies/#big-data-science", 
            "text": "The rise   What?   Process", 
            "title": "Big -- Data -- Science"
        }, 
        {
            "location": "/Infographies/#data-science", 
            "text": "Knowledge base       Learning   Talent Big Picture     Data Scientist  Hybrid role   Cross Over to Data Science     Programming Languages   Data Software", 
            "title": "Data Science"
        }, 
        {
            "location": "/Codecademy Learn the Command Line Notes/", 
            "text": "Codecademy Learn the Command Line\n\n\nForeword\n\n\nCommands and code snippets. From Codecademy.\n\n\n\n\nCONTENT\n\n\nCodecademy Learn the Command Line\n\n\nSection 1, Navigation\n\n\nSection 2, Manipulation\n\n\nSection 3, Redirection\n\n\nSection 4, Environment\n\n\nSection 5, Multi-Users\n\n\nSection 6, Multi-Tasks\n\n\n\n\n\n\n\n\n\n\n\n\nSection 1, Navigation\n\n\n\n\nBash means \nBourne again shell\n.\n\n\nThere exists other bash programs : ksh, tcsh, zsh, etc.\n\n\nCommon terminals: Ubuntu gnome, KDE konsole, eterm, txvt, kvt, nxterm, eterm.\n\n\nA CLI has its advantages over a GUI.\n\n\n\n\nTerminal\n\n\n\n\nbash\n; open another \nbash\n in the same bash window.\n\n\nexit\n; close one instance of \nbash\n.\n\n\nexport\n; export variable to the next bash.\n\n\nexport MESSAGE=\"Hi\"\n.\n\n\nVariables are associated with one bash. \n\n\nOne change in one bash does not affect the other bash.\n\n\n\n\n\n\nman \ncmd\n; call the manual, help about a command.\n\n\nman cp\n; open the manual about command \ncp\n (or any other command).\n\n\n\n\n\n\ncp --help\n; command \ncp\n arguments (or any other command).\n\n\ncommand cp\n; get info on command \ncp\n (or any other command).\n\n\ntype cp\n; find where command \ncp\n is located and what kind of command (an \nalias\n or \nelse\n) (or any other command).\n\n\nwhich cp\n; locate command \ncp\n (or any other command).\n\n\nwhich make\n; check out where \nmake\n is. \n\n\npwd\n; print working directory.\n\n\ndir\n; list of directory.\n\n\nup\n/\ndown\n arrow; resume a past command.\n\n\ntab\n; autocomplete.\n\n\n\n\nChange dir\n\n\n\n\ncd/aaa\n; change to directory \naaa\n.\n\n\ncd ~\n, \ncd ~user\n; go back home to \nuser\n directory.\n\n\ncd ..\n; go up one level.\n\n\ncd ../..\n; go up two levels.\n\n\ncd ../../..\n; go up three levels.\n\n\ncd temp\n; go to directory \ntemp\n.\n\n\ncd stuff\n; if we are in dir \ntemp\n, switch to directory \nstuff\n.\n\n\ncd temp/stuff\n; switch directly to directory \nstuff\n.\n\n\ncd 'en co re'\n; switch to directory \nen co re\n.\n\n\n\n\nMake dir\n\n\n\n\nmkdir aaa\n; make directory \naaa\n.\n\n\nmkdir -p aaa/bbb\n; make directory \naaa\n with sub-directory \nbbb\n.\n\n\nmkdir 'en co re'\n; make directory \nen co re\n.\n\n\n\n\nRemove dir\n\n\n\n\nrmdir aaa\n; remove directory \naaa\n.\n\n\nrmdir 'en co re'\n; remove directory \nen co re\n.\n\n\nrmdir aaa/bbb\n; remove both directory and sub-directory (if both empty).\n\n\nrmdir -p aaa/bbb\n; remove all \n(you cannot remove a directory if it is non-empty (filled with sub-directories and files))\n.\n\n\nrmdir -p aaa/*\n; remove all.\n\n\n\n\n\n\n\n\nSwitch dir\n\n\n\n\npushd\n; save current location; then, change location (another directory).\n\n\npopd\n; return to pushd. \n(for example, you are in \naaa\n, type \npushd\n, go to \nbbb\n, type \npushb\n, go to \nccc\n, type \npopd\n, return to \nbbb\n, type \npopd\n, return to \naaa\n)\n.\n\n\n\n\nMake file\n\n\n\n\ntouch iamcoo.txt\n; create an empty text file.\n\n\ntouch temp/ismvoo.txt\n; create an empty text file in the \ntemp\n directory.\n\n\n\n\nEdit file\n\n\n\n\nless test.txt\n; open the file and display its content.\n\n\nWithin \nless\n :\n\n\nq\n; quit.\n\n\narrows, \npgUp\n, \npgDn\n; move.\n\n\nh\n; help.\n\n\n\n\n\n\nFor other files extensions like \ntar\n and GNU archive, use \ntvf\n.\n\n\n\n\nSection 2, Manipulation\n\n\nList file \n dir\n\n\n\n\nls\n; list of directories and files.\n\n\nls /bin/bash\n; list of a remote directory and files.\n\n\nls -a\n; list all.\n\n\nls -t\n; list in alphanumeric order, when they were last modified.\n\n\nls -l\n; list in long format.\n\n\nls -la\n; list all in long format.\n\n\nls -alt\n; list all in long format and ordered.\n\n\nls /aaa /bbb\n; list both directories.\n\n\nls -r\n; list in reverse.\n\n\nls ../paint/\n; list an upper directory.\n\n\n\n\nCopy file \n dir\n\n\n\n\ncp aaa.txt bbb.txt\n; copy \naaa.txt\n file, paste it or create a copy named \nbbb.txt\n.\n\n\ncp aaa.txt dir/\n; copy \naaa.txt\n into directory \ndir\n.\n\n\ncp aaa.txt bbb.txt dir/\n; copy \naaa.txt\n and \nbbb.txt\ninto directory \ndir\n.\n\n\ncp \\*.txt dir/\n; copy all \n.txt\n files into directory \ndir\n.\n\n\ncp aaa.txt dir/dir2\n; copy \naaa.txt\n into sub-directory \ndir2\n.\n\n\ncp -r aaa bbb\n; copy \ndir1\n, create a copy names \ndir2\n with the exact same content.\n\n\ncp -i\n; interactive, to prompt the user.\n\n\ncp f\\* ../paint/\n; copy all files beginning with \nf\n to an upper directory.\n\n\n\n\nWildcards\n\n\n\n\n\\*\n; wildcard for any string.\n\n\n?\n; wildcard for any character.\n\n\n\\*.txt\n; all files finishing with \n.txt\n.\n\n\nr\\*\n; all files beginning with \nr\n.\n\n\n??a.txt\n; all files beginning with two characters + \na.txt\n.\n\n\nbackup[[:digit:]]\n; all file beginning with \nbackup\n + any digit.\n\n\n[abc]\\*\n; all files beginning with either \na\n, \nb\n or \nc\n\n\n[[:upper:]]\\*\n; all files beginning with an upper case.\n\n\n\\*[![:lower:]]\n; all file not finishing with a lower case.\n\n\n\n\nMove file \n dir\n\n\n\n\nmv aaa.txt bbb.txt\n; move file or cut \naaa.txt\n and paste \nbbb.txt\n.\n\n\nmv aaa.txt dir/\n; move file \naaa.txt\n into directory \ndir\n.\n\n\nmv aaa.txt bbb.txt dir/\n; move files \naaa.txt\n and \nbbb.txt\n into directory \ndir\n.\n\n\nmv \\*.txt dir/\n; move all \n.txt\n files into directory \ndir\n.\n\n\nmv aaa.txt dir/dir2\n; move file \naaa.txt\n into sub-directory \ndir2\n.\n\n\nmv -i\n; interactive, prompt the user.\n\n\n\n\nRemove file \n dir\n\n\n\n\nrm aaa.txt\n; remove file \naaa.txt\n.\n\n\nrm test1.txt test2.txt\n; remove both files.\n\n\nrm aaa/\\*\n; remove all files in the directory \naaa\n.\n\n\nrm -r aaa\n; remove directory \naaa\n, must be empty.\n\n\nrm -rf aaa\n; remove directory \naaa\n and its files.\n\n\nrm -i\n; interactive, prompt the user.\n\n\n\n\nSection 3, Redirection\n\n\n\n\nLet\ns begin by taking a closer look at input and output. In the terminal, after the shell prompt, type :\n\n\n\n\n$ echo 'Hello'\n\n\n\n\n\n\nThe \necho\n command accepts the string \nHello\n as standard input, and echoes the string \n'Hello'\n back to the terminal as standard output. \n\n\nstandard input, abbreviated as \nstdin\n, is information inputted into the terminal through the keyboard or input device.\n\n\nstandard output, abbreviated as \nstdout\n, is the information outputted after a process is run.\n\n\nstandard error, abbreviated as \nstderr\n, is an error message outputted by a failed process.\n\n\n\n\n\n\nRedirection (\n) reroutes standard input, standard output, and standard error to or from a different location. Type :\n\n\n\n\n$ echo 'Hello' \n hello.txt\n\n\n\n\n\n\nType :\n\n\n\n\n$ cat hello.txt\n\n\n\n\n\n\nThe \n command redirects the standard output to a file. The standard output \n'Hello'\n is redirected by \n to the file \nhello.txt\n, and entered as the standard input.\n\n\n\n\nThe cat command outputs the contents of a file to the terminal. When you type \ncat hello.txt\n, the contents of \nhello.txt\n are displayed.\n\n\n\n\n\n\nType :\n\n\n\n\n\n\n$ ls -l\n\n\n\n\n\n\nThis is the filesystem we\nll work with. Create a file. Type :\n\n\n\n\n$ touch ocean.txt\n\n\n\n\n\n\nFill the file with values (ocean names). Type :\n\n\n\n\n$ cat oceans.txt \n continents.txt\n\n\n\n\n\n\nUse \ncat\n to view the contents of \ncontinents.txt\n. Notice that we only see oceans as output:\n\n\n\n\n$ cat continents.txt\n\n\n\n\n\n\n takes the standard output of the command on the left, and redirects it to the file on the right. Here the standard output of cat \noceans.txt\n is redirected to \ncontinents.txt\n. Note that \n OVERWRITES all original content in \ncontinents.txt\n. When you view the output data by typing cat on \ncontinents.txt\n, you will see only the contents of \noceans.txt\n. Type :\n\n\n\n\n$ cat glaciers.txt \n rivers.txt\n\n\n\n\n\n\nUse \ncat\n to view the contents of \nrivers.txt\n:\n\n\n\n\n$ cat rivers.txt\n\n\n\n\n\n\nNotice that we see both rivers and glaciers as output. Type:\n\n\n\n\n$ cat glaciers.txt \n rivers.txt\n\n\n\n\n\n\n\n\n takes the standard output of the command on the left and APPENDS it to the file on the right. You can view the output data of the file with cat and the filename. Here, the output data of \nrivers.txt\n will contain the original contents of \nrivers.txt\n with the content of \nglaciers.txt\n appended to it. \n\n\n\n\n\n\nType :\n\n\n\n\n\n\n$ touch lakes.txt\n\n\n\n\n\n\nFill the files with values (lake names). Type :\n\n\n\n\n$ cat \n lakes.txt\n\n\n\n\n\n\n takes the standard input from the file on the right and inputs it into the program on the left. Here, \nlakes.txt\n is the standard input for the \ncat\n command. The standard output appears in the terminal. Let\ns try some more redirection commands. Type :\n\n\n\n\n$ touch volcanoes.txt\n\n\n\n\n\n\nFill the files with values (volcano names). Type :\n\n\n\n\n$ cat volcanoes.txt | wc\n\n\n\n\n\n\nYou get the count for: lines, words, bytes. Type :\n\n\n\n\n$ cat volcanoes.txt | wc | cat \n islands.txt\n\n\n\n\n\n\nUse \ncat\n to output the contents in \nislands.txt\n. The next command should now equals \n$ cat volcanoes.txt | wc\n.\n\n\n\n\n$ cat volcanoes.txt | wc\n\n\n\n\n\n\n|\n is a \npipe\n or \npipeline\n The \n|\n takes the standard output of the command on the left, and PIPES it as standard input to the command on the right. You can think of this as \ncommand to command\n redirection. Here, again, the output of cat \nvolcanoes.txt\n is the standard input of \nwc\n. In turn, the \nwc\n command outputs the number of lines, words, and characters in \nvolcanoes.txt\n, respectively:\n\n\n\n\n$ cat volcanoes.txt | wc | cat \n islands.txt\n$ less islands.txt\n\n\n\n\n\n\nMultiple \n|\ns can be chained together. Here the standard output of cat \nvolcanoes.txt\n is \npiped\n to the \nwc\n command. The standard output of \nwc\n is then \npiped\n to cat. Finally, the standard output of cat is redirected to \nislands.txt\n. You can view the output data of this chain by typing cat \nislands.txt\n. \n\n\nA few commands are particularly powerful when combined with redirection. Let\ns try them out. First, use \ncat\n to output the contents of \nlakes.txt\n. Then, \nsort\n it. Type :\n\n\n\n\n$ cat lakes.txt\n$ sort lakes.txt\n\n\n\n\n\n\nThe lakes in \nlakes.txt\n are listed in alphabetical order. Type:\n\n\n\n\n$ cat lakes.txt | sort\n\n\n\n\n\n\nsort\n takes the standard input and orders it alphabetically for the standard output. The lakes in \nlakes.txt\n are listed in alphabetical order.\n\n\nUse cat to output the contents of \nsorted-lakes.txt\n.\n\n\n\n\n$ cat lakes.txt | sort \n sorted-lakes.txt\n\n\n\n\n\n\nThe command takes the standard output from cat \nlakes.txt\n and \npipes\n it to sort in ascending order. The standard output of \nsort\n is redirected to \nsorted-lakes.txt\n. You can view the output data by typing:\n\n\n\n\n$ cat sorted-lakes.txt\n\n\n\n\n\n\nType :\n\n\n\n\n$ touch deserts.txt\n\n\n\n\n\n\nFill the file with values. Type :\n\n\n\n\n$ cat deserts.txt\n$ uniq deserts.txt\n\n\n\n\n\n\n\n\nYou get to see all entries and unique entries.\n\n\n\n\n\n\nType :\n\n\n\n\n\n\n$ sort deserts.txt | uniq\n\n\n\n\n\n\nYou get to see  unique entries sorted. Type :\n\n\n\n\n$ sort deserts.txt | uniq \n uniq-deserts.txt\n\n\n\n\n\n\nYou save the result in \nuniq-deserts.txt\n.\n\n\nUse \ncat\n to output the contents of \nuniq-deserts.txt\n.\n\n\nuniq\n stands for \nunique\n and filters out ADJACENT, duplicate lines in a file. Here \nuniq deserts.txt\n filters out duplicates of \nSahara Desert\n, because the duplicate of \nSahara Desert\n directly follows the previous instance. The \nKalahari Desert\n duplicates are not adjacent, and thus remain.\n\n\nType :\n\n\n\n\n$ grep Mount mountains.txt\n\n\n\n\n\n\ngrep\n stands for \nglobal regular expression print\n. It searches files for lines that match a pattern and returns the results. It is also case sensitive. Here, \ngrep\n searches for \nMount\n in \nmountains.txt\n. Type:\n\n\n\n\n$ grep -i Mount mountains.txt\n\n\n\n\n\n\ngrep -i\n enables the command to be case insensitive. Here, \ngrep\n searches for capital or lowercase strings that match \nMount\n in \nmountains.txt\n. The above commands are a great way to get started with \ngrep\n. If you are familiar with regular expressions, you can use regular expressions to search for patterns in files. \ngrep\n can also be used to search within a directory. Type :\n\n\n\n\n$ grep -R Arctic /home/ccuser/workspace/geography\n\n\n\n\n\n\nype :\n\n\n\n\n$ grep -R Arctic /home/ccuser/workspace/geography\n\n\n\n\n\n\ngrep -R\n searches files in a directory and outputs filenames and lines containing matched results. \n-R\n stands for \nrecursive\n. Here \ngrep -R\n searches the \n/home/ccuser/workspace/geography\n directory for the string \nArctic\n and outputs filenames and lines with matched results. Type:\n\n\n\n\n$ grep -R Gambino .\n\n\n\n\n\n\ngrep -R\n searches ALL files recursively and outputs filenames and lines containing matched results.\n\n\n\n\n$ grep -Rl Arctic /home/ccuser/workspace/geography\n\n\n\n\n\n\ngrep -Rl\n searches all files in a directory and outputs only filenames with matched results. \n-R\n stands for \nrecursive\n and \nl\n stands for \nfiles with matches\n. Here \ngrep -Rl\n searches the \n/home/ccuser/workspace/geography\n directory for the string \nArctic\n and outputs filenames with matched results. Use \ncat\n to display the contents of \nforests.txt\n. \n\n\ngrep this file.txt\n; search for \nthis\n in \nfile.txt,\n print the output on screen.\n\n\ngrep this \n file.txt\n; feed \nfile.txt\n to process \ngrep\n to look for \nthis\n, print on screen\n\n\ngrep this file.txt \n file_this.txt\n; write, overwrite the output in \nfile_this.txt\n.\n\n\ngrep this file.txt \n file_this.txt\n; write, append the output in \nfile_this.txt\n.\n\n\ngrep \"is\" file.txt\n; search pattern \nis\n, precisely, in words or alone.\n\n\ngrep line file.txt\n; search pattern \nline\n.\n\n\ngrep -n line file.txt\n; show the line number where it finds pattern \nline\n.\n\n\ngrep -i line file.txt\n; case insensitive\n\n\n\n\ngrep -v line file.txt\n; inverse or when it does not have the pattern line\n\n\n\n\n\n\nIf you search for one \nfile\n in the current directory, type:\n\n\n\n\n\n\nfind . _name \nfile.txt\n\n\n\n\n\n\n\nfind / _name \"file.txt\"\n searches \nfile\n in multiple directories.\n\n\nfind dirA dirB dirC _name \"file.txt\"\n search \nfile\n in one or more directories. Type :\n\n\n\n\n$ sed 's/snow/rain/' forests.txt\n\n\n\n\n\n\nsed\n stands for \nstream editor\n. It accepts standard input and modifies it based on an expression, before displaying it as output data. It is similar to \nfind and replace\n. \n\n\nLet\ns look at the expression \n's/snow/rain/'\n:\n\n\ns\n: stands for \nsubstitution\n. it is always used when using sed for substitution.\n\n\nsnow\n: the search string, the text to find.\n\n\nrain\n: the replacement string, the text to add in place.\n\n\n\n\n\n\nIn this case, \nsed\n searches \nforests.txt\n for the word \n'snow'\n and replaces it with \n'rain'\n. Importantly, the above command will only replace the FIRST instance of \n'snow'\n on a line.\n\n\n\n\n$ sed 's/snow/rain/g' forests.txt\n\n\n\n\n\n\n\n\nThe above command uses the \ng\n expression, meaning \nglobal\n. Here \nsed\n searches \nforests.txt\n for the word \n'snow'\n and replaces it with \n'rain'\n, globally. ALL instances of \n'snow'\n on a line will be turned to \n'rain'\n.\n\n\n\n\n\n\nLet\ns summarize what we\nve done so far.\n\n\n\n\nThe common redirection commands are:\n\n\n; redirects standard output of a command to a file, overwriting previous content.\n\n\n; redirects standard output of a command to a file, appending new content to old content.\n\n\n; redirects standard input to a command.\n\n\n|\n; redirects standard output of a command to another command.\n\n\n\n\n\n\nA number of other commands are powerful when combined with redirection commands:\n\n\necho 'a'\n; display \na\n in the terminal.\n\n\ntouch\n; create a file.\n\n\ncat\n; display the content of a file in the terminal.\n\n\nless\n; edit the content of a file in the terminal; prefer \nnano\n, it\ns more user-friendly.\n\n\nsort\n; sorts lines in a file in alphabetical order.\n\n\nuniq\n; filters duplicates in a file.\n\n\ngrep\n; searches for a text pattern in files and outputs it.\n\n\nfind\n; searches for files, not the content.\n\n\nsed\n; searches for a text pattern in files, modifies it, and outputs it.\n\n\nhead\n/\ntail\n; \nls\n, but only the top/bottom results.\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\necho 'thisthat'\n; show \nthisthat\n.\n\n\necho $USER\n; show user variable.\n\n\necho \\*\n, \nd\\*\n, \ns\\*\n, \n[[:upper]]\\*\n, \n/usr/\\*/share\n; show the content of the current directory according to the specified string (wildcards and characters).\n\n\necho ~\n; show \n/home/user\n.\n\n\necho .\\*\n; show hidden files.\n\n\necho 2 + 2\n; show 2 + 2.\n\n\necho $((2 + 2))\n; show 4.\n\n\necho $(($((5 \\*\\* 2)) \\* 3))\n; show 75.\n\n\necho Front-{A,B,C}-Back\n; show Front-A-Back Front-B-Back Front-C-Back.\n\n\necho Number-{1...5}\n; show Number-1 Number-2 Number-3 Number-4 Number-5.\n\n\necho {Z...A}\n; show Z Y X W\u2026\n\n\necho a{A{1,2}, B{3,4}}b\n; show aS1b aA2b aB3b aB4b.\n\n\nmkdir {2007...2009}-0{1...9} {2007...2009}-{10...12}\n; show 2007-01 2007-02 2007-03 2007-04 2007-05 2007-06 2007-07 2007-08 2007-09 2007-10 2007-11 2007-12 2008-01\u2026\n\n\necho $(ls)\n; show, not a list, but a paragraph of files and directories.\n\n\necho $USER $((2 + 2))\n; show user 4.\n\n\necho $(cal) or echo ' $(cal) '\n; show a calendar.\n\n\nWith \n.\n, special characters become ordinary characters except for \n$\n, \n\\\n or \n'\n; and with \n.\n, it doesn\nt suppress commands, variables and aliases as with \n.\n. Try :\n\n\necho text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))\n .\n\n\necho 'text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))'\n .\n\n\necho 'text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))'\n.\n\n\nprint env | less\n; show a list of available variables.\n\n\ncat test.txt\n; showthe content of file \ntext.txt\n.\n\n\nls \n file.txt\n; send command and results into a file, if the file exists, it overwrite it, use \ncat file.txt\n to edit it.\n\n\nls \n file.txt\n; send command and results into a file, if the file exists, it appends the new content to the existing one, use \ncat file.txt\n to edit it.\n\n\ncat test.txt \n bbb.txt\n; write (overwrite) the (existing) content of \ntest.txt\n into \nbbb.txt\n.\n\n\ncat test.txt \n bbb.txt\n; write the content of \ntest.txt\n into \nbbb.txt\n or append it to \nbbb.txt\n.\n\n\nwc test.txt\n; count lines, words and bytes of \ntest.txt\n.\n\n\ncat test.txt | wc\n; show the wc of \ntest.txt\n.\n\n\ncat test.txt | wc \n bbb.txt\n; write the \nwc\n of \ntest.txt\n into \nbbb.txt\n.\n\n\nsort \n file.txt\n; push the content into command sort.\n\n\ncat \n aaa.txt\n; open file \naaa.txt\n to edit.\n\n\nsort file.txt\n; sort the content.\n\n\ncat aaa.txt | sort \n sorted_aaa.txt\n; in addition, send the results in a new files.\n\n\nuniq file.txt\n; extract unique values of the content.\n\n\nls -l | less\n; the list goes into the reader.\n\n\nls -l | head\n; the list shows the 10 top lines only.\n\n\nls -l | tail\n; the list shows the 10 bottom lines only.\n\n\n\n\nSection 4, Environment\n\n\n\n\nEach time we launch the terminal application, it creates a new session. The session immediately loads settings and preferences that make up the command line environment.\n\n\n\n\nWe can configure this environment to support the commands and programs, customize greetings and command aliases, and create variables to share across commands and programs.\n\n\n\n\n\n\nA simple, command line text editor: \nnano\n. It is more powerful than \nless\n. Type :\n\n\n\n\n\n\n$ nano hello.txt\n\n\n\n\n\n\nIn \nnano\n, at the top of the window, type :\n\n\n\n\n$ 'Hello, I am nano.'\n\n\n\n\n\n\nUsing the menu at the bottom of the terminal for reference, type \nCtrl + o\n (the letter, not the number) to save the file. Press \nEnter\n, when prompted about the filename to write. Then type \nCtrl + x\n to exit \nnano\n. Finally, type \nclear\n to clear the terminal window. The command prompt should now be at the top of the window. You just edited a file in the \nnano\n text editor. Type:\n\n\n\n\n$ nano hello.txt\n\n\n\n\n\n\nnano\n is a command line text editor. It works just like a desktop text editor, except that it is accessible from the command line and only accepts keyboard input. The command \nnano hello.txt\n opens a new text file named \nhello.txt\n in the \nnano\n text editor. \nHello, I am nano\n is a text string entered in \nnano\n through the cursor.\n\n\nThe menu of keyboard commands at the bottom of the window allow us to save changes to \nhello.txt\n and exit \nnano\n. The \n^\n stands for the \nCtrl\n key.\n\n\nCtrl + o\n saves a file. \no\n stands for output.\n\n\nCtrl + x\n exits the \nnano\n program. \nx\n stands for exit.\n\n\nCtrl + g\n opens a help menu.\n\n\nClear\n clears the terminal window, moving the command prompt to the top of the screen.\n\n\n\n\n\n\nnano editor\n\n\nNow that you are familiar with editing text in \nnano\n, let\ns create a file to store environment settings. Type :\n\n\n\n\n$ nano ~/.bash_profile\n\n\n\n\n\n\nThis opens up a new file in \nnano\n. In \n~/.bash_profile\n, at the top of the file, type :\n\n\n\n\n$ echo 'Welcome, Jane Doe'\n\n\n\n\n\n\nYou can use your name in place of \nJane Doe\n. Type \nCtrl + o\n to save the file. Press \nEnter\n to write the filename. Type \nCtrl + x\n to exit. Finally, type \nclear\n to clear the terminal window. Type :\n\n\n\n\n$ source ~/.bash_profile\n\n\n\n\n\n\nYou should see the greeting you entered. You created a file in \nnano\n called \n~/.bash_profile\n and added a greeting.\n\n\n\n\n$ nano ~/.bash_profile\n\n\n\n\n\n\n~/.bash_profile\n is the name of file used to store environment settings. It is commonly called the \nbash profile\n. When a session starts, it will load the contents of the bash profile before executing commands.\n\n\nThe \n~\n represents the user\ns home directory.\n\n\nThe \n.\n indicates a hidden file.\n\n\n\n\n\n\nThe name \n~/.bash_profile\n is important, since this is how the command line recognizes the bash profile.\n\n\nThe command \nnano ~/.bash_profile\n opens up \n~/.bash_profile\n in \nnano\n. The text echoes \nWelcome, Jane Doe\n and creates a greeting in the bash profile, which is saved. It tells the command line to echo the string \nWelcome, Jane Doe\n when a terminal session begins. The command source \n~/.bash_profile\n ACTIVATES the changes in \n~/.bash_profile\n for the current session.\n\n\nNow that we know what bash profile is, let\ns continue configuring the environment by adding command aliases. Open \n~/.bash_profile\n in \nnano\n. In \n~/.bash_profile\n, beneath the greeting you created, type :\n\n\n\n\n$ alias pd='pwd'\n\n\n\n\n\n\nSave the file. Press \nEnter\n to write the filename. Exit \nnano\n. Clear the terminal window.\n\n\nIn the command line, use the source command to activate the changes in the current session.\n\n\n\n\n$ source ~/.bash_profile\n\n\n\n\n\n\nLet\ns try out the alias. Type :\n\n\n\n\n$ pd\n\n\n\n\n\n\nYou should see the same output as you would by typing the \npwd\n command. What happens when you store this alias in \n~/.bash_profile\n?\n\n\n\n\n$ alias pd='pwd'\n\n\n\n\n\n\nThe alias command allows you to create keyboard shortcuts, or aliases, for commonly used commands. Here alias \npd='pwd'\n creates the alias \npd\n for the \npwd\n command, which is then saved in the bash profile. Each time you enter \npd\n, the output will be the same as the pwd command. The command source \n~/.bash_profile\n makes the alias \npd\n available in the current session. Each time we open up the terminal, we can use the \npd\n alias.\n\n\nLet\ns practice aliases some more. Open \n~/.bash_profile\n in \nnano\n. In the bash profile, beneath the previous alias, add :\n\n\n\n\n$ alias hy='history'\n\n\n\n\n\n\nSave the file. Press \nEnter\n to write the filename.\n\n\nAdd another alias:\n\n\n\n\n$ alias ll='ls -la'\n\n\n\n\n\n\nSave the file.\n\n\nPress \nEnter\n to write the filename.\n\n\nExit \nnano\n.\n\n\nClear the terminal window.\n\n\nIn the command line, use source to activate the changes to the bash profile for the current session.\n\n\nLet\ns try out the aliases. Type:\n\n\n\n\n$ hy\n\n\n\n\n\n\nWhat happens when you store the following aliases in \n~/.bash_profile\n?\n\n\n\n\n$ alias hy='history'\n\n\n\n\n\n\nhy\n is set as alias for the history command in the bash profile. The alias is then made available in the current session through source. By typing \nhy\n, the command line outputs a history of commands that were entered in the current session. Type:\n\n\n\n\n$ alias ll='ls -la'\n\n\n\n\n\n\nll\n is set as an alias for \nls -la\n and made available in the current session through source. By typing \nll\n, the command line now outputs all contents and directories in long format, including all hidden files.\n\n\nNow that you are familiar with configuring greetings and aliases, let\ns move on to setting environment variables.\n\n\nOpen \n~/.bash_profile\n in \nnano\n.\n\n\nIn the bash profile, beneath the aliases, on a new line, type:\n\n\n\n\n$ export USER='Jane Doe'\n\n\n\n\n\n\nFeel free to use your own name. Save the file. Press \nEnter\n to write the filename. Exit \nnano\n. Finally, clear the terminal.\n\n\nIn the command line, use source to activate the changes in the bash profile for the current session. Type :\n\n\n\n\n$ echo $USER\n\n\n\n\n\n\nThis should return the value of the variable that you set. What happens when you store this in \n~/.bash_profile\n?\n\n\n\n\n$ export USER='Jane Doe'\n\n\n\n\n\n\nEnvironment variables are variables that can be used across commands and programs and hold information about the environment. The line \nUSER='Jane Doe'\n sets the environment variable \nUSER\n to a name \n'Jane Doe'\n. Usually the \nUSER\n variable is set to the name of the computer\ns owner. The line export makes the variable to be available to all child sessions initiated from the session you are in. This is a way to make the variable persist across programs. At the command line, the command echo \n$USER\n returns the value of the variable. Note that \n$\n is always used when returning a variable\ns value. Here, the command echo \n$USER\n returns the name set for the variable.\n\n\nLet\ns learn a few more environment variables, starting with the variable for the command prompt. Open \n~/.bash_profile\n in \nnano\n. On a new line, beneath the last entry, type\n\n\n\n\n$ export PS1='\n '\n\n\n\n\n\n\nSave the file. Press Enter to write the filename. Exit \nnano\n. Finally, clear the terminal window. In the command line, use source to activate the changes in the bash profile for the current shell session. Let\ns try out the new command prompt. Type :\n\n\n\n\n$ echo 'hello'\n\n\n\n\n\n\nType :\n\n\n\n\n$ ls -alt\n\n\n\n\n\n\nDid you notice that the prompt has changed? What happens when this is stored in \n~/.bash_profile\n?\n\n\n\n\n$ export PS1='\n '\n\n\n\n\n\n\nPS1\n is a variable that defines the makeup and style of the command prompt.\n\n\necho $PS1\n prints the variable.\n\n\nPS1=\"value\"\n changes the variable value.\n\n\nPS1='\n '\n sets the command prompt variable and exports the variable. It makes the variable available in all sub programs of the current shell. Here we change the default command prompt from \n$\n to \n. After using the source command, the command line displays the new command prompt. Let\ns learn about two more environment variables. Type :\n\n\n\n\n$ echo $HOME\n\n\n\n\n\n\nThis returns the value of the \nHOME\n variable. What happens when you type this command?\n\n\n\n\n$ echo $HOME\n\n\n\n\n\n\nThe \nHOME\n variable is an environment variable that displays the path of the home directory. Here by typing echo \n$HOME\n, the terminal displays the path \n/home/ccuser\n as output. \ncd $HOME\n goes to the home directory. You can customize the \nHOME\n variable if needed, but in most cases this is not necessary. In the command line, type :\n\n\n\n\n$ echo $PATH\n\n\n\n\n\n\nType :\n\n\n\n\n/bin/pwd\n\n\n\n\n\n\nType :\n\n\n\n\n/bin/ls\n\n\n\n\n\n\nWhat happens when you type this command?\n\n\n\n\n$ echo $PATH\n/home/ccuser/.gem/ruby/2.0.0/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/sbin:/sbin:/bin\n\n\n\n\n\n\nPATH\n is an environment variable that stores a list of directories separated by a colon. Looking carefully, echo \n$PATH\n lists the following directories:\n\n\n/home/ccuser/.gem/ruby/2.0.0/bin\n.\n\n\n/usr/local/sbin\n.\n\n\n/usr/local/bin\n.\n\n\n/usr/bin\n.\n\n\n/usr/sbin\n.\n\n\n/sbin\n.\n\n\n/bin\n.\n\n\n\n\n\n\nEach directory contains scripts for the command line to execute. The \nPATH\n variable simply lists which directories contain scripts. For example, many COMMANDS we\nve learned are scripts stored in the \n/bin\n directory.\n\n\n\n\n$ /bin/pwd\n\n\n\n\n\n\nThis is the script that is executed when you type the \npwd\n command.\n\n\n\n\n$ /bin/ls\n\n\n\n\n\n\nThis is the script that is executed when you type the ls command. In advanced cases, you can customize the \nPATH\n variable when adding scripts of your own. Type :\n\n\n\n\n$ env\n\n\n\n\n\n\nType :\n\n\n\n\n$ env | grep PATH\n\n\n\n\n\n\nWhat happens when you type this command?\n\n\n\n\n$ env\n\n\n\n\n\n\nThe \nenv\n command stands for \nenvironment\n, and returns a list of the environment variables for the current user. Here, the \nenv\n command returns a number of variables, including \nPATH\n, \nPWD\n, \nPS1\n, and \nHOME\n.\n\n\n\n\n$ env | grep PATH\n$ env | grep aliasname\n\n\n\n\n\n\nenv | grep PATH\n is a command that displays the value of a single environment variable. Here the standard output of env is \npiped\n to the \ngrep\n command. \ngrep\n searches for the value of the variable \nPATH\n and outputs it to the terminal. You learned to use the bash profile to configure the environment. What can we generalize so far? The environment refers to the preferences and settings of the current user.\n\n\n\n\n\n\n\n\nLet\ns summarize what we\nve done so far.\n\n\nThe \nnano\n editor is a command line text editor used to configure the environment.\n\n\n~/.bash_profile\n is where environment settings are stored. You can edit this file with \nnano\n.\n\n\nEnvironment variables are variables that can be used across commands and programs and hold information about the environment.\n\n\nexport VARIABLE='Value'\n sets and exports an environment variable.\n\n\nUSER\n is the name of the current user.\n\n\nPS1\n is the command prompt.\n\n\nHOME\n is the home directory. It is usually not customized.\n\n\nPATH\n returns a colon separated list of file paths. It is customized in advanced cases.\n\n\necho $PATH\n prints the path.\n\n\nPATH=\"value\"\n changes the path..\n\n\nexport PATH=/home/dir/bin:$PATH\n appends the new path to environment variable PATH.\n\n\nenv\n returns a list of environment variables.\n\n\n\n\nSection 5, Multi-Users\n\n\n\n\nLinux is multi-user : multiple users at the same time as opposed to OS X or Windows; just like the UNIX mainframe computers with terminals, users and superuser concepts. \n\n\n\n\nAccess rights\n\n\n\n\nType :\n\n\n\n\n$ ls -l\n\n\n\n\n\n\n\n\nRead, from left to right :\n\n\n\n\n\u2013\n or \nd\n: file or dir.\n\n\nFile or dir name.\n\n\nOwner access (\nr w x\u2026-\n).\n\n\nGroup access (\nr w x\u2026-\n).\n\n\nAll access (\nr w x\u2026-\n).\n\n\nOwner group, size, date.\n\n\n\n\n\n\n\n\nHow do we change the acces rights? With \nchmod\n.\n\n\n\n\nFirst, there are access right for:\n\n\nu\n ser.\n\n\ng\n roup.\n\n\no\n thers.\n\n\n\n\n\n\nSecond, there levels. Each level has a numeric value.:\n\n\nr\n ead: 4.\n\n\nw\n rite: 2.\n\n\nx\n ecute: 1.\n\nFor example, using levels:\n\n\no+w\n: others can write the file (create).\n\n\nu+x\n: users can execute the file.\n\n\ng-x\n: group can no longer execute the file.\n\n\netc.\n\n\nchmod o+w file1.txt\n for example.\n\n\n\n\n\n\n\n\nWhy numeric value? It an alternative way for \nchmod\n to assign access rights. Levels are ranked with values:\n\n\n\n\n\n\n\n\nLevel\n\n\nBinary\n\n\nDecimal\n\n\n\n\n\n\n\n\n\n\nr w x\n\n\n111\n\n\n7\n\n\n\n\n\n\nr w -\n\n\n110\n\n\n6\n\n\n\n\n\n\nr - -\n\n\n100\n\n\n4\n\n\n\n\n\n\n- - -\n\n\n000\n\n\n0\n\n\n\n\n\n\nr - x\n\n\n101\n\n\n5\n\n\n\n\n\n\n\n\n\n\nA \n7\n grants full rights vs a \n0\n that grants no rights. For example:\n\n\nchmod 600 file\n; change the file access rights to \nrw- --- ---\n.\n\n\nchmod 600 dir\n; change the directory access rights.\n\n\n\n\n\n\nHow are the values calculated?\n\n\n\n\nr+w+x = 4+2+1 = 7\nr     = 4     = 4\n    x =     1 = 1\nr+w   = 4+2   = 6\n...\n...\n\n\n\n\nOnce you have the values, you can set the access rights:\n\n\n u   g   o  \n--- --- ---\nrwx rw- --x\n 7   6   1\n\n\n\n\n\n\nTherefore, \nchmod 761\n set the access rights \nrwx rw- --x\n to a file, files, a directory or directories.\n\n\n\n\nOwnership\n\n\n\n\nchown thou file\n; assign a new owner, \nthou\n, to \nfile\n.\n\n\nchown thou dir\n; \n to dir.\n\n\nchgrp newgr file\n assign a new group owner, \nnewgr\n, to \nfile\n.\n\n\nchgrp newgr dir\n; \n to dir.\n\n\n\n\nSuperuser\n\n\n\n\nsu\n; superuser login.\n\n\nsu file\n; unlock the file with the superuser password.\n\n\nsudo\n; do it with the privilege of a superuser.\n\n\nsudo apt-get update\n; updates the database.\n\n\nsudo apt-get upgrade\n; upgrades all packages (update before upgrading).\n\n\nsudo apt-get install build-essential\n; install a useful package (or any other package).\n\n\nsudo apt-get install git\n; install \ngit\n.\n\n\nwhich git\n; find where \ngit\n is located (if it\ns instaled).\n\n\nsudo apt-get remove git\n; remove \ngit\n.\n\n\nsudo apt-get purge git\n; remove \ngit\n and purge any remainings.\n\n\nsudo chnow user file\n; change the owner.\n\n\n\n\nSection 6, Multi-Tasks\n\n\n\n\nLinux is multi-task; multiple tasks can run at the same time like OS X or Windows or UNIX; Linux kernel runs processes; they take turns at the processor(s). All programs and processes can be launched/killed from the GUI and the CLI.\n\n\n\n\nShow\n\n\n\n\ntop\n; show process dashboard by PID number; \n?\n for help, \nq\n for quit.\n\n\nps\n; show the process list.\n\n\nps aux\n; show all process.\n\n\nps aux | grep 'top'\n; filter top processes.\n\n\nps aux | grep bash\n; filter processes related to the bash.\n\n\nps aux | grep bash | sort\n; \n and sort them.\n\n\n\n\n\n\n\n\nManage\n\n\n\n\nctrl + z\n; pause a process, put it in the background.\n\n\nfg\n; foreground, bring back the process.\n\n\njobs\n; list paused processes.\n\n\nfg #PID\n; bring back process #PID (if there is more than one process on pause, you must identify the process).\n\n\nctrl + c\n; terminate the active process.\n\n\nxload\n; display the system load in a new windows, but jam the current terminal (open another or several terminals then).\n\n\nxload \n; runs in the background\n\n\nCTRL + x\n; suspend the process and unjam the terminal.\n\n\nby\n; resume the process.\n\n\nps\n; show processes and their #PID.\n\n\nps x | grep bad_program\n; find the bad processes.\n\n\nkill\n can send a signal from the OS to a process. For example : when you log off, you send a signal to terminate a word processing program and save the file before closing.\n\n\nkill #PID\n; kill the process.\n\n\nkill -STOP #PID\n; pause the process.\n\n\nkill -TERM #PID\n;  terminate the process.\n\n\nkill -SIGTERM #PID\n;  terminate the process.\n\n\nkill -SIGKILL #PID\n; kill the process.\n\n\nkill -KILL\n; force closing of the current process.\n\n\nkill -9 #PID\n;  force closing the process.\n\n\n\n\n\n\nKilling sequence:\n\n\nkill #PID\n; doesn\nt work\u2026\n\n\nkill -9 #PID\n, or..", 
            "title": "Codecademy Learn the Command Line Notes"
        }, 
        {
            "location": "/Codecademy Learn the Command Line Notes/#section-1-navigation", 
            "text": "Bash means  Bourne again shell .  There exists other bash programs : ksh, tcsh, zsh, etc.  Common terminals: Ubuntu gnome, KDE konsole, eterm, txvt, kvt, nxterm, eterm.  A CLI has its advantages over a GUI.   Terminal   bash ; open another  bash  in the same bash window.  exit ; close one instance of  bash .  export ; export variable to the next bash.  export MESSAGE=\"Hi\" .  Variables are associated with one bash.   One change in one bash does not affect the other bash.    man  cmd ; call the manual, help about a command.  man cp ; open the manual about command  cp  (or any other command).    cp --help ; command  cp  arguments (or any other command).  command cp ; get info on command  cp  (or any other command).  type cp ; find where command  cp  is located and what kind of command (an  alias  or  else ) (or any other command).  which cp ; locate command  cp  (or any other command).  which make ; check out where  make  is.   pwd ; print working directory.  dir ; list of directory.  up / down  arrow; resume a past command.  tab ; autocomplete.   Change dir   cd/aaa ; change to directory  aaa .  cd ~ ,  cd ~user ; go back home to  user  directory.  cd .. ; go up one level.  cd ../.. ; go up two levels.  cd ../../.. ; go up three levels.  cd temp ; go to directory  temp .  cd stuff ; if we are in dir  temp , switch to directory  stuff .  cd temp/stuff ; switch directly to directory  stuff .  cd 'en co re' ; switch to directory  en co re .   Make dir   mkdir aaa ; make directory  aaa .  mkdir -p aaa/bbb ; make directory  aaa  with sub-directory  bbb .  mkdir 'en co re' ; make directory  en co re .   Remove dir   rmdir aaa ; remove directory  aaa .  rmdir 'en co re' ; remove directory  en co re .  rmdir aaa/bbb ; remove both directory and sub-directory (if both empty).  rmdir -p aaa/bbb ; remove all  (you cannot remove a directory if it is non-empty (filled with sub-directories and files)) .  rmdir -p aaa/* ; remove all.     Switch dir   pushd ; save current location; then, change location (another directory).  popd ; return to pushd.  (for example, you are in  aaa , type  pushd , go to  bbb , type  pushb , go to  ccc , type  popd , return to  bbb , type  popd , return to  aaa ) .   Make file   touch iamcoo.txt ; create an empty text file.  touch temp/ismvoo.txt ; create an empty text file in the  temp  directory.   Edit file   less test.txt ; open the file and display its content.  Within  less  :  q ; quit.  arrows,  pgUp ,  pgDn ; move.  h ; help.    For other files extensions like  tar  and GNU archive, use  tvf .", 
            "title": "Section 1, Navigation"
        }, 
        {
            "location": "/Codecademy Learn the Command Line Notes/#section-2-manipulation", 
            "text": "List file   dir   ls ; list of directories and files.  ls /bin/bash ; list of a remote directory and files.  ls -a ; list all.  ls -t ; list in alphanumeric order, when they were last modified.  ls -l ; list in long format.  ls -la ; list all in long format.  ls -alt ; list all in long format and ordered.  ls /aaa /bbb ; list both directories.  ls -r ; list in reverse.  ls ../paint/ ; list an upper directory.   Copy file   dir   cp aaa.txt bbb.txt ; copy  aaa.txt  file, paste it or create a copy named  bbb.txt .  cp aaa.txt dir/ ; copy  aaa.txt  into directory  dir .  cp aaa.txt bbb.txt dir/ ; copy  aaa.txt  and  bbb.txt into directory  dir .  cp \\*.txt dir/ ; copy all  .txt  files into directory  dir .  cp aaa.txt dir/dir2 ; copy  aaa.txt  into sub-directory  dir2 .  cp -r aaa bbb ; copy  dir1 , create a copy names  dir2  with the exact same content.  cp -i ; interactive, to prompt the user.  cp f\\* ../paint/ ; copy all files beginning with  f  to an upper directory.   Wildcards   \\* ; wildcard for any string.  ? ; wildcard for any character.  \\*.txt ; all files finishing with  .txt .  r\\* ; all files beginning with  r .  ??a.txt ; all files beginning with two characters +  a.txt .  backup[[:digit:]] ; all file beginning with  backup  + any digit.  [abc]\\* ; all files beginning with either  a ,  b  or  c  [[:upper:]]\\* ; all files beginning with an upper case.  \\*[![:lower:]] ; all file not finishing with a lower case.   Move file   dir   mv aaa.txt bbb.txt ; move file or cut  aaa.txt  and paste  bbb.txt .  mv aaa.txt dir/ ; move file  aaa.txt  into directory  dir .  mv aaa.txt bbb.txt dir/ ; move files  aaa.txt  and  bbb.txt  into directory  dir .  mv \\*.txt dir/ ; move all  .txt  files into directory  dir .  mv aaa.txt dir/dir2 ; move file  aaa.txt  into sub-directory  dir2 .  mv -i ; interactive, prompt the user.   Remove file   dir   rm aaa.txt ; remove file  aaa.txt .  rm test1.txt test2.txt ; remove both files.  rm aaa/\\* ; remove all files in the directory  aaa .  rm -r aaa ; remove directory  aaa , must be empty.  rm -rf aaa ; remove directory  aaa  and its files.  rm -i ; interactive, prompt the user.", 
            "title": "Section 2, Manipulation"
        }, 
        {
            "location": "/Codecademy Learn the Command Line Notes/#section-3-redirection", 
            "text": "Let s begin by taking a closer look at input and output. In the terminal, after the shell prompt, type :   $ echo 'Hello'   The  echo  command accepts the string  Hello  as standard input, and echoes the string  'Hello'  back to the terminal as standard output.   standard input, abbreviated as  stdin , is information inputted into the terminal through the keyboard or input device.  standard output, abbreviated as  stdout , is the information outputted after a process is run.  standard error, abbreviated as  stderr , is an error message outputted by a failed process.    Redirection ( ) reroutes standard input, standard output, and standard error to or from a different location. Type :   $ echo 'Hello'   hello.txt   Type :   $ cat hello.txt   The   command redirects the standard output to a file. The standard output  'Hello'  is redirected by   to the file  hello.txt , and entered as the standard input.   The cat command outputs the contents of a file to the terminal. When you type  cat hello.txt , the contents of  hello.txt  are displayed.    Type :    $ ls -l   This is the filesystem we ll work with. Create a file. Type :   $ touch ocean.txt   Fill the file with values (ocean names). Type :   $ cat oceans.txt   continents.txt   Use  cat  to view the contents of  continents.txt . Notice that we only see oceans as output:   $ cat continents.txt    takes the standard output of the command on the left, and redirects it to the file on the right. Here the standard output of cat  oceans.txt  is redirected to  continents.txt . Note that   OVERWRITES all original content in  continents.txt . When you view the output data by typing cat on  continents.txt , you will see only the contents of  oceans.txt . Type :   $ cat glaciers.txt   rivers.txt   Use  cat  to view the contents of  rivers.txt :   $ cat rivers.txt   Notice that we see both rivers and glaciers as output. Type:   $ cat glaciers.txt   rivers.txt     takes the standard output of the command on the left and APPENDS it to the file on the right. You can view the output data of the file with cat and the filename. Here, the output data of  rivers.txt  will contain the original contents of  rivers.txt  with the content of  glaciers.txt  appended to it.     Type :    $ touch lakes.txt   Fill the files with values (lake names). Type :   $ cat   lakes.txt    takes the standard input from the file on the right and inputs it into the program on the left. Here,  lakes.txt  is the standard input for the  cat  command. The standard output appears in the terminal. Let s try some more redirection commands. Type :   $ touch volcanoes.txt   Fill the files with values (volcano names). Type :   $ cat volcanoes.txt | wc   You get the count for: lines, words, bytes. Type :   $ cat volcanoes.txt | wc | cat   islands.txt   Use  cat  to output the contents in  islands.txt . The next command should now equals  $ cat volcanoes.txt | wc .   $ cat volcanoes.txt | wc   |  is a  pipe  or  pipeline  The  |  takes the standard output of the command on the left, and PIPES it as standard input to the command on the right. You can think of this as  command to command  redirection. Here, again, the output of cat  volcanoes.txt  is the standard input of  wc . In turn, the  wc  command outputs the number of lines, words, and characters in  volcanoes.txt , respectively:   $ cat volcanoes.txt | wc | cat   islands.txt\n$ less islands.txt   Multiple  | s can be chained together. Here the standard output of cat  volcanoes.txt  is  piped  to the  wc  command. The standard output of  wc  is then  piped  to cat. Finally, the standard output of cat is redirected to  islands.txt . You can view the output data of this chain by typing cat  islands.txt .   A few commands are particularly powerful when combined with redirection. Let s try them out. First, use  cat  to output the contents of  lakes.txt . Then,  sort  it. Type :   $ cat lakes.txt\n$ sort lakes.txt   The lakes in  lakes.txt  are listed in alphabetical order. Type:   $ cat lakes.txt | sort   sort  takes the standard input and orders it alphabetically for the standard output. The lakes in  lakes.txt  are listed in alphabetical order.  Use cat to output the contents of  sorted-lakes.txt .   $ cat lakes.txt | sort   sorted-lakes.txt   The command takes the standard output from cat  lakes.txt  and  pipes  it to sort in ascending order. The standard output of  sort  is redirected to  sorted-lakes.txt . You can view the output data by typing:   $ cat sorted-lakes.txt   Type :   $ touch deserts.txt   Fill the file with values. Type :   $ cat deserts.txt\n$ uniq deserts.txt    You get to see all entries and unique entries.    Type :    $ sort deserts.txt | uniq   You get to see  unique entries sorted. Type :   $ sort deserts.txt | uniq   uniq-deserts.txt   You save the result in  uniq-deserts.txt .  Use  cat  to output the contents of  uniq-deserts.txt .  uniq  stands for  unique  and filters out ADJACENT, duplicate lines in a file. Here  uniq deserts.txt  filters out duplicates of  Sahara Desert , because the duplicate of  Sahara Desert  directly follows the previous instance. The  Kalahari Desert  duplicates are not adjacent, and thus remain.  Type :   $ grep Mount mountains.txt   grep  stands for  global regular expression print . It searches files for lines that match a pattern and returns the results. It is also case sensitive. Here,  grep  searches for  Mount  in  mountains.txt . Type:   $ grep -i Mount mountains.txt   grep -i  enables the command to be case insensitive. Here,  grep  searches for capital or lowercase strings that match  Mount  in  mountains.txt . The above commands are a great way to get started with  grep . If you are familiar with regular expressions, you can use regular expressions to search for patterns in files.  grep  can also be used to search within a directory. Type :   $ grep -R Arctic /home/ccuser/workspace/geography   ype :   $ grep -R Arctic /home/ccuser/workspace/geography   grep -R  searches files in a directory and outputs filenames and lines containing matched results.  -R  stands for  recursive . Here  grep -R  searches the  /home/ccuser/workspace/geography  directory for the string  Arctic  and outputs filenames and lines with matched results. Type:   $ grep -R Gambino .   grep -R  searches ALL files recursively and outputs filenames and lines containing matched results.   $ grep -Rl Arctic /home/ccuser/workspace/geography   grep -Rl  searches all files in a directory and outputs only filenames with matched results.  -R  stands for  recursive  and  l  stands for  files with matches . Here  grep -Rl  searches the  /home/ccuser/workspace/geography  directory for the string  Arctic  and outputs filenames with matched results. Use  cat  to display the contents of  forests.txt .   grep this file.txt ; search for  this  in  file.txt,  print the output on screen.  grep this   file.txt ; feed  file.txt  to process  grep  to look for  this , print on screen  grep this file.txt   file_this.txt ; write, overwrite the output in  file_this.txt .  grep this file.txt   file_this.txt ; write, append the output in  file_this.txt .  grep \"is\" file.txt ; search pattern  is , precisely, in words or alone.  grep line file.txt ; search pattern  line .  grep -n line file.txt ; show the line number where it finds pattern  line .  grep -i line file.txt ; case insensitive   grep -v line file.txt ; inverse or when it does not have the pattern line    If you search for one  file  in the current directory, type:    find . _name  file.txt    find / _name \"file.txt\"  searches  file  in multiple directories.  find dirA dirB dirC _name \"file.txt\"  search  file  in one or more directories. Type :   $ sed 's/snow/rain/' forests.txt   sed  stands for  stream editor . It accepts standard input and modifies it based on an expression, before displaying it as output data. It is similar to  find and replace .   Let s look at the expression  's/snow/rain/' :  s : stands for  substitution . it is always used when using sed for substitution.  snow : the search string, the text to find.  rain : the replacement string, the text to add in place.    In this case,  sed  searches  forests.txt  for the word  'snow'  and replaces it with  'rain' . Importantly, the above command will only replace the FIRST instance of  'snow'  on a line.   $ sed 's/snow/rain/g' forests.txt    The above command uses the  g  expression, meaning  global . Here  sed  searches  forests.txt  for the word  'snow'  and replaces it with  'rain' , globally. ALL instances of  'snow'  on a line will be turned to  'rain' .    Let s summarize what we ve done so far.   The common redirection commands are:  ; redirects standard output of a command to a file, overwriting previous content.  ; redirects standard output of a command to a file, appending new content to old content.  ; redirects standard input to a command.  | ; redirects standard output of a command to another command.    A number of other commands are powerful when combined with redirection commands:  echo 'a' ; display  a  in the terminal.  touch ; create a file.  cat ; display the content of a file in the terminal.  less ; edit the content of a file in the terminal; prefer  nano , it s more user-friendly.  sort ; sorts lines in a file in alphabetical order.  uniq ; filters duplicates in a file.  grep ; searches for a text pattern in files and outputs it.  find ; searches for files, not the content.  sed ; searches for a text pattern in files, modifies it, and outputs it.  head / tail ;  ls , but only the top/bottom results.     Examples   echo 'thisthat' ; show  thisthat .  echo $USER ; show user variable.  echo \\* ,  d\\* ,  s\\* ,  [[:upper]]\\* ,  /usr/\\*/share ; show the content of the current directory according to the specified string (wildcards and characters).  echo ~ ; show  /home/user .  echo .\\* ; show hidden files.  echo 2 + 2 ; show 2 + 2.  echo $((2 + 2)) ; show 4.  echo $(($((5 \\*\\* 2)) \\* 3)) ; show 75.  echo Front-{A,B,C}-Back ; show Front-A-Back Front-B-Back Front-C-Back.  echo Number-{1...5} ; show Number-1 Number-2 Number-3 Number-4 Number-5.  echo {Z...A} ; show Z Y X W\u2026  echo a{A{1,2}, B{3,4}}b ; show aS1b aA2b aB3b aB4b.  mkdir {2007...2009}-0{1...9} {2007...2009}-{10...12} ; show 2007-01 2007-02 2007-03 2007-04 2007-05 2007-06 2007-07 2007-08 2007-09 2007-10 2007-11 2007-12 2008-01\u2026  echo $(ls) ; show, not a list, but a paragraph of files and directories.  echo $USER $((2 + 2)) ; show user 4.  echo $(cal) or echo ' $(cal) ' ; show a calendar.  With  . , special characters become ordinary characters except for  $ ,  \\  or  ' ; and with  . , it doesn t suppress commands, variables and aliases as with  . . Try :  echo text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))  .  echo 'text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))'  .  echo 'text ~/\\*.txt {a,b} $(echo foo) $((2 + 2))' .  print env | less ; show a list of available variables.  cat test.txt ; showthe content of file  text.txt .  ls   file.txt ; send command and results into a file, if the file exists, it overwrite it, use  cat file.txt  to edit it.  ls   file.txt ; send command and results into a file, if the file exists, it appends the new content to the existing one, use  cat file.txt  to edit it.  cat test.txt   bbb.txt ; write (overwrite) the (existing) content of  test.txt  into  bbb.txt .  cat test.txt   bbb.txt ; write the content of  test.txt  into  bbb.txt  or append it to  bbb.txt .  wc test.txt ; count lines, words and bytes of  test.txt .  cat test.txt | wc ; show the wc of  test.txt .  cat test.txt | wc   bbb.txt ; write the  wc  of  test.txt  into  bbb.txt .  sort   file.txt ; push the content into command sort.  cat   aaa.txt ; open file  aaa.txt  to edit.  sort file.txt ; sort the content.  cat aaa.txt | sort   sorted_aaa.txt ; in addition, send the results in a new files.  uniq file.txt ; extract unique values of the content.  ls -l | less ; the list goes into the reader.  ls -l | head ; the list shows the 10 top lines only.  ls -l | tail ; the list shows the 10 bottom lines only.", 
            "title": "Section 3, Redirection"
        }, 
        {
            "location": "/Codecademy Learn the Command Line Notes/#section-4-environment", 
            "text": "Each time we launch the terminal application, it creates a new session. The session immediately loads settings and preferences that make up the command line environment.   We can configure this environment to support the commands and programs, customize greetings and command aliases, and create variables to share across commands and programs.    A simple, command line text editor:  nano . It is more powerful than  less . Type :    $ nano hello.txt   In  nano , at the top of the window, type :   $ 'Hello, I am nano.'   Using the menu at the bottom of the terminal for reference, type  Ctrl + o  (the letter, not the number) to save the file. Press  Enter , when prompted about the filename to write. Then type  Ctrl + x  to exit  nano . Finally, type  clear  to clear the terminal window. The command prompt should now be at the top of the window. You just edited a file in the  nano  text editor. Type:   $ nano hello.txt   nano  is a command line text editor. It works just like a desktop text editor, except that it is accessible from the command line and only accepts keyboard input. The command  nano hello.txt  opens a new text file named  hello.txt  in the  nano  text editor.  Hello, I am nano  is a text string entered in  nano  through the cursor.  The menu of keyboard commands at the bottom of the window allow us to save changes to  hello.txt  and exit  nano . The  ^  stands for the  Ctrl  key.  Ctrl + o  saves a file.  o  stands for output.  Ctrl + x  exits the  nano  program.  x  stands for exit.  Ctrl + g  opens a help menu.  Clear  clears the terminal window, moving the command prompt to the top of the screen.    nano editor  Now that you are familiar with editing text in  nano , let s create a file to store environment settings. Type :   $ nano ~/.bash_profile   This opens up a new file in  nano . In  ~/.bash_profile , at the top of the file, type :   $ echo 'Welcome, Jane Doe'   You can use your name in place of  Jane Doe . Type  Ctrl + o  to save the file. Press  Enter  to write the filename. Type  Ctrl + x  to exit. Finally, type  clear  to clear the terminal window. Type :   $ source ~/.bash_profile   You should see the greeting you entered. You created a file in  nano  called  ~/.bash_profile  and added a greeting.   $ nano ~/.bash_profile   ~/.bash_profile  is the name of file used to store environment settings. It is commonly called the  bash profile . When a session starts, it will load the contents of the bash profile before executing commands.  The  ~  represents the user s home directory.  The  .  indicates a hidden file.    The name  ~/.bash_profile  is important, since this is how the command line recognizes the bash profile.  The command  nano ~/.bash_profile  opens up  ~/.bash_profile  in  nano . The text echoes  Welcome, Jane Doe  and creates a greeting in the bash profile, which is saved. It tells the command line to echo the string  Welcome, Jane Doe  when a terminal session begins. The command source  ~/.bash_profile  ACTIVATES the changes in  ~/.bash_profile  for the current session.  Now that we know what bash profile is, let s continue configuring the environment by adding command aliases. Open  ~/.bash_profile  in  nano . In  ~/.bash_profile , beneath the greeting you created, type :   $ alias pd='pwd'   Save the file. Press  Enter  to write the filename. Exit  nano . Clear the terminal window.  In the command line, use the source command to activate the changes in the current session.   $ source ~/.bash_profile   Let s try out the alias. Type :   $ pd   You should see the same output as you would by typing the  pwd  command. What happens when you store this alias in  ~/.bash_profile ?   $ alias pd='pwd'   The alias command allows you to create keyboard shortcuts, or aliases, for commonly used commands. Here alias  pd='pwd'  creates the alias  pd  for the  pwd  command, which is then saved in the bash profile. Each time you enter  pd , the output will be the same as the pwd command. The command source  ~/.bash_profile  makes the alias  pd  available in the current session. Each time we open up the terminal, we can use the  pd  alias.  Let s practice aliases some more. Open  ~/.bash_profile  in  nano . In the bash profile, beneath the previous alias, add :   $ alias hy='history'   Save the file. Press  Enter  to write the filename.  Add another alias:   $ alias ll='ls -la'   Save the file.  Press  Enter  to write the filename.  Exit  nano .  Clear the terminal window.  In the command line, use source to activate the changes to the bash profile for the current session.  Let s try out the aliases. Type:   $ hy   What happens when you store the following aliases in  ~/.bash_profile ?   $ alias hy='history'   hy  is set as alias for the history command in the bash profile. The alias is then made available in the current session through source. By typing  hy , the command line outputs a history of commands that were entered in the current session. Type:   $ alias ll='ls -la'   ll  is set as an alias for  ls -la  and made available in the current session through source. By typing  ll , the command line now outputs all contents and directories in long format, including all hidden files.  Now that you are familiar with configuring greetings and aliases, let s move on to setting environment variables.  Open  ~/.bash_profile  in  nano .  In the bash profile, beneath the aliases, on a new line, type:   $ export USER='Jane Doe'   Feel free to use your own name. Save the file. Press  Enter  to write the filename. Exit  nano . Finally, clear the terminal.  In the command line, use source to activate the changes in the bash profile for the current session. Type :   $ echo $USER   This should return the value of the variable that you set. What happens when you store this in  ~/.bash_profile ?   $ export USER='Jane Doe'   Environment variables are variables that can be used across commands and programs and hold information about the environment. The line  USER='Jane Doe'  sets the environment variable  USER  to a name  'Jane Doe' . Usually the  USER  variable is set to the name of the computer s owner. The line export makes the variable to be available to all child sessions initiated from the session you are in. This is a way to make the variable persist across programs. At the command line, the command echo  $USER  returns the value of the variable. Note that  $  is always used when returning a variable s value. Here, the command echo  $USER  returns the name set for the variable.  Let s learn a few more environment variables, starting with the variable for the command prompt. Open  ~/.bash_profile  in  nano . On a new line, beneath the last entry, type   $ export PS1='  '   Save the file. Press Enter to write the filename. Exit  nano . Finally, clear the terminal window. In the command line, use source to activate the changes in the bash profile for the current shell session. Let s try out the new command prompt. Type :   $ echo 'hello'   Type :   $ ls -alt   Did you notice that the prompt has changed? What happens when this is stored in  ~/.bash_profile ?   $ export PS1='  '   PS1  is a variable that defines the makeup and style of the command prompt.  echo $PS1  prints the variable.  PS1=\"value\"  changes the variable value.  PS1='  '  sets the command prompt variable and exports the variable. It makes the variable available in all sub programs of the current shell. Here we change the default command prompt from  $  to  . After using the source command, the command line displays the new command prompt. Let s learn about two more environment variables. Type :   $ echo $HOME   This returns the value of the  HOME  variable. What happens when you type this command?   $ echo $HOME   The  HOME  variable is an environment variable that displays the path of the home directory. Here by typing echo  $HOME , the terminal displays the path  /home/ccuser  as output.  cd $HOME  goes to the home directory. You can customize the  HOME  variable if needed, but in most cases this is not necessary. In the command line, type :   $ echo $PATH   Type :   /bin/pwd   Type :   /bin/ls   What happens when you type this command?   $ echo $PATH\n/home/ccuser/.gem/ruby/2.0.0/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/sbin:/sbin:/bin   PATH  is an environment variable that stores a list of directories separated by a colon. Looking carefully, echo  $PATH  lists the following directories:  /home/ccuser/.gem/ruby/2.0.0/bin .  /usr/local/sbin .  /usr/local/bin .  /usr/bin .  /usr/sbin .  /sbin .  /bin .    Each directory contains scripts for the command line to execute. The  PATH  variable simply lists which directories contain scripts. For example, many COMMANDS we ve learned are scripts stored in the  /bin  directory.   $ /bin/pwd   This is the script that is executed when you type the  pwd  command.   $ /bin/ls   This is the script that is executed when you type the ls command. In advanced cases, you can customize the  PATH  variable when adding scripts of your own. Type :   $ env   Type :   $ env | grep PATH   What happens when you type this command?   $ env   The  env  command stands for  environment , and returns a list of the environment variables for the current user. Here, the  env  command returns a number of variables, including  PATH ,  PWD ,  PS1 , and  HOME .   $ env | grep PATH\n$ env | grep aliasname   env | grep PATH  is a command that displays the value of a single environment variable. Here the standard output of env is  piped  to the  grep  command.  grep  searches for the value of the variable  PATH  and outputs it to the terminal. You learned to use the bash profile to configure the environment. What can we generalize so far? The environment refers to the preferences and settings of the current user.     Let s summarize what we ve done so far.  The  nano  editor is a command line text editor used to configure the environment.  ~/.bash_profile  is where environment settings are stored. You can edit this file with  nano .  Environment variables are variables that can be used across commands and programs and hold information about the environment.  export VARIABLE='Value'  sets and exports an environment variable.  USER  is the name of the current user.  PS1  is the command prompt.  HOME  is the home directory. It is usually not customized.  PATH  returns a colon separated list of file paths. It is customized in advanced cases.  echo $PATH  prints the path.  PATH=\"value\"  changes the path..  export PATH=/home/dir/bin:$PATH  appends the new path to environment variable PATH.  env  returns a list of environment variables.", 
            "title": "Section 4, Environment"
        }, 
        {
            "location": "/Codecademy Learn the Command Line Notes/#section-5-multi-users", 
            "text": "Linux is multi-user : multiple users at the same time as opposed to OS X or Windows; just like the UNIX mainframe computers with terminals, users and superuser concepts.    Access rights   Type :   $ ls -l    Read, from left to right :   \u2013  or  d : file or dir.  File or dir name.  Owner access ( r w x\u2026- ).  Group access ( r w x\u2026- ).  All access ( r w x\u2026- ).  Owner group, size, date.     How do we change the acces rights? With  chmod .   First, there are access right for:  u  ser.  g  roup.  o  thers.    Second, there levels. Each level has a numeric value.:  r  ead: 4.  w  rite: 2.  x  ecute: 1. \nFor example, using levels:  o+w : others can write the file (create).  u+x : users can execute the file.  g-x : group can no longer execute the file.  etc.  chmod o+w file1.txt  for example.     Why numeric value? It an alternative way for  chmod  to assign access rights. Levels are ranked with values:     Level  Binary  Decimal      r w x  111  7    r w -  110  6    r - -  100  4    - - -  000  0    r - x  101  5      A  7  grants full rights vs a  0  that grants no rights. For example:  chmod 600 file ; change the file access rights to  rw- --- --- .  chmod 600 dir ; change the directory access rights.    How are the values calculated?   r+w+x = 4+2+1 = 7\nr     = 4     = 4\n    x =     1 = 1\nr+w   = 4+2   = 6\n...\n...  Once you have the values, you can set the access rights:   u   g   o  \n--- --- ---\nrwx rw- --x\n 7   6   1   Therefore,  chmod 761  set the access rights  rwx rw- --x  to a file, files, a directory or directories.   Ownership   chown thou file ; assign a new owner,  thou , to  file .  chown thou dir ;   to dir.  chgrp newgr file  assign a new group owner,  newgr , to  file .  chgrp newgr dir ;   to dir.   Superuser   su ; superuser login.  su file ; unlock the file with the superuser password.  sudo ; do it with the privilege of a superuser.  sudo apt-get update ; updates the database.  sudo apt-get upgrade ; upgrades all packages (update before upgrading).  sudo apt-get install build-essential ; install a useful package (or any other package).  sudo apt-get install git ; install  git .  which git ; find where  git  is located (if it s instaled).  sudo apt-get remove git ; remove  git .  sudo apt-get purge git ; remove  git  and purge any remainings.  sudo chnow user file ; change the owner.", 
            "title": "Section 5, Multi-Users"
        }, 
        {
            "location": "/Codecademy Learn the Command Line Notes/#section-6-multi-tasks", 
            "text": "Linux is multi-task; multiple tasks can run at the same time like OS X or Windows or UNIX; Linux kernel runs processes; they take turns at the processor(s). All programs and processes can be launched/killed from the GUI and the CLI.   Show   top ; show process dashboard by PID number;  ?  for help,  q  for quit.  ps ; show the process list.  ps aux ; show all process.  ps aux | grep 'top' ; filter top processes.  ps aux | grep bash ; filter processes related to the bash.  ps aux | grep bash | sort ;   and sort them.     Manage   ctrl + z ; pause a process, put it in the background.  fg ; foreground, bring back the process.  jobs ; list paused processes.  fg #PID ; bring back process #PID (if there is more than one process on pause, you must identify the process).  ctrl + c ; terminate the active process.  xload ; display the system load in a new windows, but jam the current terminal (open another or several terminals then).  xload  ; runs in the background  CTRL + x ; suspend the process and unjam the terminal.  by ; resume the process.  ps ; show processes and their #PID.  ps x | grep bad_program ; find the bad processes.  kill  can send a signal from the OS to a process. For example : when you log off, you send a signal to terminate a word processing program and save the file before closing.  kill #PID ; kill the process.  kill -STOP #PID ; pause the process.  kill -TERM #PID ;  terminate the process.  kill -SIGTERM #PID ;  terminate the process.  kill -SIGKILL #PID ; kill the process.  kill -KILL ; force closing of the current process.  kill -9 #PID ;  force closing the process.    Killing sequence:  kill #PID ; doesn t work\u2026  kill -9 #PID , or..", 
            "title": "Section 6, Multi-Tasks"
        }, 
        {
            "location": "/Command Line Crash Course/", 
            "text": "Command Line Crash Course\n\n\nForeword\n\n\nCommands and code snippets.\n\n\n\n\nCONTENT\n\n\nCommand Line Crash Course\n\n\nThe Setup\n\n\nMac OS X\n\n\nLinux\n\n\nWindows\n\n\nLinux/Mac OSX\n\n\nWindows\n\n\n\n\n\n\nPaths, Folders, Directories (pwd)\n\n\nLinux/OSX\n\n\nWindows\n\n\n\n\n\n\nView a File (less, more)\n\n\nLinux/OSX\n\n\nWindows\n\n\n\n\n\n\nStream a File (cat)\n\n\nLinux/OSX\n\n\nWindows\n\n\nEdit a file (cat, nano, pico, vim)\n\n\n\n\n\n\nExiting Your Terminal (exit)\n\n\nLinux/OSX\n\n\nWindows\n\n\nUnix Bash References\n\n\nPowerShell References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Setup\n\n\nYou will be instructed to do three things:\n\n\n\n\nDo some things in your shell (command line, Terminal, PowerShell).\n\n\nLearn about what you just did.\n\n\nDo more on your own.\n\n\n\n\nFor this first exercise you\nll be expected to get your Terminal open.\n\n\nMac OS X\n\n\n\n\nHold down the \ncommand\n key and hit the spacebar.\n\n\nIn the top right the blue \nsearch bar\n will pop up.\n\n\nType: \nterminal\n\n\nClick on the terminal application that looks kind of like a black box.\n\n\nThis will open Terminal.\n\n\nYou can now go to your dock and CTRL-click to pull up the menu, then select \nOptions-\nKeep\n In dock.\n\n\n\n\nLinux\n\n\nLook through the menu for your window manager for anything named \nshell or \nterminal\n.\n\n\nWindows\n\n\nOn Windows we\nre going to use PowerShell. People used to work with a program called \ncmd.exe\n, but it\ns not nearly as usable as PowerShell. If you have Windows 7 or later, do this:\n\n\n\n\nClick Start.\n\n\nIn \nSearch programs and files\n type: \npowershell\n\n\nHit Enter. \n\n\n\n\nLinux/Mac OSX\n\n\nList of commands:\n\n\n\n\npwd\n; print working directory.\n\n\nhostname\n; my computer\ns network name.\n\n\nmkdir\n; make directory.\n\n\ncd\n; change directory.\n\n\nls\n; list directory.\n\n\nrmdir\n; remove directory.\n\n\npushd\n; push directory.\n\n\npopd\n; pop directory.\n\n\ncp\n; copy a file or directory.\n\n\nmv\n; move a file or directory.\n\n\nless\n; page through a file. :q to quit.\n\n\ncat\n; print the whole file.\n\n\nxargs\n; execute arguments.\n\n\nfind\n; find files.\n\n\ngrep\n; find things inside files.\n\n\nman\n; read a manual page.\n\n\napropos\n; find what man page is appropriate.\n\n\nenv\n; look at your environment.\n\n\necho\n; print some arguments.\n\n\nexport\n; export/set a new environment variable.\n\n\nexit\n; exit the shell.\n\n\nsudo\n; become super user root.\n\n\n\n\nWindows\n\n\nList of commands:\n\n\n\n\npwd\n; print working directory.\n\n\nhostname\n; my computer\ns network name.\n\n\nmkdir\n; make directory.\n\n\ncd\n; change directory.\n\n\nls\n; list directory.\n\n\nrmdir\n; remove directory.\n\n\npushd\n; push directory.\n\n\npopd\n; pop directory.\n\n\ncp\n; copy a file or directory.\n\n\nrobocopy\n; robust copy.\n\n\nmv\n; move a file or directory.\n\n\nmore\n; page through a file.\n\n\ntype\n; print the whole file.\n\n\nforfiles\n; run a command on lots of files.\n\n\ndir -r\n; find files.\n\n\nselect-string\n; find things inside files.\n\n\nhelp\n; read a manual page.\n\n\nhelpctr\n; find what man page is appropriate.\n\n\necho\n; print some arguments.\n\n\nset\n; export/set a new environment variable.\n\n\nexit\n; exit the shell.\n\n\nrunas\n; become super user root.\n\n\n\n\nPaths, Folders, Directories (pwd)\n\n\nYou do not type in the \n$\n (Unix) or \n (Windows). That\ns just me showing you my session so you can see what I got.\n\n\nYou type in the stuff after \n$ or \n, then hit Enter. So if I have \n$ pwd\n you type just \npwd\n and hit Enter.\n\n\nYou can then see what I have for output followed by another \n$\n or \n prompt. That content is the output and you should see the same output.\n\n\nLet\ns do a simple first command so you can get the hang of this:\n\n\nLinux/OSX\n\n\n$ pwd \n/Users/zedshaw\n$\n\n\n\n\nWindows\n\n\nPS C:\\\n pwd\n\nC:\\Users\\zed\n\nPS C:\\\n\n\n\n\n\nView a File (less, more)\n\n\nTo do this exercise you\nre going to do some work using the commands you know so far. You\nll also need a text editor that can make plain text (.txt) files. Here\ns what you do:\n\n\n\n\nOpen your text editor and type some stuff into a new file. On OS X this could be TextWrangler. On Windows this might be Notepad++. On Linux this could be gedit. Any editor will work.\n\n\nSave that file to your desktop and name it \ntest.txt\n.\n\n\nIn your shell use the commands you know to copy this file to your \ntemp\n directory that you\nve been working with.\n\n\n\n\nOnce you\nve done that, complete this exercise:\n\n\nLinux/OSX\n\n\n$ less test.txt\n[displays file here]\n$\n\n\n\n\nThat\ns it. To get out of \nless\n just type \n:q\n (as in quit).\n\n\nWindows\n\n\n more test.txt\n[displays file here]\n\n \n\n\n\n\nStream a File (cat)\n\n\nYou\nre going to do some more setup for this one so you get used to making files in one program and then accessing them from the command line. With the same text editor from the last exercise, create another file named \ntest2.txt\n but this time save it directly to your \ntemp\n directory:\n\n\nLinux/OSX\n\n\n$ less test2.txt\n[displays file here]\n$ cat test2.txt\nI am a fun guy.\nDont you know why?\nBecause I make poems,\nthat make babies cry.\n$ cat test.txt\nHi there this is cool.\n$\n\n\n\n\nWindows\n\n\n more test2.txt\n[displays file here]\n\n cat test2.txt\nI am a fun guy.\nDont you know why?\nBecause I make poems,\nthat make babies cry.\n\n cat test.txt\nHi there this is cool.\n\n \n\n\n\n\nEdit a file (cat, nano, pico, vim)\n\n\n\n\nUnix: Try \ncat test.txt test2.txt\n to concatenate the files on screen.\n\n\nWindows: Try \ncat test.txt,test2.txt\n.\n\n\ncat test.txt\n will print on screen.\n\n\ncat file1.txt \n file2.txt\n to copy.\n\n\ncat file1.txt \n file2.txt\n to append.\n\n\nAlso \nnano test.txt\n, \npico test.txt\n, and \nvim test.txt\n.\n\n\n\n\nExiting Your Terminal (exit)\n\n\nLinux/OSX\n\n\n$ exit\n\n\n\n\nWindows\n\n\n exit\n\n\n\n\nUnix Bash References\n\n\n\n\nReference Manual\n\n\n\n\nPowerShell References\n\n\n\n\n[Owner\ns Manual](http://te\n\n\nchnet.microsoft.com/en-us/library/ee221100.aspx)\n\n\nMaster PowerShell", 
            "title": "Command Line Crash Course"
        }, 
        {
            "location": "/Command Line Crash Course/#the-setup", 
            "text": "You will be instructed to do three things:   Do some things in your shell (command line, Terminal, PowerShell).  Learn about what you just did.  Do more on your own.   For this first exercise you ll be expected to get your Terminal open.", 
            "title": "The Setup"
        }, 
        {
            "location": "/Command Line Crash Course/#mac-os-x", 
            "text": "Hold down the  command  key and hit the spacebar.  In the top right the blue  search bar  will pop up.  Type:  terminal  Click on the terminal application that looks kind of like a black box.  This will open Terminal.  You can now go to your dock and CTRL-click to pull up the menu, then select  Options- Keep  In dock.", 
            "title": "Mac OS X"
        }, 
        {
            "location": "/Command Line Crash Course/#linux", 
            "text": "Look through the menu for your window manager for anything named  shell or  terminal .", 
            "title": "Linux"
        }, 
        {
            "location": "/Command Line Crash Course/#windows", 
            "text": "On Windows we re going to use PowerShell. People used to work with a program called  cmd.exe , but it s not nearly as usable as PowerShell. If you have Windows 7 or later, do this:   Click Start.  In  Search programs and files  type:  powershell  Hit Enter.", 
            "title": "Windows"
        }, 
        {
            "location": "/Command Line Crash Course/#linuxmac-osx", 
            "text": "List of commands:   pwd ; print working directory.  hostname ; my computer s network name.  mkdir ; make directory.  cd ; change directory.  ls ; list directory.  rmdir ; remove directory.  pushd ; push directory.  popd ; pop directory.  cp ; copy a file or directory.  mv ; move a file or directory.  less ; page through a file. :q to quit.  cat ; print the whole file.  xargs ; execute arguments.  find ; find files.  grep ; find things inside files.  man ; read a manual page.  apropos ; find what man page is appropriate.  env ; look at your environment.  echo ; print some arguments.  export ; export/set a new environment variable.  exit ; exit the shell.  sudo ; become super user root.", 
            "title": "Linux/Mac OSX"
        }, 
        {
            "location": "/Command Line Crash Course/#windows_1", 
            "text": "List of commands:   pwd ; print working directory.  hostname ; my computer s network name.  mkdir ; make directory.  cd ; change directory.  ls ; list directory.  rmdir ; remove directory.  pushd ; push directory.  popd ; pop directory.  cp ; copy a file or directory.  robocopy ; robust copy.  mv ; move a file or directory.  more ; page through a file.  type ; print the whole file.  forfiles ; run a command on lots of files.  dir -r ; find files.  select-string ; find things inside files.  help ; read a manual page.  helpctr ; find what man page is appropriate.  echo ; print some arguments.  set ; export/set a new environment variable.  exit ; exit the shell.  runas ; become super user root.", 
            "title": "Windows"
        }, 
        {
            "location": "/Command Line Crash Course/#paths-folders-directories-pwd", 
            "text": "You do not type in the  $  (Unix) or   (Windows). That s just me showing you my session so you can see what I got.  You type in the stuff after  $ or  , then hit Enter. So if I have  $ pwd  you type just  pwd  and hit Enter.  You can then see what I have for output followed by another  $  or   prompt. That content is the output and you should see the same output.  Let s do a simple first command so you can get the hang of this:", 
            "title": "Paths, Folders, Directories (pwd)"
        }, 
        {
            "location": "/Command Line Crash Course/#linuxosx", 
            "text": "$ pwd \n/Users/zedshaw\n$", 
            "title": "Linux/OSX"
        }, 
        {
            "location": "/Command Line Crash Course/#windows_2", 
            "text": "PS C:\\  pwd\n\nC:\\Users\\zed\n\nPS C:\\", 
            "title": "Windows"
        }, 
        {
            "location": "/Command Line Crash Course/#view-a-file-less-more", 
            "text": "To do this exercise you re going to do some work using the commands you know so far. You ll also need a text editor that can make plain text (.txt) files. Here s what you do:   Open your text editor and type some stuff into a new file. On OS X this could be TextWrangler. On Windows this might be Notepad++. On Linux this could be gedit. Any editor will work.  Save that file to your desktop and name it  test.txt .  In your shell use the commands you know to copy this file to your  temp  directory that you ve been working with.   Once you ve done that, complete this exercise:", 
            "title": "View a File (less, more)"
        }, 
        {
            "location": "/Command Line Crash Course/#linuxosx_1", 
            "text": "$ less test.txt\n[displays file here]\n$  That s it. To get out of  less  just type  :q  (as in quit).", 
            "title": "Linux/OSX"
        }, 
        {
            "location": "/Command Line Crash Course/#windows_3", 
            "text": "more test.txt\n[displays file here]", 
            "title": "Windows"
        }, 
        {
            "location": "/Command Line Crash Course/#stream-a-file-cat", 
            "text": "You re going to do some more setup for this one so you get used to making files in one program and then accessing them from the command line. With the same text editor from the last exercise, create another file named  test2.txt  but this time save it directly to your  temp  directory:", 
            "title": "Stream a File (cat)"
        }, 
        {
            "location": "/Command Line Crash Course/#linuxosx_2", 
            "text": "$ less test2.txt\n[displays file here]\n$ cat test2.txt\nI am a fun guy.\nDont you know why?\nBecause I make poems,\nthat make babies cry.\n$ cat test.txt\nHi there this is cool.\n$", 
            "title": "Linux/OSX"
        }, 
        {
            "location": "/Command Line Crash Course/#windows_4", 
            "text": "more test2.txt\n[displays file here]  cat test2.txt\nI am a fun guy.\nDont you know why?\nBecause I make poems,\nthat make babies cry.  cat test.txt\nHi there this is cool.", 
            "title": "Windows"
        }, 
        {
            "location": "/Command Line Crash Course/#edit-a-file-cat-nano-pico-vim", 
            "text": "Unix: Try  cat test.txt test2.txt  to concatenate the files on screen.  Windows: Try  cat test.txt,test2.txt .  cat test.txt  will print on screen.  cat file1.txt   file2.txt  to copy.  cat file1.txt   file2.txt  to append.  Also  nano test.txt ,  pico test.txt , and  vim test.txt .", 
            "title": "Edit a file (cat, nano, pico, vim)"
        }, 
        {
            "location": "/Command Line Crash Course/#exiting-your-terminal-exit", 
            "text": "", 
            "title": "Exiting Your Terminal (exit)"
        }, 
        {
            "location": "/Command Line Crash Course/#linuxosx_3", 
            "text": "$ exit", 
            "title": "Linux/OSX"
        }, 
        {
            "location": "/Command Line Crash Course/#windows_5", 
            "text": "exit", 
            "title": "Windows"
        }, 
        {
            "location": "/Command Line Crash Course/#unix-bash-references", 
            "text": "Reference Manual", 
            "title": "Unix Bash References"
        }, 
        {
            "location": "/Command Line Crash Course/#powershell-references", 
            "text": "[Owner s Manual](http://te  chnet.microsoft.com/en-us/library/ee221100.aspx)  Master PowerShell", 
            "title": "PowerShell References"
        }, 
        {
            "location": "/Command Shell Snippets/", 
            "text": "Command Shell Snippets\n\n\nForeword\n\n\nCode snippets.\n\n\n\n\nSort in alphabetical order:\n\n\n$ sort myfile.txt\n\n\n\n\nSort in numerical order:\n\n\n$ sort -n myfile.txt\n\n\n\n\nSort on multiple column (on column 2 in numerical order, then on column 1 in alphabetical order):\n\n\n$ sort myfile.txt -k2n -k1\n\n\n\n\nSort a comma-separated table:\n\n\n$ sort -k2 -k3 k1 -t ',' myfile.txt\n\n\n\n\nSort in reversed order:\n\n\n$ sort -r myfile.txt\n\n\n\n\nSort a pip-separated file in reverses order by the second column:\n\n\n$ sort myfile.txt -nrk 2 -st '|'\n\n\n\n\nSearch text files for lines matching regular expressions (regex) with \ngrep \n'matching string'\n \nsource\n:\n\n\n$ grep ArticleTitle webpage.html\n\n\n\n\nAn asterisk (\n*\n) indicates any character (the matching expression starts with \nAr\n and ends with \nle\n):\n\n\n$ grep Ar*le mytext.txt\n\n\n\n\nSearch for starting metacharacter \n:\n\n\n$ grep ^'\n' mytext.txt\n\n\n\n\nList the access rights for all files:\n\n\n$ ls -lag\n\n\n\n\nRun commands in background, kill the job running in the foreground, suspend the job running in the foreground, and background the suspended job:\n\n\n$ command \n\n\n$ ^C\n\n$ ^Z\n\n$ bg\n\n\n\n\nList the current jobs, foreground job number 1, and kills job number 1:\n\n\n$ jobs\n\n$ fg%1\n\n$ kill%1\n\n\n\n\nList current processes, and kill process number 26152:\n\n\n$ ps\n\n$ kill 26152\n\n\n\n\nHelp about commands:\n\n\n$ man \ncommand name\n\n\n$ whatis \ncommand name", 
            "title": "Command Shell Snippets"
        }, 
        {
            "location": "/The Linux Command Line/", 
            "text": "The Linux Command Line\n\n\nForeword\n\n\nNotes. The 537-page volume covers the same material as \nLinuxCommand.org\n, but in much greater detail. In addition to the basics of command line use and shell scripting, The Linux Command Line includes chapters on many common programs used on the command line, as well as more advanced topics.\n\n\n\n\nCONTENT\n\n\nThe Linux Command Line\n\n\n1.0 Learning the Shell\n\n\n2.0 Writing Shell Scripts\n\n\n3.0 Resources (some)\n\n\nAdventures\n\n\nShell Scripts\n\n\nManuals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.0 Learning the Shell\n\n\n1.1 What Is \nThe Shell\n?\n,\n\n\nterminal\n\n\n1.2 Navigation\n,\n\n\nfile, system, organization, working directory\n\n\n1.3 Looking Around\n,\n\n\nlist, file, size, group, owner, permission, create text file, long format, classify, examine\n\n\n1.4 A Guided Tour\n,\n\n\ndirectory, root, boot, etc, bin, usr, local, var, lib, home, root, tmp, dev, proc, media\n\n\n1.5 Manipulating Files\n,\n\n\ncopy, move, remove, create directory\n\n\n1.6 Working With Commands\n,\n\n\ntype, display information, which, locate, help, command, manual\n\n\n1.7 I/O Redirection\n,\n\n\ninput, output, pipeline, pipe, filter, sort, uniq, pattern, read text, grep, fmt, split, page break, header, footer, pr, first line, last line, head, tail, translate, tr, editor, sed, awk\n\n\n1.8 Expansion\n,\n\n\necho, pathname, quote, double-quote, escape, backslash\n\n\n1.9 Permissions\n,\n\n\npermission, chmod, su, sudo, chown, chgrp, directory\n\n\n1.10 Job Control\n,\n\n\njob, process, ps, kill, jobs, background, bg, foreground, fg\n\n\n2.0 Writing Shell Scripts\n\n\n2.1 Writing Your First Script And Getting It To Work\n,\n\n\neditor, vi, vim, emacs, nano, gedit, kwrite, path\n\n\n2.2 Editing The Scripts You Already Have\n,\n\n\nedit, shell, location, path, export path, alias, environmental variable, today\n\n\n2.3 Here Scripts\n,\n\n\nscript, here, shebang, output, append\n\n\n2.4 Variables\n,\n\n\nscript, variable, create, environment\n\n\n2.5 Command Substitution And Constants\n,\n\n\ncommand substitution, constant, assigning, assign\n\n\n2.6 Shell Functions\n,\n\n\nshell function, open page, head section, right title, close, body section, timestamp, right, time, macro, \n\n\n2.7 Some Real Work\n,\n\n\nuptime, time, show_uptime, drive, space, drive_space, home, space, home_space\n\n\n2.8 Flow Control - Part 1\n,\n\n\nif, test, testing, exit, clear\n\n\n2.9 Stay Out Of Trouble\n,\n\n\nempty variable, missing quote, isolating, isolate, run script, watch\n\n\n2.10 Keyboard Input And Arithmetic\n,\n\n\nread, arithmetic\n\n\n2.11 Flow Control - Part 2\n,\n\n\nbranch, branching, case, loop, while, building an menu\n\n\n2.12 Positional Parameters\n,\n\n\npositional parameters, command line, arguments, options, command line processor into script, interactive\n\n\n2.13 Flow Control - Part 3\n,\n\n\nloop, for, if, else, find, wc, system_info\n\n\n2.14 Errors And Signals And Traps (Oh My!) - Part 1\n,\n\n\ncheck, checking, exit status, error, and, or\n\n\n2.15 Errors And Signals And Traps (Oh My!) - Part 2\n,\n\n\nclean, cleaning, trap, kill, clean_up, temporary, file\n\n\n3.0 Resources (some)\n\n\nAdventures\n\n\n3.1 Midnight Commander\n\n\nA directory browser and file manager for command line users.\n\n\ngui, file, directory, manipulation, copy, move, rename, delete, permission, remote system, ftp, ssh, .tar, .zip, local, hotlist, name, content\n\n\n3.2 Terminal Multiplexers\n\n\nGive your terminal some serious muscle.\n\n\nlike gnome, konsole, gnu, desktop, multiple shell session single terminal, display, several computer\n\n\n3.3 Less Typing\n\n\nFingers getting tired? Let\ns look at ways to save our digits!\n\n\nalias, shell functions, script, program, alias, cli, editor, control\n\n\n3.4 More Redirection\n\n\nWe take a deeper look at this powerful feature.\n\n\ni/o, input, output, pipeline, pipe, exec\n\n\n3.5 tput\n\n\nOur scripts can have more visual appeal!\n\n\ntput, ncurses, terminal, manipulate, change, color, effect, text\n\n\n3.6 dialog\n\n\nLet\ns give our scripts a better user interface.\n\n\nincorporate, script, build list, calendar, checklist, directory, box, form, file select, gauge, pause, gui, simple, simplify\n\n\n3.7 AWK\n\n\nPattern scanning and text processing language. One of the truly classic Unix tools.\n\n\nawk, programming language, coding, code, script\n\n\n3.8 Other Shells\n\n\nWhile most Linux users rely on the bash shell every day, it\ns not the only game in town. Let\ns look at some of the others.\n\n\ndash, debian almquist shell, tcsh, texex c shell, ksh, korn shell, zsh, z shell\n\n\nShell Scripts\n\n\n3.9 new_script\n\n\nA bash shell script template generator. Use this script to help write your own shell scripts. Generated templates include useful shell functions, error and signal handling, and command-line option and argument parsing.\n\n\nscript, example, case\n\n\n3.10 my_cloud\n\n\nImplements a primitive cloud storage system using any available remote host running an SSH server.\n\n\nscript, example, case, dropbox\n\n\n3.11 photo2mail\n\n\nRe-sizes large image files (photos) for use as attachments to email messages, blog postings, etc.\n\n\nscript, example, case, read image file, convert, size\n\n\n3.12 program_list\n\n\nCreates an annotated list of programs in a directory. Useful for exploring your system.\n\n\nscript, example, case, program_list, list, whatis, what\n\n\nManuals\n\n\nGranneman, Scott, Linux Phrasebook, 2015\n\n\nParker, Steve, Shell Scripting: Expert Recipes for Linux, Bash and more, 2011\n\n\nWard, Brian, How Linux Works: What Every Superuser Should Know, 2014", 
            "title": "The Linux Command Line"
        }, 
        {
            "location": "/The Linux Command Line/#10-learning-the-shell", 
            "text": "1.1 What Is  The Shell ? ,  terminal  1.2 Navigation ,  file, system, organization, working directory  1.3 Looking Around ,  list, file, size, group, owner, permission, create text file, long format, classify, examine  1.4 A Guided Tour ,  directory, root, boot, etc, bin, usr, local, var, lib, home, root, tmp, dev, proc, media  1.5 Manipulating Files ,  copy, move, remove, create directory  1.6 Working With Commands ,  type, display information, which, locate, help, command, manual  1.7 I/O Redirection ,  input, output, pipeline, pipe, filter, sort, uniq, pattern, read text, grep, fmt, split, page break, header, footer, pr, first line, last line, head, tail, translate, tr, editor, sed, awk  1.8 Expansion ,  echo, pathname, quote, double-quote, escape, backslash  1.9 Permissions ,  permission, chmod, su, sudo, chown, chgrp, directory  1.10 Job Control ,  job, process, ps, kill, jobs, background, bg, foreground, fg", 
            "title": "1.0 Learning the Shell"
        }, 
        {
            "location": "/The Linux Command Line/#20-writing-shell-scripts", 
            "text": "2.1 Writing Your First Script And Getting It To Work ,  editor, vi, vim, emacs, nano, gedit, kwrite, path  2.2 Editing The Scripts You Already Have ,  edit, shell, location, path, export path, alias, environmental variable, today  2.3 Here Scripts ,  script, here, shebang, output, append  2.4 Variables ,  script, variable, create, environment  2.5 Command Substitution And Constants ,  command substitution, constant, assigning, assign  2.6 Shell Functions ,  shell function, open page, head section, right title, close, body section, timestamp, right, time, macro,   2.7 Some Real Work ,  uptime, time, show_uptime, drive, space, drive_space, home, space, home_space  2.8 Flow Control - Part 1 ,  if, test, testing, exit, clear  2.9 Stay Out Of Trouble ,  empty variable, missing quote, isolating, isolate, run script, watch  2.10 Keyboard Input And Arithmetic ,  read, arithmetic  2.11 Flow Control - Part 2 ,  branch, branching, case, loop, while, building an menu  2.12 Positional Parameters ,  positional parameters, command line, arguments, options, command line processor into script, interactive  2.13 Flow Control - Part 3 ,  loop, for, if, else, find, wc, system_info  2.14 Errors And Signals And Traps (Oh My!) - Part 1 ,  check, checking, exit status, error, and, or  2.15 Errors And Signals And Traps (Oh My!) - Part 2 ,  clean, cleaning, trap, kill, clean_up, temporary, file", 
            "title": "2.0 Writing Shell Scripts"
        }, 
        {
            "location": "/The Linux Command Line/#30-resources-some", 
            "text": "", 
            "title": "3.0 Resources (some)"
        }, 
        {
            "location": "/The Linux Command Line/#adventures", 
            "text": "3.1 Midnight Commander  A directory browser and file manager for command line users.  gui, file, directory, manipulation, copy, move, rename, delete, permission, remote system, ftp, ssh, .tar, .zip, local, hotlist, name, content  3.2 Terminal Multiplexers  Give your terminal some serious muscle.  like gnome, konsole, gnu, desktop, multiple shell session single terminal, display, several computer  3.3 Less Typing  Fingers getting tired? Let s look at ways to save our digits!  alias, shell functions, script, program, alias, cli, editor, control  3.4 More Redirection  We take a deeper look at this powerful feature.  i/o, input, output, pipeline, pipe, exec  3.5 tput  Our scripts can have more visual appeal!  tput, ncurses, terminal, manipulate, change, color, effect, text  3.6 dialog  Let s give our scripts a better user interface.  incorporate, script, build list, calendar, checklist, directory, box, form, file select, gauge, pause, gui, simple, simplify  3.7 AWK  Pattern scanning and text processing language. One of the truly classic Unix tools.  awk, programming language, coding, code, script  3.8 Other Shells  While most Linux users rely on the bash shell every day, it s not the only game in town. Let s look at some of the others.  dash, debian almquist shell, tcsh, texex c shell, ksh, korn shell, zsh, z shell", 
            "title": "Adventures"
        }, 
        {
            "location": "/The Linux Command Line/#shell-scripts", 
            "text": "3.9 new_script  A bash shell script template generator. Use this script to help write your own shell scripts. Generated templates include useful shell functions, error and signal handling, and command-line option and argument parsing.  script, example, case  3.10 my_cloud  Implements a primitive cloud storage system using any available remote host running an SSH server.  script, example, case, dropbox  3.11 photo2mail  Re-sizes large image files (photos) for use as attachments to email messages, blog postings, etc.  script, example, case, read image file, convert, size  3.12 program_list  Creates an annotated list of programs in a directory. Useful for exploring your system.  script, example, case, program_list, list, whatis, what", 
            "title": "Shell Scripts"
        }, 
        {
            "location": "/The Linux Command Line/#manuals", 
            "text": "Granneman, Scott, Linux Phrasebook, 2015  Parker, Steve, Shell Scripting: Expert Recipes for Linux, Bash and more, 2011  Ward, Brian, How Linux Works: What Every Superuser Should Know, 2014", 
            "title": "Manuals"
        }, 
        {
            "location": "/Ubuntu/", 
            "text": "Ubuntu\n\n\nForeword\n\n\nNotes.\n\n\n\n\nCONTENT\n\n\nUbuntu\n\n\nInstall Ubuntu 16.04\n\n\nFollowing the Installation\n\n\nUbuntu for desktops and for\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Ubuntu 16.04\n\n\n\n\nUbuntu\n.\n\n\n\n\nFollowing the Installation\n\n\n\n\nAdd a few things 1\n.\n\n\nAdd a few things 2\n.\n\n\nTweaks to do after installing\n.\n\n\nrsync\n\n\nrsync is a tool for efficiently copying and backing up data from one location (the source) to another (the destination). It is efficient because it only transfers files which are different between the source and destination directories.\n\n\n\n\n\n\n\n\nUbuntu for desktops and for\n\n\n\n\nTablets\n.\n\n\nMobile phones.\n\n\nServers.\n\n\nThe cloud.\n\n\nThe IoT.", 
            "title": "Ubuntu 16.04"
        }, 
        {
            "location": "/Ubuntu/#install-ubuntu-1604", 
            "text": "Ubuntu .", 
            "title": "Install Ubuntu 16.04"
        }, 
        {
            "location": "/Ubuntu/#following-the-installation", 
            "text": "Add a few things 1 .  Add a few things 2 .  Tweaks to do after installing .  rsync  rsync is a tool for efficiently copying and backing up data from one location (the source) to another (the destination). It is efficient because it only transfers files which are different between the source and destination directories.", 
            "title": "Following the Installation"
        }, 
        {
            "location": "/Ubuntu/#ubuntu-for-desktops-and-for", 
            "text": "Tablets .  Mobile phones.  Servers.  The cloud.  The IoT.", 
            "title": "Ubuntu for desktops and for..."
        }, 
        {
            "location": "/Codecademy Learn SQL/", 
            "text": "Codecademy Learn SQL\n\n\nForeword\n\n\nCode Snippets. From Codecademy.\n\n\n\n\nCONTENT\n\n\nCodecademy Learn SQL\n\n\nSpecifying Comments\n\n\n1, Manipulation\n\n\nProject Create Table\n\n\n\n\n\n\n2, Queries\n\n\nProject Writing Queries\n\n\n\n\n\n\n3, Aggregate Functions\n\n\nProject Fake Apps\n\n\n\n\n\n\n4, Multiple Tables\n\n\nRecap\n\n\nProject Querying Tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecifying Comments\n\n\n\n\nLine comment. This is indicated by two negative signs (eg. \n). The remainder of the text on the line is the comment.\n\n\nBlock comment. The start of the block comment is indicated by /*, the end of the comment by the same string. A block comment can cover text in part of a line, or can span multiple lines.\n\n\nRem or @. For Oracle, a line starting with either REM or @ is a comment line.\n\n\n\n\n1, Manipulation\n\n\nSQL, \nStructured Query Language\n, is a programming language designed to manage data stored in relational databases. SQL operates through simple, declarative statements. This keeps data accurate and secure, and helps maintain the integrity of databases, regardless of size.\n\n\nThe SQL language is widely used today across web frameworks and database applications. Knowing SQL gives you the freedom to explore your data, and the power to make better decisions. By learning SQL, you will also learn concepts that apply to nearly every data storage system.\n\n\nThe statements covered in this course, use SQLite Relational Database Management System (RDBMS). You can learn more about RDBMS\ns here. You can also access a glossary of all the SQL commands taught in this course here.\n\n\n--Show, create, use database\n\nSHOW databases;\nCREATE DATABASE dbname;\nSHOW databases;\nUSE dbname;\n\n\n\n\n--Show tables\n\nSHOW tables;\n\n\n\n\n--Create the table:\n\nCREATE TABLE table_name (\n    column_1 data_type, \n    column_2 data_type, \n    column_3 data_type\n  );\n\n\n\n\nCREATE TABLE celebs (id INTEGER, name TEXT, age INTEGER);\n\n\n\n\n--Add a row:\n\nINSERT INTO celebs (id,name,age) VALUES (1,\nJustin Bieber\n,21);\n\n\n\n\n--View:\n\nSELECT * From celebs;\n\n\n\n\n--Insert more rows:\n\nINSERT INTO celebs (id,name,age) VALUES (2,\nBeyonce Knowles\n,33);\nINSERT INTO celebs (id,name,age) VALUES (3,\nJeremy Lin\n,26);\nINSERT INTO celebs (id,name,age) VALUES (4,\nTaylor Swift\n,26);\n\n\n\n\n--View:\n\nSELECT name FROM celebs;\n\n\n\n\n--Edit a row:\n\nUPDATE celebs SET age = 22 WHERE id = 1;\n\n\n\n\n--View:\n\nSELECT * FROM celebs;\n\n\n\n\n--Add a new column:\n\nALTER TABLE celebs ADD COLUMN twitter_handle TEXT;\n\n\n\n\n--View:\n\nSELECT * FROM celebs;\n\n\n\n\n--Update the table:\n\nUPDATE celebs SET twitter_handle = '@taylorswift13' WHERE id = 4;\n\n\n\n\n--View:\n\nSELECT * FROM celebs;\n\n\n\n\n--Delete rows. NULL for missing or unknown data:\n\nDELETE FROM celebs WHERE twitter_handle IS NULL;\n\n\n\n\n--View:\n\nSELECT * FROM celebs;\n\n\n\n\nProject Create Table\n\n\nIn this project you will create your own friends table and add and delete data from it.\n\n\nThe instructions provided are a general guideline, but feel free to add more columns, insert more data, or create more tables.\n\n\nNotes: plan a project by drawing an ULM schema.\n\n\nCREATE TABLE friends (id INTEGER, name TEXT, birthday DATE);\n\nSELECT * FROM friends;\n\nDROP tables friends;\n\nCREATE TABLE friends (id INTEGER, name TEXT, birthday DATE);\n\nSHOW tables;\n\nINSERT INTO friends (id, name, birthday) VALUES (1,\nJane Doe\n,'1993-05-19');\nINSERT INTO friends (id, name, birthday) VALUES (2,\nJade Donot\n,'1995-06-12');\nINSERT INTO friends (id, name, birthday) VALUES (3,\nJack Doom\n,'1990-10-01');\nINSERT INTO friends (id, name, birthday) VALUES (4,\nJohn Doe\n,'1988-12-09');\n\nUPDATE friends SET name = \nJane Smith\n WHERE id = 1; \n\nALTER TABLE friends ADD COLUMN email TEXT;\n\nUPDATE friends SET email = \njdoe@example.com\n WHERE id = 1;\nUPDATE friends SET email = \njade@example.com\n WHERE id = 2;\nUPDATE friends SET email = \ndoom@example.com\n WHERE id = 3;\nUPDATE friends SET email = \njohndoe@example.com\n WHERE id = 4;\n\nDELETE FROM friends WHERE id = 1;\n\n\n\n\n2, Queries\n\n\nOne of the core purposes of the SQL language is to retrieve information stored in a database. This is commonly referred to as querying. Queries allow us to communicate with the database by asking questions and having the result set return data relevant to the question. In this lesson, you will be querying a database with one table named movies. Let\ns get started.\n\n\n--Find rows:\n\nSELECT name, imdb_rating FROM movies;\n\n\n\n\n--Find rows. SELECT DISTINCT for unique values in the result set. It filters out all duplicate values:\n\nSELECT DISTINCT genre FROM movies;\n\n\n\n\n--Find rows with WHERE:\n\nSELECT * FROM movies WHERE imdb_rating \n 8;\n\n\n\n\nClauses:\n\n\n\n\n= equals\n\n\n!= not equals\n\n\n\n\n\n\ngreater than\n\n\n\n\n\n\n less than\n\n\n\n\n\n\n= greater than or equal to\n\n\n\n\n\n\n= less than or equal to\n\n\n\n\n--Find rows with LIKE. Se_en represents a pattern with a wildcard character. The _ means you can substitute any individual character here without breaking the pattern. The names Seven and Se7en both match this pattern:\n\nSELECT * FROM movies WHERE name LIKE 'Se_en';\n\n\n\n\n--Find rows with LIKE. % is another wildcard character that can be used with LIKE. % is a wildcard character that matches zero or more missing letters in the pattern. A% matches all movies with names that begin with \nA\n. %a matches all movies that end with \na\n:\n\nSELECT * FROM movies WHERE name LIKE 'a%';\nSELECT * FROM movies WHERE name LIKE '%man%';\n\n\n\n\n--Find rows with BETWEEN. The values can be numbers, text or dates. Names that begin with letters \nA\n up to but not including \nJ\n. Years between 1990 up to and including 2000:\n\nSELECT * FROM movies WHERE name BETWEEN 'A' AND 'J';\nSELECT * FROM movies WHERE year BETWEEN 1990 AND 2000;\n\n\n\n\n--Find rows with BETWEEN, AND:\n\nSELECT * FROM movies WHERE year BETWEEN 1990 AND 2000 AND genre = 'comedy';\n\n\n\n\n--Find rows with BETWEEN, OR:\n\nSELECT * FROM movies WHERE genre = 'comedy' OR year \n 1980;\n\n\n\n\n--Find rows with BETWEEN, ORDERED BY, DESC, ASC:\n\nSELECT * FROM movies ORDER BY imdb_rating DESC;\n\n\n\n\n--Find rows, BETWEEN, ORDERED BY, ASC. LIMIT is a clause that lets you specify the maximum number of rows the result set will have:\n\nSELECT * FROM movies ORDER BY imdb_rating ASC LIMIT 3;\n\n\n\n\nProject Writing Queries\n\n\nIn this project you will write queries to retrieve information from the movies table.\n\n\nThe instructions provided are a general guideline, but feel free to experiment writing your own queries.\n\n\n--Return all of the unique years in the movies table.\n\nSELECT DISTINCT year FROM movies;\n\n--Return all of the unique years in the movies table sorted from oldest to newest.\n\nSELECT DISTINCT year FROM movies ORDER BY year ASC;\n\n--Return all movies that are dramas.\n\nSELECT * FROM movies WHERE genre = \ndrama\n;\n\n--Return all of the movies with names that contain \nbride\n.\n\nSELECT * FROM movies WHERE name LIKE '%Br%';\n\n--Return all of the movies that were made between 2000 and 2015.\n\nSELECT * FROM movies WHERE year \n= 2000 AND year \n= 2015;\n\n--Return all of the movies that were made in 1995 or have an IMDb rating of 9.\n\nSELECT * FROM movies WHERE year = 1995 OR imdb_rating = 9;\n\n--Return the name and IMDb rating of every movie made after 2009 in alphabetical order.\n\nSELECT name, imdb_rating FROM movies WHERE year \n 2009 ORDER BY name ASC;\n\n--Return 3 movies with an IMDb rating of 7.\n\nSELECT * FROM movies WHERE imdb_rating = 7 LIMIT 3;\n\n--Return 10 movies with an IMDb rating greater than 6, are comedies, were made after 1995, and sorted by IMDb rating from highest to lowest. (Hint: you can use AND more than once in a statement).\n\nSELECT * FROM movies WHERE imdb_rating \n 6 AND genre = 'comedy' AND year \n 1995 ORDER BY imdb_rating ASC LIMIT 10;\n\n--Return all movies named 'Cast Away'.\nSELECT * FROM movies WHERE name = 'Cast Away';\n\n--Return all movies with an IMDb rating not equal to 7.\nSELECT * FROM movies WHERE imdb_rating != 7;\n\n--Return all movies with a horror genre and an IMDb rating less than 6.\nSELECT * FROM movies WHERE genre = 'horror' AND imdb_rating \n 6;\n\n--Return 10 movies with an IMDb rating greater than 8 sorted by their genre.\nSELECT * FROM movies WHERE imdb_rating \n 8 ORDER BY genre ASC LIMIT 10;\n\n--Return all movies that include 'King' in the name.\nSELECT * FROM movies WHERE name LIKE '%King%';\n\n--Return all movies with names that end with the word 'Out'\nSELECT * FROM movies WHERE name LIKE '%Out';\n\n--Return all movies with names that begin with the word \nThe\n sorted by IMDb rating from highest to lowest\nSELECT * FROM movies WHERE name LIKE 'The%' ORDER BY imdb_rating DESC;\n\n--Return all of the movies.\nSELECT * FROM movies;\n\n--Return the name and id of each movie with an id greater than 125.\nSELECT name, id FROM movies WHERE id \n 125;\n\n--Return all movies with names that begin with 'X-Men'\nSELECT * FROM movies WHERE name LIKE 'X-Men%';\n\n--Return the first 10 movies sorted in reverse alphabetical order.\nSELECT * FROM movies DESC LIMIT 10;\n\n--Return the id, name, and genre of all movies that are romances.\nSELECT id, name, genre FROM movies WHERE genre = 'romance';\n\n--Return all of the Twilight movies in order from the year they were released from oldest to newest.\nSELECT * FROM movies WHERE name LIKE '%Twilight%' ORDER BY year ASC;\n\n--Return all of the movies that were released in 2012 that are comedies.\nSELECT * FROM movies WHERE year = 2012 AND genre = 'comedy';\n\n\n\n\n3, Aggregate Functions\n\n\nAggregate functions compute a single result from a set of input values. For instance, when we need the sum or average of a particular column, we can use aggregate functions to quickly compute it for us. We will be learning about different aggregate functions in this lesson.\n\n\nFor this lesson we have given you a table named fake_apps which is made up of data for fake mobile applications.\n\n\n--View:\n\nSELECT * FROM fake_apps;\n\n\n\n\n--Calculate the number of rows in a table. where the column is not NULL. Here, we want to count every row so we pass * as an argument:\n\nSELECT COUNT(*) FROM fake_apps;\n\n\n\n\n--Count * or columns):\n\nSELECT COUNT(*) FROM fake_apps WHERE price = 0;\n\n\n\n\n--Aggregate. GROUP BY is a clause in SQL that is only used with aggregate functions. It is used in collaboration with the SELECT statement to arrange identical data into groups. Here, our aggregate function is COUNT() and we are passing price as an argument to GROUP BY. SQL will count the total number of apps for each price in the table:\n\nSELECT price, COUNT(*) FROM fake_apps GROUP BY price;\n\n\n\n\n--It calculates the total number of apartments in each neighborhood in the cities table and groups them by neighborhood.\n\nSELECT neighborhood, SUM(apartments) FROM cities GROUP BY neighborhood;\n\n\n\n\n--Sum up, aggregate by category:\n\nSELECT category, SUM(downloads) FROM fake_apps GROUP BY category;\n\n\n\n\n--Find the maximum, minimum, global, categorical:\nSELECT MAX(downloads) FROM fake_apps;\nSELECT name, category, MAX(downloads) FROM fake_apps GROUP BY category;\n\nSELECT MIN(downloads) FROM fake_apps;\nSELECT name, category, MIN(downloads) FROM fake_apps GROUP BY category;\n\n\n\n\n--It returns the title, genre and checkout count for the book with the most checkouts in the library_books table.\n\nSELECT title, genre, MAX(checkouts) FROM library_books\nGROUP BY genre;\n\n\n\n\n--Find the average, or category average:\n\nSELECT AVG(downloads) FROM fake_apps;\nSELECT price, AVG(downloads) FROM fake_apps GROUP BY price;\n\n\n\n\n--Round up to x decimal(s):\n\nSELECT price, ROUND(AVG(downloads), 2) FROM fake_apps GROUP BY price;\n\n\n\n\nProject Fake Apps\n\n\nIn this project you will write queries with aggregate functions to retrieve information from the fake_apps table.\n\n\nThe instructions provided are a general guideline, but feel free to experiment writing your own queries, creating your own tables, or adding your own apps to the existing table!\n\n\n--Return the total number of apps in the table fake_apps.\n\nSELECT COUNT(*) FROM fake_apps;\n\n--Return the name, category, and price of the app that has been downloaded the least amount of times.\n\nSELECT name, category, price, MIN(downloads) FROM fake_apps;\n\n--Return the total number of apps for each category.\n\nSELECT COUNT(*) FROM fake_apps GROUP BY category;\n\n--Return the name and category of the app that has been downloaded the most amount of times.\n\nSELECT name, category, MAX(downloads) FROM fake_apps;\n\n--Return the name and category of the app that has been downloaded the least amount of times.\n\nSELECT name, category, MIN(downloads) FROM fake_apps;\n\n--Return the average price for an app in each category.\n\nSELECT AVG(price) FROM fake_apps GROUP BY category;\n\n--Return the average price for an app in each category. Round the averages to two decimal places.\n\nSELECT ROUND(AVG(price),2) FROM fake_apps GROUP BY category;\n\n--Return the maximum price for an app.\n\nSELECT MAX(price) FROM fake_apps;\n\n--Return the minimum number of downloads for an app.\n\nSELECT MIN(downloads) FROM fake_apps;\n\n--Return the total number of downloads for apps that belong to the Games category.\n\nSELECT MIN(downloads) FROM fake_apps WHERE category = 'Games';\n\n--Return the total number of apps that are free.\n\nSELECT COUNT(*) FROM fake_apps WHERE price = 0;\n\n--Return the total number of apps that cost 14.99.\n\nSELECT COUNT(*) FROM fake_apps WHERE price = 14.99;\n\n--Return the sum of the total number of downloads for apps that belong to the Music category.\n\nSELECT SUM(downloads) FROM fake_apps WHERE category = 'Music';\n\n--Return the sum of the total number of downloads for apps that belong to the Business category.\n\nSELECT SUM(downloads) FROM fake_apps WHERE category = 'Business';\n\n--Return the name of each category with the total number of apps that belong to it.\n\nSELECT name, COUNT(*) FROM fake_apps GROUP BY category;\n\n--Return the price and average number of downloads grouped by price.\n\nSELECT price, AVG(downloads) FROM fake_apps GROUP BY price;\n\n--Return the price and average number of downloads grouped by price. Round the averages to the nearest integer.\n\nSELECT ROUND(price,0), AVG(downloads) FROM fake_apps GROUP BY price;\n\n--Return the name and category and price of the most expensive app for each category.\n\nSELECT name, category, price, MAX(price) FROM fake_apps GROUP BY category;\n\n--Return the total number of apps whose name begin with the letter 'A'.\n\nSELECT COUNT(*) FROM fake_apps WHERE name LIKE 'A%';\n\n--Return the total number of downloads for apps belonging to the Sports or Health \n Fitness category.\n\nSELECT SUM(downloads) FROM fake_apps WHERE category = 'Sports' OR category = 'Health \n Fitness';\n\n\n\n\n4, Multiple Tables\n\n\nMost of the time, data is distributed across multiple tables in the database. Imagine a database with two tables, artists and albums. An artist can produce many different albums, and an album is produced by an artist.\n\n\nThe data in these tables are related to each other. Through SQL, we can write queries that combine data from multiple tables that are related to one another. This is one of the most powerful features of relational databases.\n\n\n--We have created a table named albums for you. Create a second table named artists. Using the CREATE TABLE statement we added a PRIMARY KEY to the id column. A primary key serves as a unique identifier for each row or record in a given table. The primary key is literally an id value for a record. We're going to use this value to connect artists to the albums they have produced. By specifying that the id column is the PRIMARY KEY, SQL makes sure that none of the values in this column are NULL and each value in this column is unique:\n\nCREATE TABLE artists(id INTEGER PRIMARY KEY, name TEXT);\n\n--View both tables:\n\nSELECT * FROM albums;\nSELECT * FROM artists;\n\n\n\n\n--Query:\n\nSELECT * FROM artists WHERE id = 3;\n\n\n\n\n--A foreign key is a column that contains the primary key of another table in the database. We use foreign keys and primary keys to connect rows in two different tables. One table's foreign key holds the value of another table's primary key. Unlike primary keys, foreign keys do not need to be unique and can be NULL. Here, artist_id is a foreign key in the albums table. We can see that Michael Jackson has an id of 3 in the artists table. All of the albums by Michael Jackson also have a 3 in the artist_id column in the albums table. This is how SQL is linking data between the two tables. The relationship between the artists table and the albums table is the id value of the artists.\n\nSELECT * FROM albums WHERE artist_id = 3;\n\n\n\n\n--One way to query multiple tables is to write a SELECT statement with multiple table names separated by a comma. This is also known as a CROSS JOIN. Here, albums and artists are the different tables we are querying. When querying more than one table, column names need to be specified by table_name.column_name. Here, the result set includes the name and year columns from the albums table and the name column from the artists table. Unfortunately the result of this cross join is not very useful. It combines every row of the artists table with every row of the albums table. It would be more useful to only combine the rows where the album was created by the artist.\n\nSELECT albums.name, albums.year, artists.name FROM albums, artists;\n\n\n\n\n--In SQL, joins are used to combine rows from two or more tables. The most common type of join in SQL is an INNER JOIN. An inner join will combine rows from different tables if the join condition is true. Let's look at the syntax to see how it works. SELECT * specifies the columns our result set will have. Here, we want to include every column in both tables. FROM albums specifies the first table we are querying. JOIN artists ON specifies the type of join we are going to use as well as the name of the second table. Here, we want to do an inner join and the second table we want to query is artists. albums.artist_id = artists.id is the join condition that describes how the two tables are related to each other. Here, SQL uses the foreign key column artist_id in the albums table to match it with exactly one row in the artists table with the same value in the id column. We know it will only match one row in the artists table because id is the PRIMARY KEY of artists.\n\nSELECT * FROM albums JOIN artists ON albums.artist_id = artists.id;\n\n\n\n\n--OUTER JOINS (LEFT or RIGHT) also combine rows from two or more tables, but unlike inner joins, they do not require the join condition to be met. Instead, every row in the left table is returned in the result set, and if the join condition is not met, then NULL values are used to fill in the columns from the right table. The left table is simply the first table that appears in the statement. Here, the left table is albums. Likewise, the right table is the second table that appears. Here, artists is the right table.\n\nSELECT * FROM albums LEFT JOIN artists ON albums.artist_id = artists.id;\n\n\n\n\n--AS is a keyword in SQL that allows you to rename a column or table using an alias. The new name can be anything you want as long as you put it inside of single quotes. Here we want to rename the albums.name column as 'Album', and the artists.name column as 'Artist'. It is important to note that the columns have not been renamed in either table. The aliases only appear in the result set.\n\nSELECT albums.name, albums.year, artists.name FROM albums JOIN artists ON albums.artist_id = artists.id WHERE albums.year \n 1980;\n\n\n\n\nSELECT albums.name AS 'Album', albums.year, artists.name AS 'Artist' FROM albums JOIN artists ON albums.artist_id = artists.id WHERE albums.year \n 1980;\n\n\n\n\nRecap\n\n\n\n\nPrimary Key is a column that serves a unique identifier for row in the table. Values in this column must be unique and cannot be NULL.\n\n\nForeign Key is a column that contains the primary key to another table in the database. It is used to identify a particular row in the referenced table.\n\n\nJoins are used in SQL to combine data from multiple tables.\n\n\nINNER JOIN will combine rows from different tables if the join condition is true.\n\n\nLEFT OUTER JOIN will return every row in the left table, and if the join condition is not met, NULL values are used to fill in the columns from the right table.\n\n\nAS is a keyword in SQL that allows you to rename a column or table in the result set using an alias.\n\n\n\n\nProject Querying Tables\n\n\nIn this project you will practice querying multiple tables using joins.\n\n\nThe instructions provided are a general guideline, but feel free to experiment creating your own tables, adding data, and writing more queries.\n\n\n--Create a table named tracks with an id, title, and album_id column. The id column should be the PRIMARY KEY.\n\nCREATE TABLE tracks (id INTEGER PRIMARY KEY, title TEXT, albums_id INTEGER);\n\n--\nSmooth Criminal\n is a track from Michael Jackson's \nBad\n album. Add this track to the database.\n\nINSERT INTO tracks (id,title,albums_id) VALUES (1,\nSmooth Criminal\n, 8);\n\n--Add more tracks to the database.\nINSERT INTO tracks (id,title,albums_id) VALUES (2,\naaa\n, 1);\nINSERT INTO tracks (id,title,albums_id) VALUES (3,\nbbb\n, 1);\nINSERT INTO tracks (id,title,albums_id) VALUES (4,\nccc\n, 8);\nINSERT INTO tracks (id,title,albums_id) VALUES (5,\nddd\n, 8);\nINSERT INTO tracks (id,title,albums_id) VALUES (6,\neee\n, 8);\n\n--Combine the albums and tracks tables using an INNER JOIN. Order the query by album_id.\n\nSELECT * FROM albums JOIN tracks ON albums.artist_id = tracks.id;\n\n--Combine the albums and artists table using a LEFT OUTER JOIN. Let albums be the left table.\n\nSELECT * FROM albums LEFT JOIN artists ON albums.artist_id = artists.id;\n\n--Combine the albums and artists table using a LEFT OUTER JOIN. Let artists be the left table.\n\nSELECT * FROM artists LEFT JOIN albums ON albums.artist_id = artists.id;\n\n--Use any join you like to combine the albums and tracks table. Rename the album_id column to Albums.\n\nSELECT * FROM albums LEFT JOIN tracks ON albums.artist_id = tracks.id;", 
            "title": "Codecademy Learn SQL"
        }, 
        {
            "location": "/Codecademy Learn SQL/#specifying-comments", 
            "text": "Line comment. This is indicated by two negative signs (eg.  ). The remainder of the text on the line is the comment.  Block comment. The start of the block comment is indicated by /*, the end of the comment by the same string. A block comment can cover text in part of a line, or can span multiple lines.  Rem or @. For Oracle, a line starting with either REM or @ is a comment line.", 
            "title": "Specifying Comments"
        }, 
        {
            "location": "/Codecademy Learn SQL/#1-manipulation", 
            "text": "SQL,  Structured Query Language , is a programming language designed to manage data stored in relational databases. SQL operates through simple, declarative statements. This keeps data accurate and secure, and helps maintain the integrity of databases, regardless of size.  The SQL language is widely used today across web frameworks and database applications. Knowing SQL gives you the freedom to explore your data, and the power to make better decisions. By learning SQL, you will also learn concepts that apply to nearly every data storage system.  The statements covered in this course, use SQLite Relational Database Management System (RDBMS). You can learn more about RDBMS s here. You can also access a glossary of all the SQL commands taught in this course here.  --Show, create, use database\n\nSHOW databases;\nCREATE DATABASE dbname;\nSHOW databases;\nUSE dbname;  --Show tables\n\nSHOW tables;  --Create the table:\n\nCREATE TABLE table_name (\n    column_1 data_type, \n    column_2 data_type, \n    column_3 data_type\n  );  CREATE TABLE celebs (id INTEGER, name TEXT, age INTEGER);  --Add a row:\n\nINSERT INTO celebs (id,name,age) VALUES (1, Justin Bieber ,21);  --View:\n\nSELECT * From celebs;  --Insert more rows:\n\nINSERT INTO celebs (id,name,age) VALUES (2, Beyonce Knowles ,33);\nINSERT INTO celebs (id,name,age) VALUES (3, Jeremy Lin ,26);\nINSERT INTO celebs (id,name,age) VALUES (4, Taylor Swift ,26);  --View:\n\nSELECT name FROM celebs;  --Edit a row:\n\nUPDATE celebs SET age = 22 WHERE id = 1;  --View:\n\nSELECT * FROM celebs;  --Add a new column:\n\nALTER TABLE celebs ADD COLUMN twitter_handle TEXT;  --View:\n\nSELECT * FROM celebs;  --Update the table:\n\nUPDATE celebs SET twitter_handle = '@taylorswift13' WHERE id = 4;  --View:\n\nSELECT * FROM celebs;  --Delete rows. NULL for missing or unknown data:\n\nDELETE FROM celebs WHERE twitter_handle IS NULL;  --View:\n\nSELECT * FROM celebs;", 
            "title": "1, Manipulation"
        }, 
        {
            "location": "/Codecademy Learn SQL/#project-create-table", 
            "text": "In this project you will create your own friends table and add and delete data from it.  The instructions provided are a general guideline, but feel free to add more columns, insert more data, or create more tables.  Notes: plan a project by drawing an ULM schema.  CREATE TABLE friends (id INTEGER, name TEXT, birthday DATE);\n\nSELECT * FROM friends;\n\nDROP tables friends;\n\nCREATE TABLE friends (id INTEGER, name TEXT, birthday DATE);\n\nSHOW tables;\n\nINSERT INTO friends (id, name, birthday) VALUES (1, Jane Doe ,'1993-05-19');\nINSERT INTO friends (id, name, birthday) VALUES (2, Jade Donot ,'1995-06-12');\nINSERT INTO friends (id, name, birthday) VALUES (3, Jack Doom ,'1990-10-01');\nINSERT INTO friends (id, name, birthday) VALUES (4, John Doe ,'1988-12-09');\n\nUPDATE friends SET name =  Jane Smith  WHERE id = 1; \n\nALTER TABLE friends ADD COLUMN email TEXT;\n\nUPDATE friends SET email =  jdoe@example.com  WHERE id = 1;\nUPDATE friends SET email =  jade@example.com  WHERE id = 2;\nUPDATE friends SET email =  doom@example.com  WHERE id = 3;\nUPDATE friends SET email =  johndoe@example.com  WHERE id = 4;\n\nDELETE FROM friends WHERE id = 1;", 
            "title": "Project Create Table"
        }, 
        {
            "location": "/Codecademy Learn SQL/#2-queries", 
            "text": "One of the core purposes of the SQL language is to retrieve information stored in a database. This is commonly referred to as querying. Queries allow us to communicate with the database by asking questions and having the result set return data relevant to the question. In this lesson, you will be querying a database with one table named movies. Let s get started.  --Find rows:\n\nSELECT name, imdb_rating FROM movies;  --Find rows. SELECT DISTINCT for unique values in the result set. It filters out all duplicate values:\n\nSELECT DISTINCT genre FROM movies;  --Find rows with WHERE:\n\nSELECT * FROM movies WHERE imdb_rating   8;  Clauses:   = equals  != not equals    greater than     less than    = greater than or equal to    = less than or equal to   --Find rows with LIKE. Se_en represents a pattern with a wildcard character. The _ means you can substitute any individual character here without breaking the pattern. The names Seven and Se7en both match this pattern:\n\nSELECT * FROM movies WHERE name LIKE 'Se_en';  --Find rows with LIKE. % is another wildcard character that can be used with LIKE. % is a wildcard character that matches zero or more missing letters in the pattern. A% matches all movies with names that begin with  A . %a matches all movies that end with  a :\n\nSELECT * FROM movies WHERE name LIKE 'a%';\nSELECT * FROM movies WHERE name LIKE '%man%';  --Find rows with BETWEEN. The values can be numbers, text or dates. Names that begin with letters  A  up to but not including  J . Years between 1990 up to and including 2000:\n\nSELECT * FROM movies WHERE name BETWEEN 'A' AND 'J';\nSELECT * FROM movies WHERE year BETWEEN 1990 AND 2000;  --Find rows with BETWEEN, AND:\n\nSELECT * FROM movies WHERE year BETWEEN 1990 AND 2000 AND genre = 'comedy';  --Find rows with BETWEEN, OR:\n\nSELECT * FROM movies WHERE genre = 'comedy' OR year   1980;  --Find rows with BETWEEN, ORDERED BY, DESC, ASC:\n\nSELECT * FROM movies ORDER BY imdb_rating DESC;  --Find rows, BETWEEN, ORDERED BY, ASC. LIMIT is a clause that lets you specify the maximum number of rows the result set will have:\n\nSELECT * FROM movies ORDER BY imdb_rating ASC LIMIT 3;", 
            "title": "2, Queries"
        }, 
        {
            "location": "/Codecademy Learn SQL/#project-writing-queries", 
            "text": "In this project you will write queries to retrieve information from the movies table.  The instructions provided are a general guideline, but feel free to experiment writing your own queries.  --Return all of the unique years in the movies table.\n\nSELECT DISTINCT year FROM movies;\n\n--Return all of the unique years in the movies table sorted from oldest to newest.\n\nSELECT DISTINCT year FROM movies ORDER BY year ASC;\n\n--Return all movies that are dramas.\n\nSELECT * FROM movies WHERE genre =  drama ;\n\n--Return all of the movies with names that contain  bride .\n\nSELECT * FROM movies WHERE name LIKE '%Br%';\n\n--Return all of the movies that were made between 2000 and 2015.\n\nSELECT * FROM movies WHERE year  = 2000 AND year  = 2015;\n\n--Return all of the movies that were made in 1995 or have an IMDb rating of 9.\n\nSELECT * FROM movies WHERE year = 1995 OR imdb_rating = 9;\n\n--Return the name and IMDb rating of every movie made after 2009 in alphabetical order.\n\nSELECT name, imdb_rating FROM movies WHERE year   2009 ORDER BY name ASC;\n\n--Return 3 movies with an IMDb rating of 7.\n\nSELECT * FROM movies WHERE imdb_rating = 7 LIMIT 3;\n\n--Return 10 movies with an IMDb rating greater than 6, are comedies, were made after 1995, and sorted by IMDb rating from highest to lowest. (Hint: you can use AND more than once in a statement).\n\nSELECT * FROM movies WHERE imdb_rating   6 AND genre = 'comedy' AND year   1995 ORDER BY imdb_rating ASC LIMIT 10;\n\n--Return all movies named 'Cast Away'.\nSELECT * FROM movies WHERE name = 'Cast Away';\n\n--Return all movies with an IMDb rating not equal to 7.\nSELECT * FROM movies WHERE imdb_rating != 7;\n\n--Return all movies with a horror genre and an IMDb rating less than 6.\nSELECT * FROM movies WHERE genre = 'horror' AND imdb_rating   6;\n\n--Return 10 movies with an IMDb rating greater than 8 sorted by their genre.\nSELECT * FROM movies WHERE imdb_rating   8 ORDER BY genre ASC LIMIT 10;\n\n--Return all movies that include 'King' in the name.\nSELECT * FROM movies WHERE name LIKE '%King%';\n\n--Return all movies with names that end with the word 'Out'\nSELECT * FROM movies WHERE name LIKE '%Out';\n\n--Return all movies with names that begin with the word  The  sorted by IMDb rating from highest to lowest\nSELECT * FROM movies WHERE name LIKE 'The%' ORDER BY imdb_rating DESC;\n\n--Return all of the movies.\nSELECT * FROM movies;\n\n--Return the name and id of each movie with an id greater than 125.\nSELECT name, id FROM movies WHERE id   125;\n\n--Return all movies with names that begin with 'X-Men'\nSELECT * FROM movies WHERE name LIKE 'X-Men%';\n\n--Return the first 10 movies sorted in reverse alphabetical order.\nSELECT * FROM movies DESC LIMIT 10;\n\n--Return the id, name, and genre of all movies that are romances.\nSELECT id, name, genre FROM movies WHERE genre = 'romance';\n\n--Return all of the Twilight movies in order from the year they were released from oldest to newest.\nSELECT * FROM movies WHERE name LIKE '%Twilight%' ORDER BY year ASC;\n\n--Return all of the movies that were released in 2012 that are comedies.\nSELECT * FROM movies WHERE year = 2012 AND genre = 'comedy';", 
            "title": "Project Writing Queries"
        }, 
        {
            "location": "/Codecademy Learn SQL/#3-aggregate-functions", 
            "text": "Aggregate functions compute a single result from a set of input values. For instance, when we need the sum or average of a particular column, we can use aggregate functions to quickly compute it for us. We will be learning about different aggregate functions in this lesson.  For this lesson we have given you a table named fake_apps which is made up of data for fake mobile applications.  --View:\n\nSELECT * FROM fake_apps;  --Calculate the number of rows in a table. where the column is not NULL. Here, we want to count every row so we pass * as an argument:\n\nSELECT COUNT(*) FROM fake_apps;  --Count * or columns):\n\nSELECT COUNT(*) FROM fake_apps WHERE price = 0;  --Aggregate. GROUP BY is a clause in SQL that is only used with aggregate functions. It is used in collaboration with the SELECT statement to arrange identical data into groups. Here, our aggregate function is COUNT() and we are passing price as an argument to GROUP BY. SQL will count the total number of apps for each price in the table:\n\nSELECT price, COUNT(*) FROM fake_apps GROUP BY price;  --It calculates the total number of apartments in each neighborhood in the cities table and groups them by neighborhood.\n\nSELECT neighborhood, SUM(apartments) FROM cities GROUP BY neighborhood;  --Sum up, aggregate by category:\n\nSELECT category, SUM(downloads) FROM fake_apps GROUP BY category;  --Find the maximum, minimum, global, categorical:\nSELECT MAX(downloads) FROM fake_apps;\nSELECT name, category, MAX(downloads) FROM fake_apps GROUP BY category;\n\nSELECT MIN(downloads) FROM fake_apps;\nSELECT name, category, MIN(downloads) FROM fake_apps GROUP BY category;  --It returns the title, genre and checkout count for the book with the most checkouts in the library_books table.\n\nSELECT title, genre, MAX(checkouts) FROM library_books\nGROUP BY genre;  --Find the average, or category average:\n\nSELECT AVG(downloads) FROM fake_apps;\nSELECT price, AVG(downloads) FROM fake_apps GROUP BY price;  --Round up to x decimal(s):\n\nSELECT price, ROUND(AVG(downloads), 2) FROM fake_apps GROUP BY price;", 
            "title": "3, Aggregate Functions"
        }, 
        {
            "location": "/Codecademy Learn SQL/#project-fake-apps", 
            "text": "In this project you will write queries with aggregate functions to retrieve information from the fake_apps table.  The instructions provided are a general guideline, but feel free to experiment writing your own queries, creating your own tables, or adding your own apps to the existing table!  --Return the total number of apps in the table fake_apps.\n\nSELECT COUNT(*) FROM fake_apps;\n\n--Return the name, category, and price of the app that has been downloaded the least amount of times.\n\nSELECT name, category, price, MIN(downloads) FROM fake_apps;\n\n--Return the total number of apps for each category.\n\nSELECT COUNT(*) FROM fake_apps GROUP BY category;\n\n--Return the name and category of the app that has been downloaded the most amount of times.\n\nSELECT name, category, MAX(downloads) FROM fake_apps;\n\n--Return the name and category of the app that has been downloaded the least amount of times.\n\nSELECT name, category, MIN(downloads) FROM fake_apps;\n\n--Return the average price for an app in each category.\n\nSELECT AVG(price) FROM fake_apps GROUP BY category;\n\n--Return the average price for an app in each category. Round the averages to two decimal places.\n\nSELECT ROUND(AVG(price),2) FROM fake_apps GROUP BY category;\n\n--Return the maximum price for an app.\n\nSELECT MAX(price) FROM fake_apps;\n\n--Return the minimum number of downloads for an app.\n\nSELECT MIN(downloads) FROM fake_apps;\n\n--Return the total number of downloads for apps that belong to the Games category.\n\nSELECT MIN(downloads) FROM fake_apps WHERE category = 'Games';\n\n--Return the total number of apps that are free.\n\nSELECT COUNT(*) FROM fake_apps WHERE price = 0;\n\n--Return the total number of apps that cost 14.99.\n\nSELECT COUNT(*) FROM fake_apps WHERE price = 14.99;\n\n--Return the sum of the total number of downloads for apps that belong to the Music category.\n\nSELECT SUM(downloads) FROM fake_apps WHERE category = 'Music';\n\n--Return the sum of the total number of downloads for apps that belong to the Business category.\n\nSELECT SUM(downloads) FROM fake_apps WHERE category = 'Business';\n\n--Return the name of each category with the total number of apps that belong to it.\n\nSELECT name, COUNT(*) FROM fake_apps GROUP BY category;\n\n--Return the price and average number of downloads grouped by price.\n\nSELECT price, AVG(downloads) FROM fake_apps GROUP BY price;\n\n--Return the price and average number of downloads grouped by price. Round the averages to the nearest integer.\n\nSELECT ROUND(price,0), AVG(downloads) FROM fake_apps GROUP BY price;\n\n--Return the name and category and price of the most expensive app for each category.\n\nSELECT name, category, price, MAX(price) FROM fake_apps GROUP BY category;\n\n--Return the total number of apps whose name begin with the letter 'A'.\n\nSELECT COUNT(*) FROM fake_apps WHERE name LIKE 'A%';\n\n--Return the total number of downloads for apps belonging to the Sports or Health   Fitness category.\n\nSELECT SUM(downloads) FROM fake_apps WHERE category = 'Sports' OR category = 'Health   Fitness';", 
            "title": "Project Fake Apps"
        }, 
        {
            "location": "/Codecademy Learn SQL/#4-multiple-tables", 
            "text": "Most of the time, data is distributed across multiple tables in the database. Imagine a database with two tables, artists and albums. An artist can produce many different albums, and an album is produced by an artist.  The data in these tables are related to each other. Through SQL, we can write queries that combine data from multiple tables that are related to one another. This is one of the most powerful features of relational databases.  --We have created a table named albums for you. Create a second table named artists. Using the CREATE TABLE statement we added a PRIMARY KEY to the id column. A primary key serves as a unique identifier for each row or record in a given table. The primary key is literally an id value for a record. We're going to use this value to connect artists to the albums they have produced. By specifying that the id column is the PRIMARY KEY, SQL makes sure that none of the values in this column are NULL and each value in this column is unique:\n\nCREATE TABLE artists(id INTEGER PRIMARY KEY, name TEXT);\n\n--View both tables:\n\nSELECT * FROM albums;\nSELECT * FROM artists;  --Query:\n\nSELECT * FROM artists WHERE id = 3;  --A foreign key is a column that contains the primary key of another table in the database. We use foreign keys and primary keys to connect rows in two different tables. One table's foreign key holds the value of another table's primary key. Unlike primary keys, foreign keys do not need to be unique and can be NULL. Here, artist_id is a foreign key in the albums table. We can see that Michael Jackson has an id of 3 in the artists table. All of the albums by Michael Jackson also have a 3 in the artist_id column in the albums table. This is how SQL is linking data between the two tables. The relationship between the artists table and the albums table is the id value of the artists.\n\nSELECT * FROM albums WHERE artist_id = 3;  --One way to query multiple tables is to write a SELECT statement with multiple table names separated by a comma. This is also known as a CROSS JOIN. Here, albums and artists are the different tables we are querying. When querying more than one table, column names need to be specified by table_name.column_name. Here, the result set includes the name and year columns from the albums table and the name column from the artists table. Unfortunately the result of this cross join is not very useful. It combines every row of the artists table with every row of the albums table. It would be more useful to only combine the rows where the album was created by the artist.\n\nSELECT albums.name, albums.year, artists.name FROM albums, artists;  --In SQL, joins are used to combine rows from two or more tables. The most common type of join in SQL is an INNER JOIN. An inner join will combine rows from different tables if the join condition is true. Let's look at the syntax to see how it works. SELECT * specifies the columns our result set will have. Here, we want to include every column in both tables. FROM albums specifies the first table we are querying. JOIN artists ON specifies the type of join we are going to use as well as the name of the second table. Here, we want to do an inner join and the second table we want to query is artists. albums.artist_id = artists.id is the join condition that describes how the two tables are related to each other. Here, SQL uses the foreign key column artist_id in the albums table to match it with exactly one row in the artists table with the same value in the id column. We know it will only match one row in the artists table because id is the PRIMARY KEY of artists.\n\nSELECT * FROM albums JOIN artists ON albums.artist_id = artists.id;  --OUTER JOINS (LEFT or RIGHT) also combine rows from two or more tables, but unlike inner joins, they do not require the join condition to be met. Instead, every row in the left table is returned in the result set, and if the join condition is not met, then NULL values are used to fill in the columns from the right table. The left table is simply the first table that appears in the statement. Here, the left table is albums. Likewise, the right table is the second table that appears. Here, artists is the right table.\n\nSELECT * FROM albums LEFT JOIN artists ON albums.artist_id = artists.id;  --AS is a keyword in SQL that allows you to rename a column or table using an alias. The new name can be anything you want as long as you put it inside of single quotes. Here we want to rename the albums.name column as 'Album', and the artists.name column as 'Artist'. It is important to note that the columns have not been renamed in either table. The aliases only appear in the result set.\n\nSELECT albums.name, albums.year, artists.name FROM albums JOIN artists ON albums.artist_id = artists.id WHERE albums.year   1980;  SELECT albums.name AS 'Album', albums.year, artists.name AS 'Artist' FROM albums JOIN artists ON albums.artist_id = artists.id WHERE albums.year   1980;", 
            "title": "4, Multiple Tables"
        }, 
        {
            "location": "/Codecademy Learn SQL/#recap", 
            "text": "Primary Key is a column that serves a unique identifier for row in the table. Values in this column must be unique and cannot be NULL.  Foreign Key is a column that contains the primary key to another table in the database. It is used to identify a particular row in the referenced table.  Joins are used in SQL to combine data from multiple tables.  INNER JOIN will combine rows from different tables if the join condition is true.  LEFT OUTER JOIN will return every row in the left table, and if the join condition is not met, NULL values are used to fill in the columns from the right table.  AS is a keyword in SQL that allows you to rename a column or table in the result set using an alias.", 
            "title": "Recap"
        }, 
        {
            "location": "/Codecademy Learn SQL/#project-querying-tables", 
            "text": "In this project you will practice querying multiple tables using joins.  The instructions provided are a general guideline, but feel free to experiment creating your own tables, adding data, and writing more queries.  --Create a table named tracks with an id, title, and album_id column. The id column should be the PRIMARY KEY.\n\nCREATE TABLE tracks (id INTEGER PRIMARY KEY, title TEXT, albums_id INTEGER);\n\n-- Smooth Criminal  is a track from Michael Jackson's  Bad  album. Add this track to the database.\n\nINSERT INTO tracks (id,title,albums_id) VALUES (1, Smooth Criminal , 8);\n\n--Add more tracks to the database.\nINSERT INTO tracks (id,title,albums_id) VALUES (2, aaa , 1);\nINSERT INTO tracks (id,title,albums_id) VALUES (3, bbb , 1);\nINSERT INTO tracks (id,title,albums_id) VALUES (4, ccc , 8);\nINSERT INTO tracks (id,title,albums_id) VALUES (5, ddd , 8);\nINSERT INTO tracks (id,title,albums_id) VALUES (6, eee , 8);\n\n--Combine the albums and tracks tables using an INNER JOIN. Order the query by album_id.\n\nSELECT * FROM albums JOIN tracks ON albums.artist_id = tracks.id;\n\n--Combine the albums and artists table using a LEFT OUTER JOIN. Let albums be the left table.\n\nSELECT * FROM albums LEFT JOIN artists ON albums.artist_id = artists.id;\n\n--Combine the albums and artists table using a LEFT OUTER JOIN. Let artists be the left table.\n\nSELECT * FROM artists LEFT JOIN albums ON albums.artist_id = artists.id;\n\n--Use any join you like to combine the albums and tracks table. Rename the album_id column to Albums.\n\nSELECT * FROM albums LEFT JOIN tracks ON albums.artist_id = tracks.id;", 
            "title": "Project Querying Tables"
        }, 
        {
            "location": "/Codecademy SQL, Analyzing Business Metrics/", 
            "text": "Codecademy SQL, Analyzing Business Metrics\n\n\nForeword\n\n\nCode snippets. With SQLite. From Codecademy in collaboration with \nPeriscope Data\n.\n\n\n\n\nCONTENT\n\n\nCodecademy SQL, Analyzing Business Metrics\n\n\nSpecifying Comments\n\n\nAvanced Aggregates\n\n\n\n\n\n\n\n\n\n\n\n\nSpecifying Comments\n\n\n\n\nLine comment. This is indicated by two negative signs (eg. ). The remainder of the text on the line is the comment.\n\n\nBlock comment. The start of the block comment is indicated by /*, the end of the comment by the same string. A block comment can cover text in part of a line, or can span multiple lines.\n\n\nRem or @. For Oracle, a line starting with either REM or @ is a comment line.\n\n\n\n\nAvanced Aggregates\n\n\nAt the heart of every great business decision is data. Since most businesses store critical data in SQL databases, a deep understanding of SQL is a necessary skill for every data analyst.\n\n\nChief among data analysis tasks is data aggregation, the grouping of data to express in summary form. We\nll be working with SpeedySpoon, a meal delivery app. The folks at SpeedySpoon have asked us to look into their deliveries and help them optimize their process.\n\n\nThis course was developed in partnership with our good friends at Periscope Data. If you\nre new to SQL, we recommend you do this course first.\n\n\nComplete each query by replacing the comments /**/ with SQL code.\n\n\nWe\nll start by looking at SpeedySpoon\ns data. The orders table has a row for each order of a SpeedySpoon delivery. It says when the order took place, and who ordered it.\n\n\nAdd code to the select statement to select all columns in the orders table.\n\n\nselect *\nfrom /**/\norder by id\nlimit 100;\n\n\n\n\nNote that the order and limit clauses keep the data organized.\n\n\nSELECT * FROM orders ORDER BY id LIMIT 100;\n\n\n\n\nThe order_items table lists the individual foods and their prices in each order. Complete the query to select all columns in the order_items table.\n\n\nSELECT * FROM order_items ORDER BY id LIMIT 100;\n\n\n\n\nNow that we have a good handle on our data, let\ns dive into some common business queries. We\nll begin with the Daily Count of orders placed. To make our Daily Count metric, we\nll focus on the date function and the ordered_at field.\n\n\nTo get the Daily Metrics we need the date. Most dates in databases are timestamps, which have hours and minutes, as well as the year, month, and day. Timestamps look like 2015-01-05 14:43:31, while dates are the first part: 2015-01-05.\n\n\nWe can easily select the date an item was ordered at with the date function and the ordered_at field:\n\n\nSELECT date(ordered_at)\nFROM orders;\n\n\n\n\nLet\ns get a Daily Count of orders from the orders table. Complete the query using the date function to cast the timestamps in ordered_at to dates.\n\n\n\u200bSELECT date(ordered_at)\nFROM orders\nORDER BY 1\nLIMIT 100;\n\n\n\n\nThe order by 1 statement is a shortcut for order by date(ordered_at). The 1 refers to the first column.\n\n\nNow that we can convert timestamps to dates, we can count the Orders Per Date. Generally, when we count all records in a table we run a query with the count function, as follows:\n\n\nSELECT count(1)\nFROM users;\n\n\n\n\nThis will treat all rows as a single group, and return one row in the result set - the total count.\n\n\nTo count orders by their dates, we\nll use the date and count functions and pair them with the group by clause. Together this will count the records in the table, grouped by date.\n\n\nFor example, to see the user records counted by the date created, we use the date and count functions and group by clause as follows:\n\n\nSELECT date(created_at), count(1)\nFROM users\nGROUP BY date(created_at)\n\n\n\n\nUse the date and count functions and group by clause to count and group the orders by the dates they were ordered_at.\n\n\nSELECT date(ordered_at), count(1)\nFROM orders\nGROUP BY 1\nORDER BY 1;\n\n\n\n\nGroup before ordering\n\n\nWe have the Daily Count of orders, but what we really want to know is revenue. How much money has SpeedySpoon made from orders each day?\n\n\nWe can make a few changes to our Daily Count query to get the revenue.\n\n\nFirst, instead of using count(1) to count the rows per date, we\nll use round(sum(amount_paid), 2) to add up the revenue per date. Complete the query by adding revenue per date.\n\n\nSecond, we need to join in the order_items table because that table has an amount_paid column representing revenue. Complete the query by adding a join clause where orders.id = order_items.order_id.\n\n\nNote that the round function rounds decimals to digits, based on the number passed in. Here round(\n, 2) rounds the sum paid to two digits.\n\n\nSELECT date(ordered_at), round(sum(amount_paid), 2)\nFROM orders JOIN order_items ON \n    orders.id = order_items.order_id\nGROUP BY 1\nORDER BY 1;\n\n\n\n\nNow with a small change, we can find out how much we\nre making per day for any single dish. What\ns the daily revenue from customers ordering kale smoothies? Complete the query by using a where clause to filter the daily sums down to orders where the name = \nkale-smoothie\n.\n\n\nSELECT date(ordered_at), round(sum(amount_paid), 2)\nFROM orders JOIN order_items ON \n    orders.id = order_items.order_id\nWHERE name = 'kale-smoothie'\nGROUP BY 1\nORDER BY 1;\n\n\n\n\nIt looks like the smoothies might not be performing well, but to be sure we need to see how they\nre doing in the context of the other order items. We\nll look at the data several different ways, the first of which is determining what percent of revenue these smoothies represent.\n\n\nTo get the percent of revenue that each item represents, we need to know the total revenue of each item. We will later divide the per-item total with the overall total revenue.\n\n\nThe following query groups and sum the products by price to get the total revenue for each item. Complete the query by passing in sum(amount_paid) into the round function and rounding to two decimal places.\n\n\nSELECT name, round(sum(amount_paid), 2)\nFROM order_items\nGROUP BY name\nORDER BY 2 DESC;\n\n\n\n\nWe have the sum of the the products by revenue, but we still need the percent. For that, we\nll need to get the total using a subquery. A subquery can perform complicated calculations and create filtered or aggregate tables on the fly.\n\n\nSubqueries are useful when you want to perform an aggregation outside the context of the current query. This will let us calculate the overall total and per-item total at the same time.\n\n\nComplete the denominator in the subquery, which is the total revenue from order_items. Use the sum function to query the amount_paid from the order_items table.\n\n\nWe now have the percent or revenue each product represents!\n\n\nSELECT name, round(sum(amount_paid) / \n    (SELECT sum(amount_paid) FROM order_items) * 100.0, 2)\nFROM order_items\nGROUP BY 1\nORDER BY 2 DESC;\n\n\n\n\nHere order by 2 desc sorts by the second column (the percent) to show the products in order of their contribution to revenue.\n\n\nAs we suspected, kale smoothies are not bringing in the money. And thanks to this analysis, we found what might be a trend - several of the other low performing products are also smoothies. Let\ns keep digging to find out what\ns going on with these smoothies.\n\n\nTo see if our smoothie suspicion has merit, let\ns look at purchases by category. We can group the order items by what type of food they are, and go from there. Since our order_items table does not include categories already, we\nll need to make some!\n\n\nPreviously we\nve been using group by with a column (like order_items.name) or a function (like date(orders.ordered_at)).\n\n\nWe can also use group by with expressions. In this case a case statement is just what we need to build our own categories. case statements are similar to if/else in other languages. Here\ns the basic structure of a case statement:\n\n\nCASE {condition}\n    WHEN {value1} THEN {result1}\n    WHEN {value2} THEN {result2}\n    ELSE {result3}\nEND\n\n\n\n\nWe\nll build our own categories using a case statement. Complete the query below with a case condition of name that lists out each product, and decides its group.\n\n\nSELECT *,\n    CASE name\n        WHEN 'kale-smoothie'    THEN 'smoothie'\n        WHEN 'banana-smoothie'  THEN 'smoothie'\n        WHEN 'orange-juice'     THEN 'drink'\n        WHEN 'soda'             THEN 'drink'\n        WHEN 'blt'              THEN 'sandwich'\n        WHEN 'grilled-cheese'   THEN 'sandwich'\n        WHEN 'tikka-masala'     THEN 'dinner'\n        WHEN 'chicken-parm'     THEN 'dinner'\n        ELSE 'other'\n    END AS category\nFROM order_items\nORDER BY id\nLIMIT 100;\n\n\n\n\nComplete the query by using the category column created by the case statement in our previous revenue percent calculation. Add the denominator that will sum the amount_paid.\n\n\nSELECT\n    CASE name\n        WHEN 'kale-smoothie'    THEN 'smoothie'\n        WHEN 'banana-smoothie'  THEN 'smoothie'\n        WHEN 'orange-juice'     THEN 'drink'\n        WHEN 'soda'             THEN 'drink'\n        WHEN 'blt'              THEN 'sandwich'\n        WHEN 'grilled-cheese'   THEN 'sandwich'\n        WHEN 'tikka-masala'     THEN 'dinner'\n        WHEN 'chicken-parm'     THEN 'dinner'\n        ELSE 'other'\n    END AS category, round(1.0 * sum(amount_paid) / (SELECT sum(amount_paid) FROM order_items) * 100, 2) AS pct\nFROM order_items\nGROUP BY 1\nORDER BY 2 DESC;\n\n\n\n\nHere 1.0 * is a shortcut to ensure the database represents the percent as a decimal.\n\n\nIt\ns true that the whole smoothie category is performing poorly compared to the others. We\nll certainly take this discovery to SpeedySpoon. Before we do, let\ns go one level deeper and figure out why.\n\n\nWhile we do know that kale smoothies (and drinks overall) are not driving a lot of revenue, we don\nt know why. A big part of data analysis is implementing your own metrics to get information out of the piles of data in your database.\n\n\nIn our case, the reason could be that no one likes kale, but it could be something else entirely. To find out, we\nll create a metric called reorder rate and see how that compares to the other products at SpeedySpoon.\n\n\nWe\nll define reorder rate as the ratio of the total number of orders to the number of people making those orders. A lower ratio means most of the orders are reorders. A higher ratio means more of the orders are first purchases.\n\n\nLet\ns calculate the reorder ratio for all of SpeedySpoon\ns products and take a look at the results. Counting the total orders per product is straightforward. We count the distinct order_ids (different) in the order_items table.\n\n\nComplete the query by passing in the distinct keyword and the order_id column name into the count function\n\n\nHere\ns a hint on how to use the count function to count distinct columns in a table.\n\n\nSELECT DISTINCT column_name FROM table_name;\nSELECT column_name FROM table_name GROUP BY column_name;\n\nSELECT name, count(DISTINCT order_id)\nFROM order_items\nGROUP BY 1\nORDER BY 1;\n\n\n\n\nNow we need the number of people making these orders.\n\n\nTo get that information, we need to join in the orders table and count unique values in the delivered_to field, and sort by the reorder_rate.\n\n\nComplete the query below. The numerator should count the distinct order_ids. The denominator should count the distinct values of the orders table\ns delivered_to field (orders.delivered_to).\n\n\nSELECT name, round(1.0 * \ncount(DISTINCT order_id) /\n    count(DISTINCT delivered_to), 2) AS reorder_rate\nFROM order_items\n    JOIN orders ON\n        orders.id = order_items.order_id\nGROUP BY 1\nORDER BY 2 DESC;\n\n\n\n\nThat\ns unexpected. While smoothies aren\nt making a lot of money for SpeedySpoon, they have a very high reorder rate. That means these smoothie customers are strong repeat customers.\n\n\nInstead of recommending smoothies be taken off the menu, we should talk to the smoothie customers and see what they like so much about these smoothies. There could be an opportunity here to expand the product line, or get other customers as excited as these kale fanatics. Nice work!\n\n\nLet\ns generalize what we\nve learned so far:\n\n\n\n\n\n\nData aggregation is the grouping of data in summary form.\n\n\n\n\n\n\nDaily Count is the count of orders in a day.\n\n\n\n\n\n\nDaily Revenue Count is the revenue on orders per day.\n\n\n\n\n\n\nProduct Sum is the total revenue of a product.\n\n\n\n\n\n\nSubqueries can be used to perform complicated calculations and create filtered or aggregate tables on the fly.\n\n\n\n\n\n\nReorder Rate is the ratio of the total number of orders to the number of people making orders.\n\n\n\n\n\n\n2.Common Metrics\n\n\nAs a data scientist, when you\nre not investigating spikes or dips in your data, you might be building dashboards of KPIs, or key performance indicators for a company.\n\n\nKPIs are often displayed on TVs on the walls of the office, and serve as high level health metrics for the business. While every company\ns metrics are defined slightly differently, the basics are usually very similar.\n\n\nIn this lesson we\nll take a look at basic KPIs like Daily Revenue, Daily Active Users, ARPU, and Retention for a video game, Mineblocks.\n\n\nThis company has two tables, gameplays and purchases. The purchases table lists all purchases made by players while they\nre playing Mineblocks. Complete the query to select from purchases.\n\n\nSELECT * FROM purchases ORDER BY id LIMIT 10;\n\n\n\n\nThe gameplays table lists the date and platform for each session a user plays. Select from gameplays.\n\n\nSELECT * FROM gameplays ORDER BY id LIMIT 10;\n\n\n\n\nAt the heart of every company is revenue, and Mineblocks is no exception. For our first KPI we\nll calculate daily revenue.\n\n\nDaily Revenue is simply the sum of money made per day.\n\n\nTo get close to Daily Revenue, we calculate the daily sum of the prices in the purchases table. Complete the query by using the sum function and passing the price column from the purchases table. \n\n\nSELECT \n    date(created_at),\n    round(sum(price),2)\nFROM purchases\nGROUP BY 1\nORDER BY 1;\n\n\n\n\nUpdate our daily revenue query to exclude refunds. Complete the query by filtering for refunded_at is not null. Update our daily revenue query to exclude refunds. Complete the query by filtering for refunded_at is not null.\n\n\nMineblocks is a game, and one of the core metrics to any game is the number people who play each day. That KPI is called Daily Active Users, or DAU.\n\n\nDAU is defined as the number of unique players seen in-game each day. It\ns important not to double count users who played multiple times, so we\nll use distinct in our count function.\n\n\nLikewise, Weekly Active Users (WAU) and Monthly Active Users (MAU) are in the same family.\n\n\nFor Mineblocks, we\nll use the gameplays table to calculate DAU. Each time a user plays the game, their session is recorded in gameplays. Thus, a distinct count of users per day from gameplays will give us DAU.\n\n\nCalculate Daily Active Users for Mineblocks. Complete the query\ns count function by passing in the distinct keyword and the user_id column name.\n\n\nSELECT\n    date(created_at), \n    COUNT(DISTINCT user_id) AS dau\nFROM gameplays\nGROUP BY 1\nORDER BY 1;\n\n\n\n\nSince Mineblocks is on multiple platforms, we can calculate DAU per-platform.\n\n\nPreviously we calculated DAU only per day, so the output we wanted was [date, dau_count]. Now we want DAU per platform, making the desired output [date, platform, dau_count].\n\n\nCalculate DAU for Mineblocks per-platform. Complete the query below. You will need to select the platform column and add a count function by passing in the distinct keyword and the user_id column name.\n\n\nSELECT\n    date(created_at), \n    platform,\n    COUNT(DISTINCT user_id) AS dau\nFROM gameplays\nGROUP BY 1, 2\nORDER BY 1, 2;\n\n\n\n\ngroup by 1 (date), 2 (platform)\n\n order by 1 (date), 2 (platform)\n\n\nWe\nve looked at DAU and Daily Revenue in Mineblocks. Now we must understand the purchasing habits of our users.\n\n\nMineblocks, like every freemium game, has two types of users:\n\n\npurchasers: users who have bought things in the game\nplayers: users who play the game but have not yet purchased\n\n\n\nThe next KPI we\nll look at Daily ARPPU - Average Revenue Per Purchasing User. This metric shows if the average amount of money spent by purchasers is going up over time.\n\n\nDaily ARPPU is defined as the sum of revenue divided by the number of purchasers per day.\n\n\nTo get Daily ARPPU, modify the daily revenue query from earlier to divide by the number of purchasers.\n\n\nComplete the query by adding a numerator and a denominator. The numerator will display daily revenue, or sum the price columns. The denominator will display the number of purchasers by passing the distinct keyword and the user_id column name into the count function.\n\n\nSELECT\n    date(created_at), \n    round(sum(price) / COUNT(DISTINCT user_id), 2) AS arppu\nFROM purchases\nWHERE refunded_at IS NULL\nGROUP BY 1\nORDER BY 1;\n\n\n\n\nThe more popular (and difficult) cousin to Daily ARPPU is Daily ARPU, Average Revenue Per User. ARPU measures the average amount of money we\nre getting across all players, whether or not they\nve purchased.\n\n\nARPPU increases if purchasers are spending more money. ARPU increases if more players are choosing to purchase, even if the purchase size stays consistent.\n\n\nNo one metric can tell the whole story. That\ns why it\ns so helpful to have many KPIs on the same dashboard.\n\n\nDaily ARPU is defined as revenue divided by the number of players, per-day. To get that, we\nll need to calculate the daily revenue and daily active users separately, and then join them on their dates.\n\n\nOne way to easily create and organize temporary results in a query is with CTEs, Common Table Expressions, also known as with clauses. The with clauses make it easy to define and use results in a more organized way than subqueries. These clauses usually look like this:\n\n\nWITH {subquery_name} AS (\n    {subquery_body}\n)\nSELECT ...\nFROM {subquery_name}\nWHERE ...\n\n\n\n\nUse a with clause to define daily_revenue and then select from it.\n\n\ndaily_revenue AS (\n    SELECT\n        date(created_at) AS dt,\n        round(sum(price), 2) AS rev\n    FROM purchases\n    WHERE refunded_at IS NULL\n    GROUP BY 1\n)\nSELECT * FROM daily_revenue ORDER BY dt;\n\n\n\n\nUse a with clause to define daily_revenue and then select from it.\n\n\nWITH daily_revenue AS (\n    SELECT\n        date(created_at) AS dt,\n        round(sum(price), 2) AS rev\n    FROM purchases\n    WHERE refunded_at IS NULL\n    GROUP BY 1\n)\nSELECT * FROM daily_revenue ORDER BY dt;\n\n\n\n\nNow you\nre familiar with using the with clause to create temporary result sets.\n\n\nYou just built the first part of ARPU, daily_revenue. From here we can build the second half of ARPU in our with clause, daily_players, and use both together to create ARPU.\n\n\nBuilding on this CTE, we can add in DAU from earlier. Complete the query by calling the DAU query we created earlier, now aliased as daily_players:\n\n\nWITH daily_revenue AS (\n    SELECT\n        date(created_at) AS dt,\n        round(sum(price), 2) AS rev\n    FROM purchases\n    WHERE refunded_at IS NULL\n    GROUP BY 1\n),\ndaily_players AS (\n    SELECT\n        date(created_at) AS dt,\n        COUNT(DISTINCT user_id) AS players\n    FROM gameplays\n    GROUP BY 1\n)\nSELECT * FROM daily_players\nORDER BY dt;\n\n\n\n\nNow that we have the revenue and DAU, join them on their dates and calculate daily ARPU. Complete the query by adding the keyword using in the join clause.\n\n\nWITH daily_revenue AS (\n    SELECT\n        date(created_at) AS dt,\n        round(sum(price), 2) AS rev\n    FROM purchases\n    WHERE refunded_at IS NULL\n    GROUP BY 1\n),\ndaily_players AS (\n    SELECT\n        date(created_at) AS dt,\n        COUNT(DISTINCT user_id) AS players\n    FROM gameplays\n    GROUP BY 1\n)\nSELECT \n    daily_revenue.dt,\n    daily_revenue.rev /\n    daily_players.players\nFROM daily_players\nJOIN daily_players USING (dt);\n\n\n\n\nIn the final select statement, daily_revenue.dt represents the date, while daily_revenue.rev / daily_players.players is the daily revenue divided by the number of players that day. In full, it represents how much the company is making per player, per day.\n\n\nIn our ARPU query, we used using instead of on in the join clause. This is a special case join.\n\n\nFROM daily_revenue JOIN daily_players USING (dt);\n\n\n\n\nWhen the columns to join have the same name in both tables you can use USING instead of on. Our use of the using keyword is in this case equivalent to this clause:\n\n\nFROM daily_revenue JOIN daily_players ON daily_revenue.dt = daily_players.dt;\n\nJOIN daily_players USING (dt);\nJOIN daily_players ON daily_revenue.dt = daily_players.dt;\n\n\n\n\nNow let\ns find out what percent of Mineblock players are returning to play the next day. This KPI is called 1 Day Retention.\n\n\nRetention can be defined many different ways, but we\nll stick to the most basic definition. For all players on Day N, we\nll consider them retained if they came back to play again on Day N+1.\n\n\nThis will let us track whether or not Mineblocks is getting \nstickier\n over time. The stickier our game, the more days players will spend in-game.\n\n\nAnd more time in-game means more opportunities to monetize and grow our business.\n\n\nBefore we can calculate retention we need to get our data formatted in a way where we can determine if a user returned.\n\n\nCurrently the gameplays table is a list of when the user played, and it\ns not easy to see if any user came back.\n\n\nBy using a self-join, we can make multiple gameplays available on the same row of results. This will enable us to calculate retention.\n\n\nThe power of self-join comes from joining every row to every other row. This makes it possible to compare values from two different rows in the new result set. In our case, we\nll compare rows that are one date apart from each user.\n\n\nTo calculate retention, start from a query that selects the date(created_at) as dt and user_id columns from the gameplays table.\n\n\nSELECT\n    date(created_at) AS dt,\n    user_id\nFROM gameplays AS g1\nORDER BY dt\nLIMIT 100;\n\n\n\n\nNow we\nll join gameplays on itself so that we can have access to all gameplays for each player, for each of their gameplays.\n\n\nThis is known as a self-join and will let us connect the players on Day N to the players on Day N+1. In order to join a table to itself, it must be aliased so we can access each copy separately.\n\n\nWe aliased gameplays in the query above because in the next step, we need to join gameplays to itself so we can get a result selecting [date, user_id, user_id_if_retained].\n\n\nComplete the query by using a join statement to join gameplays to itself on user_id using the aliases g1 and g2.\n\n\nSELECT\n    date(g1.created_at) as dt,\n    g1.user_id\nFROM gameplays AS g1\nJOIN gameplays AS g2 ON\n    g1.user_id = g2.user_id\nORDER BY 1\nLIMIT 100;\n\n\n\n\nWe don\nt use the using clause here because the join is about to get more complicated.\n\n\nNow that we have our gameplays table joined to itself, we can start to calculate retention.\n\n\n1 Day Retention is defined as the number of players who returned the next day divided by the number of original players, per day. Suppose 10 players played Mineblocks on Dec 10th. If 4 of them play on Dec 11th, the 1 day retention for Dec 10th is 40%.\n\n\nThe previous query joined all rows in gameplays against all other rows for each user, making a massive result set that we don\nt need.\n\n\nWe\nll need to modify this query.\n\n\nSELECT\n  date(g1.created_at) AS dt,\n  g1.user_id,\n  g2.user_id\nFROM gameplays AS g1\n  JOIN gameplays AS g2 ON\n    g1.user_id = g2.user_id\n    AND /**/\nORDER BY 1\nLIMIT 100;\n\n\n\n\nComplete the query above such that the join clause includes a date join:\n\n\ndate(g1.created_at) = date(datetime(g2.created_at, '-1 day'))\n\n\n\n\nThis means \nonly join rows where the date in g1 is one less than the date in g2\n, which makes it possible to see if users have returned!\n\n\nSELECT\n    date(g1.created_at) as dt,\n    g1.user_id,\n    g2.user_id\nFROM gameplays AS g1\nJOIN gameplays AS g2 ON\n    g1.user_id = g2.user_id\n    AND date(g1.created_at) = date(datetime(g2.created_at, '-1 day'))\nORDER BY 1\nLIMIT 100;\n\n\n\n\nThe query above won\nt return meaningful results because we\nre using an inner join. This type of join requires that the condition be met for all rows, effectively limiting our selection to only the users that have returned.\n\n\nInstead, we want to use a left join, this way all rows in g1 are preserved, leaving nulls in the rows from g2 where users did not return to play the next day.\n\n\nChange the join clause to use left join and count the distinct number of users from g1 and g2 per date.\n\n\nSELECT\n    date(g1.created_at) as dt,\n    count(DISTINCT g1.user_id) as total_users,\n    count(DISTINCT g2.user_id) as retained_users\nFROM gameplays AS g1\nLEFT JOIN gameplays AS g2 ON\n    g1.user_id = g2.user_id\n    AND date(g1.created_at) = date(datetime(g2.created_at, '-1 day'))\nGROUP BY 1\nORDER BY 1\nLIMIT 100;\n\n\n\n\nNow that we have retained users as count(distinct g2.user_id) and total users as count(distinct g1.user_id), divide retained users by total users to calculate 1 day retention!\n\n\nSELECT\n    date(g1.created_at) as dt,\n    round(100 * count(DISTINCT g2.user_id) / count(DISTINCT g1.user_id)) AS retention\nFROM gameplays AS g1\nLEFT JOIN gameplays AS g2 ON\n    g1.user_id = g2.user_id\n    AND date(g1.created_at) = date(datetime(g2.created_at, '-1 day'))\nGROUP BY 1\nORDER BY 1\nLIMIT 100;\n\n\n\n\nWhile every business has different metrics to track their success, most are based on revenue and usage.\n\n\nThe metrics in this lesson are merely a starting point, and from here you\nll be able to create and customize metrics to track whatever is most important to your company.\n\n\nAnd remember, data science is exploratory! The current set of metrics can always be improved and there\ns usually more to any spike or dip than what immediately meets the eye.\n\n\nLet\ns generalize what we\nve learned so far:\n\n\n\n\nKey Performance Indicators are high level health metrics for a business.\n\n\nDaily Revenue is the sum of money made per day.\n\n\nDaily Active Users are the number of unique users seen each day.\n\n\nDaily Average Revenue Per Purchasing User (ARPPU) is the average amount of money spent by purchasers each day.\n\n\nDaily Average Revenue Per User (ARPU) is the average amount of money across all users.\n\n\n1 Day Retention is defined as the number of players from Day N who came back to play again on Day N+1.", 
            "title": "Codecademy SQL, Analyzing Business Metrics"
        }, 
        {
            "location": "/Codecademy SQL, Analyzing Business Metrics/#specifying-comments", 
            "text": "Line comment. This is indicated by two negative signs (eg. ). The remainder of the text on the line is the comment.  Block comment. The start of the block comment is indicated by /*, the end of the comment by the same string. A block comment can cover text in part of a line, or can span multiple lines.  Rem or @. For Oracle, a line starting with either REM or @ is a comment line.", 
            "title": "Specifying Comments"
        }, 
        {
            "location": "/Codecademy SQL, Analyzing Business Metrics/#avanced-aggregates", 
            "text": "At the heart of every great business decision is data. Since most businesses store critical data in SQL databases, a deep understanding of SQL is a necessary skill for every data analyst.  Chief among data analysis tasks is data aggregation, the grouping of data to express in summary form. We ll be working with SpeedySpoon, a meal delivery app. The folks at SpeedySpoon have asked us to look into their deliveries and help them optimize their process.  This course was developed in partnership with our good friends at Periscope Data. If you re new to SQL, we recommend you do this course first.  Complete each query by replacing the comments /**/ with SQL code.  We ll start by looking at SpeedySpoon s data. The orders table has a row for each order of a SpeedySpoon delivery. It says when the order took place, and who ordered it.  Add code to the select statement to select all columns in the orders table.  select *\nfrom /**/\norder by id\nlimit 100;  Note that the order and limit clauses keep the data organized.  SELECT * FROM orders ORDER BY id LIMIT 100;  The order_items table lists the individual foods and their prices in each order. Complete the query to select all columns in the order_items table.  SELECT * FROM order_items ORDER BY id LIMIT 100;  Now that we have a good handle on our data, let s dive into some common business queries. We ll begin with the Daily Count of orders placed. To make our Daily Count metric, we ll focus on the date function and the ordered_at field.  To get the Daily Metrics we need the date. Most dates in databases are timestamps, which have hours and minutes, as well as the year, month, and day. Timestamps look like 2015-01-05 14:43:31, while dates are the first part: 2015-01-05.  We can easily select the date an item was ordered at with the date function and the ordered_at field:  SELECT date(ordered_at)\nFROM orders;  Let s get a Daily Count of orders from the orders table. Complete the query using the date function to cast the timestamps in ordered_at to dates.  \u200bSELECT date(ordered_at)\nFROM orders\nORDER BY 1\nLIMIT 100;  The order by 1 statement is a shortcut for order by date(ordered_at). The 1 refers to the first column.  Now that we can convert timestamps to dates, we can count the Orders Per Date. Generally, when we count all records in a table we run a query with the count function, as follows:  SELECT count(1)\nFROM users;  This will treat all rows as a single group, and return one row in the result set - the total count.  To count orders by their dates, we ll use the date and count functions and pair them with the group by clause. Together this will count the records in the table, grouped by date.  For example, to see the user records counted by the date created, we use the date and count functions and group by clause as follows:  SELECT date(created_at), count(1)\nFROM users\nGROUP BY date(created_at)  Use the date and count functions and group by clause to count and group the orders by the dates they were ordered_at.  SELECT date(ordered_at), count(1)\nFROM orders\nGROUP BY 1\nORDER BY 1;  Group before ordering  We have the Daily Count of orders, but what we really want to know is revenue. How much money has SpeedySpoon made from orders each day?  We can make a few changes to our Daily Count query to get the revenue.  First, instead of using count(1) to count the rows per date, we ll use round(sum(amount_paid), 2) to add up the revenue per date. Complete the query by adding revenue per date.  Second, we need to join in the order_items table because that table has an amount_paid column representing revenue. Complete the query by adding a join clause where orders.id = order_items.order_id.  Note that the round function rounds decimals to digits, based on the number passed in. Here round( , 2) rounds the sum paid to two digits.  SELECT date(ordered_at), round(sum(amount_paid), 2)\nFROM orders JOIN order_items ON \n    orders.id = order_items.order_id\nGROUP BY 1\nORDER BY 1;  Now with a small change, we can find out how much we re making per day for any single dish. What s the daily revenue from customers ordering kale smoothies? Complete the query by using a where clause to filter the daily sums down to orders where the name =  kale-smoothie .  SELECT date(ordered_at), round(sum(amount_paid), 2)\nFROM orders JOIN order_items ON \n    orders.id = order_items.order_id\nWHERE name = 'kale-smoothie'\nGROUP BY 1\nORDER BY 1;  It looks like the smoothies might not be performing well, but to be sure we need to see how they re doing in the context of the other order items. We ll look at the data several different ways, the first of which is determining what percent of revenue these smoothies represent.  To get the percent of revenue that each item represents, we need to know the total revenue of each item. We will later divide the per-item total with the overall total revenue.  The following query groups and sum the products by price to get the total revenue for each item. Complete the query by passing in sum(amount_paid) into the round function and rounding to two decimal places.  SELECT name, round(sum(amount_paid), 2)\nFROM order_items\nGROUP BY name\nORDER BY 2 DESC;  We have the sum of the the products by revenue, but we still need the percent. For that, we ll need to get the total using a subquery. A subquery can perform complicated calculations and create filtered or aggregate tables on the fly.  Subqueries are useful when you want to perform an aggregation outside the context of the current query. This will let us calculate the overall total and per-item total at the same time.  Complete the denominator in the subquery, which is the total revenue from order_items. Use the sum function to query the amount_paid from the order_items table.  We now have the percent or revenue each product represents!  SELECT name, round(sum(amount_paid) / \n    (SELECT sum(amount_paid) FROM order_items) * 100.0, 2)\nFROM order_items\nGROUP BY 1\nORDER BY 2 DESC;  Here order by 2 desc sorts by the second column (the percent) to show the products in order of their contribution to revenue.  As we suspected, kale smoothies are not bringing in the money. And thanks to this analysis, we found what might be a trend - several of the other low performing products are also smoothies. Let s keep digging to find out what s going on with these smoothies.  To see if our smoothie suspicion has merit, let s look at purchases by category. We can group the order items by what type of food they are, and go from there. Since our order_items table does not include categories already, we ll need to make some!  Previously we ve been using group by with a column (like order_items.name) or a function (like date(orders.ordered_at)).  We can also use group by with expressions. In this case a case statement is just what we need to build our own categories. case statements are similar to if/else in other languages. Here s the basic structure of a case statement:  CASE {condition}\n    WHEN {value1} THEN {result1}\n    WHEN {value2} THEN {result2}\n    ELSE {result3}\nEND  We ll build our own categories using a case statement. Complete the query below with a case condition of name that lists out each product, and decides its group.  SELECT *,\n    CASE name\n        WHEN 'kale-smoothie'    THEN 'smoothie'\n        WHEN 'banana-smoothie'  THEN 'smoothie'\n        WHEN 'orange-juice'     THEN 'drink'\n        WHEN 'soda'             THEN 'drink'\n        WHEN 'blt'              THEN 'sandwich'\n        WHEN 'grilled-cheese'   THEN 'sandwich'\n        WHEN 'tikka-masala'     THEN 'dinner'\n        WHEN 'chicken-parm'     THEN 'dinner'\n        ELSE 'other'\n    END AS category\nFROM order_items\nORDER BY id\nLIMIT 100;  Complete the query by using the category column created by the case statement in our previous revenue percent calculation. Add the denominator that will sum the amount_paid.  SELECT\n    CASE name\n        WHEN 'kale-smoothie'    THEN 'smoothie'\n        WHEN 'banana-smoothie'  THEN 'smoothie'\n        WHEN 'orange-juice'     THEN 'drink'\n        WHEN 'soda'             THEN 'drink'\n        WHEN 'blt'              THEN 'sandwich'\n        WHEN 'grilled-cheese'   THEN 'sandwich'\n        WHEN 'tikka-masala'     THEN 'dinner'\n        WHEN 'chicken-parm'     THEN 'dinner'\n        ELSE 'other'\n    END AS category, round(1.0 * sum(amount_paid) / (SELECT sum(amount_paid) FROM order_items) * 100, 2) AS pct\nFROM order_items\nGROUP BY 1\nORDER BY 2 DESC;  Here 1.0 * is a shortcut to ensure the database represents the percent as a decimal.  It s true that the whole smoothie category is performing poorly compared to the others. We ll certainly take this discovery to SpeedySpoon. Before we do, let s go one level deeper and figure out why.  While we do know that kale smoothies (and drinks overall) are not driving a lot of revenue, we don t know why. A big part of data analysis is implementing your own metrics to get information out of the piles of data in your database.  In our case, the reason could be that no one likes kale, but it could be something else entirely. To find out, we ll create a metric called reorder rate and see how that compares to the other products at SpeedySpoon.  We ll define reorder rate as the ratio of the total number of orders to the number of people making those orders. A lower ratio means most of the orders are reorders. A higher ratio means more of the orders are first purchases.  Let s calculate the reorder ratio for all of SpeedySpoon s products and take a look at the results. Counting the total orders per product is straightforward. We count the distinct order_ids (different) in the order_items table.  Complete the query by passing in the distinct keyword and the order_id column name into the count function  Here s a hint on how to use the count function to count distinct columns in a table.  SELECT DISTINCT column_name FROM table_name;\nSELECT column_name FROM table_name GROUP BY column_name;\n\nSELECT name, count(DISTINCT order_id)\nFROM order_items\nGROUP BY 1\nORDER BY 1;  Now we need the number of people making these orders.  To get that information, we need to join in the orders table and count unique values in the delivered_to field, and sort by the reorder_rate.  Complete the query below. The numerator should count the distinct order_ids. The denominator should count the distinct values of the orders table s delivered_to field (orders.delivered_to).  SELECT name, round(1.0 * \ncount(DISTINCT order_id) /\n    count(DISTINCT delivered_to), 2) AS reorder_rate\nFROM order_items\n    JOIN orders ON\n        orders.id = order_items.order_id\nGROUP BY 1\nORDER BY 2 DESC;  That s unexpected. While smoothies aren t making a lot of money for SpeedySpoon, they have a very high reorder rate. That means these smoothie customers are strong repeat customers.  Instead of recommending smoothies be taken off the menu, we should talk to the smoothie customers and see what they like so much about these smoothies. There could be an opportunity here to expand the product line, or get other customers as excited as these kale fanatics. Nice work!  Let s generalize what we ve learned so far:    Data aggregation is the grouping of data in summary form.    Daily Count is the count of orders in a day.    Daily Revenue Count is the revenue on orders per day.    Product Sum is the total revenue of a product.    Subqueries can be used to perform complicated calculations and create filtered or aggregate tables on the fly.    Reorder Rate is the ratio of the total number of orders to the number of people making orders.    2.Common Metrics  As a data scientist, when you re not investigating spikes or dips in your data, you might be building dashboards of KPIs, or key performance indicators for a company.  KPIs are often displayed on TVs on the walls of the office, and serve as high level health metrics for the business. While every company s metrics are defined slightly differently, the basics are usually very similar.  In this lesson we ll take a look at basic KPIs like Daily Revenue, Daily Active Users, ARPU, and Retention for a video game, Mineblocks.  This company has two tables, gameplays and purchases. The purchases table lists all purchases made by players while they re playing Mineblocks. Complete the query to select from purchases.  SELECT * FROM purchases ORDER BY id LIMIT 10;  The gameplays table lists the date and platform for each session a user plays. Select from gameplays.  SELECT * FROM gameplays ORDER BY id LIMIT 10;  At the heart of every company is revenue, and Mineblocks is no exception. For our first KPI we ll calculate daily revenue.  Daily Revenue is simply the sum of money made per day.  To get close to Daily Revenue, we calculate the daily sum of the prices in the purchases table. Complete the query by using the sum function and passing the price column from the purchases table.   SELECT \n    date(created_at),\n    round(sum(price),2)\nFROM purchases\nGROUP BY 1\nORDER BY 1;  Update our daily revenue query to exclude refunds. Complete the query by filtering for refunded_at is not null. Update our daily revenue query to exclude refunds. Complete the query by filtering for refunded_at is not null.  Mineblocks is a game, and one of the core metrics to any game is the number people who play each day. That KPI is called Daily Active Users, or DAU.  DAU is defined as the number of unique players seen in-game each day. It s important not to double count users who played multiple times, so we ll use distinct in our count function.  Likewise, Weekly Active Users (WAU) and Monthly Active Users (MAU) are in the same family.  For Mineblocks, we ll use the gameplays table to calculate DAU. Each time a user plays the game, their session is recorded in gameplays. Thus, a distinct count of users per day from gameplays will give us DAU.  Calculate Daily Active Users for Mineblocks. Complete the query s count function by passing in the distinct keyword and the user_id column name.  SELECT\n    date(created_at), \n    COUNT(DISTINCT user_id) AS dau\nFROM gameplays\nGROUP BY 1\nORDER BY 1;  Since Mineblocks is on multiple platforms, we can calculate DAU per-platform.  Previously we calculated DAU only per day, so the output we wanted was [date, dau_count]. Now we want DAU per platform, making the desired output [date, platform, dau_count].  Calculate DAU for Mineblocks per-platform. Complete the query below. You will need to select the platform column and add a count function by passing in the distinct keyword and the user_id column name.  SELECT\n    date(created_at), \n    platform,\n    COUNT(DISTINCT user_id) AS dau\nFROM gameplays\nGROUP BY 1, 2\nORDER BY 1, 2;  group by 1 (date), 2 (platform) \n order by 1 (date), 2 (platform)  We ve looked at DAU and Daily Revenue in Mineblocks. Now we must understand the purchasing habits of our users.  Mineblocks, like every freemium game, has two types of users:  purchasers: users who have bought things in the game\nplayers: users who play the game but have not yet purchased  The next KPI we ll look at Daily ARPPU - Average Revenue Per Purchasing User. This metric shows if the average amount of money spent by purchasers is going up over time.  Daily ARPPU is defined as the sum of revenue divided by the number of purchasers per day.  To get Daily ARPPU, modify the daily revenue query from earlier to divide by the number of purchasers.  Complete the query by adding a numerator and a denominator. The numerator will display daily revenue, or sum the price columns. The denominator will display the number of purchasers by passing the distinct keyword and the user_id column name into the count function.  SELECT\n    date(created_at), \n    round(sum(price) / COUNT(DISTINCT user_id), 2) AS arppu\nFROM purchases\nWHERE refunded_at IS NULL\nGROUP BY 1\nORDER BY 1;  The more popular (and difficult) cousin to Daily ARPPU is Daily ARPU, Average Revenue Per User. ARPU measures the average amount of money we re getting across all players, whether or not they ve purchased.  ARPPU increases if purchasers are spending more money. ARPU increases if more players are choosing to purchase, even if the purchase size stays consistent.  No one metric can tell the whole story. That s why it s so helpful to have many KPIs on the same dashboard.  Daily ARPU is defined as revenue divided by the number of players, per-day. To get that, we ll need to calculate the daily revenue and daily active users separately, and then join them on their dates.  One way to easily create and organize temporary results in a query is with CTEs, Common Table Expressions, also known as with clauses. The with clauses make it easy to define and use results in a more organized way than subqueries. These clauses usually look like this:  WITH {subquery_name} AS (\n    {subquery_body}\n)\nSELECT ...\nFROM {subquery_name}\nWHERE ...  Use a with clause to define daily_revenue and then select from it.  daily_revenue AS (\n    SELECT\n        date(created_at) AS dt,\n        round(sum(price), 2) AS rev\n    FROM purchases\n    WHERE refunded_at IS NULL\n    GROUP BY 1\n)\nSELECT * FROM daily_revenue ORDER BY dt;  Use a with clause to define daily_revenue and then select from it.  WITH daily_revenue AS (\n    SELECT\n        date(created_at) AS dt,\n        round(sum(price), 2) AS rev\n    FROM purchases\n    WHERE refunded_at IS NULL\n    GROUP BY 1\n)\nSELECT * FROM daily_revenue ORDER BY dt;  Now you re familiar with using the with clause to create temporary result sets.  You just built the first part of ARPU, daily_revenue. From here we can build the second half of ARPU in our with clause, daily_players, and use both together to create ARPU.  Building on this CTE, we can add in DAU from earlier. Complete the query by calling the DAU query we created earlier, now aliased as daily_players:  WITH daily_revenue AS (\n    SELECT\n        date(created_at) AS dt,\n        round(sum(price), 2) AS rev\n    FROM purchases\n    WHERE refunded_at IS NULL\n    GROUP BY 1\n),\ndaily_players AS (\n    SELECT\n        date(created_at) AS dt,\n        COUNT(DISTINCT user_id) AS players\n    FROM gameplays\n    GROUP BY 1\n)\nSELECT * FROM daily_players\nORDER BY dt;  Now that we have the revenue and DAU, join them on their dates and calculate daily ARPU. Complete the query by adding the keyword using in the join clause.  WITH daily_revenue AS (\n    SELECT\n        date(created_at) AS dt,\n        round(sum(price), 2) AS rev\n    FROM purchases\n    WHERE refunded_at IS NULL\n    GROUP BY 1\n),\ndaily_players AS (\n    SELECT\n        date(created_at) AS dt,\n        COUNT(DISTINCT user_id) AS players\n    FROM gameplays\n    GROUP BY 1\n)\nSELECT \n    daily_revenue.dt,\n    daily_revenue.rev /\n    daily_players.players\nFROM daily_players\nJOIN daily_players USING (dt);  In the final select statement, daily_revenue.dt represents the date, while daily_revenue.rev / daily_players.players is the daily revenue divided by the number of players that day. In full, it represents how much the company is making per player, per day.  In our ARPU query, we used using instead of on in the join clause. This is a special case join.  FROM daily_revenue JOIN daily_players USING (dt);  When the columns to join have the same name in both tables you can use USING instead of on. Our use of the using keyword is in this case equivalent to this clause:  FROM daily_revenue JOIN daily_players ON daily_revenue.dt = daily_players.dt;\n\nJOIN daily_players USING (dt);\nJOIN daily_players ON daily_revenue.dt = daily_players.dt;  Now let s find out what percent of Mineblock players are returning to play the next day. This KPI is called 1 Day Retention.  Retention can be defined many different ways, but we ll stick to the most basic definition. For all players on Day N, we ll consider them retained if they came back to play again on Day N+1.  This will let us track whether or not Mineblocks is getting  stickier  over time. The stickier our game, the more days players will spend in-game.  And more time in-game means more opportunities to monetize and grow our business.  Before we can calculate retention we need to get our data formatted in a way where we can determine if a user returned.  Currently the gameplays table is a list of when the user played, and it s not easy to see if any user came back.  By using a self-join, we can make multiple gameplays available on the same row of results. This will enable us to calculate retention.  The power of self-join comes from joining every row to every other row. This makes it possible to compare values from two different rows in the new result set. In our case, we ll compare rows that are one date apart from each user.  To calculate retention, start from a query that selects the date(created_at) as dt and user_id columns from the gameplays table.  SELECT\n    date(created_at) AS dt,\n    user_id\nFROM gameplays AS g1\nORDER BY dt\nLIMIT 100;  Now we ll join gameplays on itself so that we can have access to all gameplays for each player, for each of their gameplays.  This is known as a self-join and will let us connect the players on Day N to the players on Day N+1. In order to join a table to itself, it must be aliased so we can access each copy separately.  We aliased gameplays in the query above because in the next step, we need to join gameplays to itself so we can get a result selecting [date, user_id, user_id_if_retained].  Complete the query by using a join statement to join gameplays to itself on user_id using the aliases g1 and g2.  SELECT\n    date(g1.created_at) as dt,\n    g1.user_id\nFROM gameplays AS g1\nJOIN gameplays AS g2 ON\n    g1.user_id = g2.user_id\nORDER BY 1\nLIMIT 100;  We don t use the using clause here because the join is about to get more complicated.  Now that we have our gameplays table joined to itself, we can start to calculate retention.  1 Day Retention is defined as the number of players who returned the next day divided by the number of original players, per day. Suppose 10 players played Mineblocks on Dec 10th. If 4 of them play on Dec 11th, the 1 day retention for Dec 10th is 40%.  The previous query joined all rows in gameplays against all other rows for each user, making a massive result set that we don t need.  We ll need to modify this query.  SELECT\n  date(g1.created_at) AS dt,\n  g1.user_id,\n  g2.user_id\nFROM gameplays AS g1\n  JOIN gameplays AS g2 ON\n    g1.user_id = g2.user_id\n    AND /**/\nORDER BY 1\nLIMIT 100;  Complete the query above such that the join clause includes a date join:  date(g1.created_at) = date(datetime(g2.created_at, '-1 day'))  This means  only join rows where the date in g1 is one less than the date in g2 , which makes it possible to see if users have returned!  SELECT\n    date(g1.created_at) as dt,\n    g1.user_id,\n    g2.user_id\nFROM gameplays AS g1\nJOIN gameplays AS g2 ON\n    g1.user_id = g2.user_id\n    AND date(g1.created_at) = date(datetime(g2.created_at, '-1 day'))\nORDER BY 1\nLIMIT 100;  The query above won t return meaningful results because we re using an inner join. This type of join requires that the condition be met for all rows, effectively limiting our selection to only the users that have returned.  Instead, we want to use a left join, this way all rows in g1 are preserved, leaving nulls in the rows from g2 where users did not return to play the next day.  Change the join clause to use left join and count the distinct number of users from g1 and g2 per date.  SELECT\n    date(g1.created_at) as dt,\n    count(DISTINCT g1.user_id) as total_users,\n    count(DISTINCT g2.user_id) as retained_users\nFROM gameplays AS g1\nLEFT JOIN gameplays AS g2 ON\n    g1.user_id = g2.user_id\n    AND date(g1.created_at) = date(datetime(g2.created_at, '-1 day'))\nGROUP BY 1\nORDER BY 1\nLIMIT 100;  Now that we have retained users as count(distinct g2.user_id) and total users as count(distinct g1.user_id), divide retained users by total users to calculate 1 day retention!  SELECT\n    date(g1.created_at) as dt,\n    round(100 * count(DISTINCT g2.user_id) / count(DISTINCT g1.user_id)) AS retention\nFROM gameplays AS g1\nLEFT JOIN gameplays AS g2 ON\n    g1.user_id = g2.user_id\n    AND date(g1.created_at) = date(datetime(g2.created_at, '-1 day'))\nGROUP BY 1\nORDER BY 1\nLIMIT 100;  While every business has different metrics to track their success, most are based on revenue and usage.  The metrics in this lesson are merely a starting point, and from here you ll be able to create and customize metrics to track whatever is most important to your company.  And remember, data science is exploratory! The current set of metrics can always be improved and there s usually more to any spike or dip than what immediately meets the eye.  Let s generalize what we ve learned so far:   Key Performance Indicators are high level health metrics for a business.  Daily Revenue is the sum of money made per day.  Daily Active Users are the number of unique users seen each day.  Daily Average Revenue Per Purchasing User (ARPPU) is the average amount of money spent by purchasers each day.  Daily Average Revenue Per User (ARPU) is the average amount of money across all users.  1 Day Retention is defined as the number of players from Day N who came back to play again on Day N+1.", 
            "title": "Avanced Aggregates"
        }, 
        {
            "location": "/Codecademy SQL, Table Transformation/", 
            "text": "Codecademy SQL, Table Transformation\n\n\nForeword\n\n\nCode snippets. From Codecademy.\n\n\n\n\nCONTENT\n\n\nCodecademy SQL, Table Transformation\n\n\n1, Subqueries\n\n\nNon-Correlated Subqueries\n\n\nCorrelated Subqueries\n\n\nWhat can we generalize so far?\n\n\n\n\n\n\nMerging Tables Together\n\n\nUNION\n\n\nINTERSECT\n\n\nEXCEPT\n\n\nWhat can we generalize so far?\n\n\n\n\n\n\n3, Conditional Aggregates\n\n\nWhat can we generalize so far?\n\n\n\n\n\n\n4, Date, Number, and String Functions\n\n\nWhat can we generalize so far?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile working with databases, we often need to transform data from one format to achieve a desired result. In SQL, this is often called data transformation or table transformation.\n\n\n1, Subqueries\n\n\nSubqueries, sometimes referred to as inner queries or nested queries, are used to transform table data by nesting one query within another query. \n\n\nTwo tables: \nairports\n and \nflights\n.\n\n\nSelect ten rows from the flights table.\n\n\nSELECT * FROM flights LIMIT 10;\n```sql\n\nWe first create an inner query, or subquery, that finds the 'airports' with elevation greater than 2000 from the airports table. \n\n```sql\nSELECT code FROM airports WHERE elevation \n 2000;\n\n\n\n\nNext, we take the result set of the inner query and use it to filter on the \nflights\n table, to find the flight detail that meets the elevation criteria.\n\n\nSELECT * FROM flights WHERE origin in (\n    SELECT code FROM airports WHERE elevation \n 2000);\n\n\n\n\nFind flight information about flights where the origin elevation is less than 2000 feet.\n\n\nSELECT * FROM flights WHERE origin in (\n    SELECT code FROM airports WHERE elevation \n 2000);\n\n\n\n\nNon-Correlated Subqueries\n\n\nA non-correlated subquery is a subquery that can be run independently of the outer query and as we saw, can be used to complete a multi-step transformation.\n\n\nPerhaps we\nd like to look at a selection of flights whose origin airport is a seaplane base, designated by SEAPLANE_BASE. The facility type of an airport is located in the fac_type field of the airports table.\n\n\nSELECT * FROM flights WHERE origin in (\n    SELECT code FROM airports WHERE fac_type = 'SEAPLANE_BASE');\n\n\n\n\nUsing the same pattern, find flight information about flights where the Federal Aviation Administration region (faa_region) is the Southern region (ASO).\n\n\nSELECT * FROM flights WHERE origin in (\n    SELECT code FROM airports WHERE faa_region = 'ASO');\n\n\n\n\nPerform transformations on a single table. For instance, sometimes we need to aggregate in multiple steps - like taking an average of a count.\n\n\nImagine you\u2019d like to know how many flights there are on average, for all Fridays in a given month from the flights table. First, we\u2019d need to calculate the number of flights per day, and then we\u2019d need to calculate the average based on the daily flight count for each day of the week. We can do this all in one step using a subquery:\n\n\nSELECT a.dep_month,\n       a.dep_day_of_week,\n       AVG(a.flight_count) AS average_flights\n    FROM (\n        SELECT dep_month,\n               dep_day_of_week,\n               dep_date,\n               COUNT(*) AS flight_count\n        FROM flights\n        GROUP BY 1,2,3\n        ) a\nGROUP BY 1,2\nORDER BY 1,2;\n\n\n\n\nThe inner query provides the count of flights by day, and the outer query uses the inner query\u2019s result set to compute the average by day of week of a given month.\n\n\nUsing a subquery, find the average total distance flown by day of week and month. Be sure to alias the outer query as average_distance and the inner query as flight_distance.\n\n\nSELECT a.dep_month,\n       a.dep_day_of_week,\n       AVG(a.flight_distance) AS average_distance\n    FROM (\n        SELECT dep_month,\n              dep_day_of_week,\n               dep_date,\n               sum(distance) AS flight_distance\n        FROM flights\n        GROUP BY 1,2,3\n    ) a\nGROUP BY 1,2\nORDER BY 1,2;\n\n\n\n\nCorrelated Subqueries\n\n\nIn a correlated subquery, the subquery can not be run independently of the outer query. The order of operations is important in a correlated subquery:\n\n\nA row is processed in the outer query.\nThen, for that particular row in the outer query, the subquery is executed.\n\n\n\nThis means that for each row processed by the outer query, the subquery will also be processed for that row. In this example, we will find the list of all flights whose distance is above average for their carrier (query on a query, same table).\n\n\nSELECT id\nFROM flights AS f\nWHERE distance \n (\n    SELECT AVG(distance)\n    FROM flights\n    WHERE carrier = f.carrier);\n\n\n\n\nIn the above query the inner query has to be re-executed for each flight. Correlated subqueries may appear elsewhere besides the WHERE clause, they can also appear in the SELECT.\n\n\nFind the id of the flights whose distance is below average for their carrier.\n\n\nSELECT id\nFROM flights AS f\nWHERE distance \n (\n    SELECT AVG(distance)\n    FROM flights\n    WHERE carrier = f.carrier);\n\n\n\n\nIt would also be interesting to order flights by giving them a sequence number based on time, by carrier. For instance, assuming flight_id increments with each additional flight, we could use the following query to view flights by carrier, flight id, and sequence number:\n\n\nSELECT carrier, id,\n    (SELECT COUNT(*)\n        FROM flights f\n        WHERE f.id \n flights.id\n        AND f.carrier=flights.carrier) + 1 \n        AS flight_sequence_number\n        FROM flights;\n\n\n\n\nUsing the same pattern, write a query to view flights by origin, flight id, and sequence number. Alias the sequence number column as flight_sequence_number.\n\n\nSELECT origin, id,\n    (SELECT COUNT(*)\n    FROM flights f\n    WHERE f.id \n flights.id\n    AND f.origin=flights.origin) + 1 \n    AS flight_sequence_number\n    FROM flights;\n\n\n\n\nWhat can we generalize so far?\n\n\n\n\nSubqueries are used to complete an SQL transformation by nesting one query within another query.\n\n\nA non-correlated subquery is a subquery that can be run independently of the outer query and can be used to complete a multi-step transformation.\n\n\nA correlated subquery is a subquery that cannot be run independently of the outer query. The order of operations in a correlated subquery is as follows:\n\n\nA row is processed in the outer query.\n\n\nThen, for that particular row in the outer query, the subquery is executed.\n\n\nSet Operations.\n\n\n\n\n\n\n\n\nUnions allow us to utilize information from multiple tables in our queries. In this lesson, we\u2019ll utilize data from an e-commerce store. Let\u2019s explore the available data we\u2019ll be using. \n\n\nFour tables: \nnew_products\n, legacy_products\n, \norder_items\n and \norder_items_historic\n.\n\n\nIn our database, we have products tables that contain metadata about each product in the store. Select ten rows from the new_products table.\n\n\nSELECT * FROM new_products LIMIT 10;\n\n\n\n\nMerging Tables Together\n\n\nSometimes, in order to answer certain questions based on data, we need to merge two tables together and then query the merged result. Perhaps we have two tables that contain information about products in an ecommerce store that we would like to combine. There are two ways of doing this:\n\n\n\n\nMerge the rows, called a JOIN.\n\n\nMerge the columns, called a UNION.\n\n\n\n\nUNION\n\n\nWe\nll focus on unions here. Union combines the result of two or more SELECT statements, using the following syntax:\n\n\nSELECT column_name(s) FROM table1\nUNION\nSELECT column_name(s) FROM table2;\n\n\n\n\nEach SELECT statement within the UNION must have the same number of columns with similar data types. The columns in each SELECT statement must be in the same order. By default, the UNION operator selects only distinct values.\n\n\nSuppose we are a growing ecommerce store and recently acquired another store to diversify our offering. The product data still exists in two separate tables: a legacy_products table and a new_products table. To get the complete list of product names from both tables, we can perform the following union.\n\n\nSELECT item_name FROM legacy_products\nUNION \nSELECT item_name FROM new_products;\n\n\n\n\nSelect a complete list of brand names from the legacy_products and new_products tables.\n\n\nSELECT brand FROM legacy_products\nUNION\nSELECT brand FROM new_products;\n\n\n\n\nWhat if we wanted to allow duplicate values? We can do this by using the ALL keyword with UNION, with the following syntax:\n\n\nSELECT column_name(s) FROM table1\nUNION ALL\nSELECT column_name(s) FROM table2;\n\n\n\n\nIn our ecommerce store, if we learned that we had records from historic order items in an additional table, we could use the following query to combine the tables for a complete analysis of sale price:\n\n\nSELECT id, sale_price FROM order_items\nUNION ALL\nSELECT id, sale_price FROM order_items_historic;\n\n\n\n\nThen we can perform an analysis on top of the combined result set, like finding the total count of order items.\n\n\nSELECT count(*) FROM (\n    SELECT id, sale_price FROM order_items\n    UNION ALL\n    SELECT id, sale_price FROM order_items_historic) as a;\n\n\n\n\nUsing the same pattern, utilize a subquery to find the average sale price over both order_items and order_items_historic tables.\n\n\nSELECT id, avg(a.sale_price) FROM (\n    SELECT id, sale_price FROM order_items\n    UNION ALL\n    SELECT id, sale_price FROM order_items_historic) AS a \n    GROUP BY 1;\n\n\n\n\n(before running the top analysis, create an alias a with the preliminary results, run the avg(a.sale_price), and group by 1 to view separate records and not a unique aggregate record!!!)\n\n\nINTERSECT\n\n\nis used to combine two SELECT statements, but returns rows only from the first SELECT statement that are identical to a row in the second SELECT statement. This means that it returns only common rows returned by the two SELECT statements.\n\n\nSELECT column_name(s) FROM table1\nINTERSECT\nSELECT column_name(s) FROM table2;\n\n\n\n\nFor instance, we might want to know what brands in our newly acquired store are also in our legacy store. We can do so using the following query:\n\n\nSELECT brand FROM new_products\nINTERSECT\nSELECT brand FROM legacy_products;\n\n\n\n\nSelect the items in the category column that are both in the newly acquired new_products table and the legacy_products table.\n\n\nSELECT category FROM new_products\nINTERSECT\nSELECT category FROM legacy_products;\n\n\n\n\nEXCEPT\n\n\nis constructed in the same way, but returns distinct rows from the first SELECT statement that aren\u2019t output by the second SELECT statement.\n\n\nSELECT column_name(s) FROM table1\nEXCEPT\nSELECT column_name(s) FROM table2;\n\n\n\n\nSuppose we want to see if there are any categories that are in the new_products table that aren\nt in the legacy_products table. We can use an EXCEPT query to perform this analysis:\n\n\nSELECT category FROM new_products\nEXCEPT\nSELECT category FROM legacy_products;\n\n\n\n\nConversely, select the items in the category column that are in the legacy_products table and not in the new_products table.\n\n\nSELECT category FROM legacy_products\nEXCEPT\nSELECT category FROM new_products;\n\n\n\n\nWhat can we generalize so far?\n\n\n\n\nThe UNION clause allows us to utilize information from multiple tables in our queries.\n\n\nThe UNION ALL clause allows us to utilize information from multiple tables in our queries, including duplicate values.\n\n\nINTERSECT is used to combine two SELECT statements, but returns rows only from the first SELECT statement that are identical to a row in the second SELECT statement.\n\n\nEXCEPT returns distinct rows from the first SELECT statement that aren\u2019t output by the second SELECT statement\n\n\n\n\n3, Conditional Aggregates\n\n\nAggregate functions compute a single result from a set of multiple input values. You can think of aggregate data as data collected from multiple rows at a time. In this lesson, we\nll continue learning about aggregate functions by focusing on conditionals, sums, and combining aggregate functions.\n\n\nConditional Aggregates are aggregate functions that compute a result set based on a given set of conditions.\n\n\nThe count function is an aggregate function, since it aggregates data from multiple rows.\n\n\nCount the number of rows in the flights table, representing the total number of flights contained in the table.\n\n\nSELECT COUNT(*) FROM flights;\n\n\n\n\nWhile working with databases, it\ns common to have empty or unknown \ncells\n in data tables.\n\n\nWhat do we do when we need to test whether a value is or is not null? We use the special keywords IS NULL or IS NOT NULL in the WHERE clause (= NULL does not work).\n\n\nCount the number of rows from the flights table, where arr_time is not null and the destination is ATL.\n\n\nSELECT COUNT(*) FROM flights WHERE arr_time IS NOT NULL AND destination = \nATL\n;\n\n\n\n\nAlmost every programming language has a way to represent \nif, then, else\n, or conditional logic. In SQL, we represent this logic with the CASE statement, as follows:\n\n\nSELECT\n    CASE\n        WHEN elevation \n 500 THEN 'Low'\n        WHEN elevation BETWEEN 500 AND 1999 THEN 'Medium'\n        WHEN elevation \n= 2000 THEN 'High'\n        ELSE 'Unknown'\n    END AS elevation_tier, \n    COUNT(*)\nFROM airports\nGROUP BY 1;\n\n\n\n\nIn the above statement, END is required to terminate the statement, but ELSE is optional. If ELSE is not included, the result will be NULL. Also notice the shorthand method of referencing columns to use in GROUP BY, so we don\nt have to rewrite the entire Case Statement.\n\n\nModify the case statement\ns such that when the elevation is less than 250, the elevation_tier column returns \nLow\n, when between 250 and 1749 it returns \nMedium\n, and when greater than or equal to 1750 it returns \nHigh\n.\n\n\nBe sure to alias the conditional statement as elevation_tier, in your query.\n\n\nSELECT\n    CASE\n        WHEN elevation \n 250 THEN 'Low'\n        WHEN elevation BETWEEN 250 AND 1749 THEN 'Medium'\n        WHEN elevation \n= 1750 THEN 'High'\n        ELSE 'Unknown'\n    END AS elevation_tier, \n    COUNT(*)\nFROM airports\nGROUP BY 1;\n\n\n\n\nSometimes you want to look at an entire result set, but want to implement conditions on certain aggregates.\n\n\nFor instance, maybe you want to identify the total amount of airports as well as the total amount of airports with high elevation in the same result set. We can accomplish this by putting a CASE WHEN statement in the aggregate.\n\n\nSELECT state, \n    COUNT(CASE WHEN elevation \n= 2000 THEN 1 ELSE NULL END) as count_high_elevation_aiports \nFROM airports \nGROUP BY state;\n\n\n\n\nUsing the same pattern, write a query to count the number of low elevation airports by state where low elevation is defined as less than 1000 ft. Be sure to alias the counted airports as count_low_elevation_airports.\n\n\nSELECT state, \n    COUNT(CASE WHEN elevation \n 1000 THEN 1 ELSE NULL END) as count_low_elevation_aiports \nFROM airports \nGROUP BY state;\n\n\n\n\nWe can do that same thing for other aggregates like SUM(). For instance, if we wanted to sum the total flight distance and compare that to the sum of flight distance from a particular airline (in this case, United Airlines) by origin airport, we could run the following query:\n\nsum(distance) for all carriers (total_flight_distance)\n\nsum(distance) for UA only (others are turned to 0) (total_united_flight_distance).\n\n\nSELECT origin, sum(distance) as total_flight_distance, sum(CASE WHEN carrier = 'UA' THEN distance ELSE 0 END) as total_united_flight_distance \nFROM flights \nGROUP BY origin;\n\n\n\n\nUsing the same pattern, find both the total flight distance as and flight distance by origin for Delta (carrier = \nDL\n).\n\n\nAlias the flight distance as total_flight_distance and the and flight distance by origin as total_delta_flight_distance.\n\n\nSELECT origin, sum(distance) as total_flight_distance, sum(CASE WHEN carrier = 'DL' THEN distance ELSE 0 END) as total_delta_flight_distance \nFROM flights \nGROUP BY origin;\n\n\n\n\nUsing the same pattern, find the percentage of flights from Delta by origin (carrier = \nDL\n):\n\n\nSELECT origin, 100.0*(sum(CASE WHEN carrier = 'DL' THEN distance ELSE 0 END)/sum(distance)) as percentage_flight_distance_from_delta FROM flights \nGROUP BY origin;\n\n\n\n\nFind the percentage of high elevation airports (elevation \n= 2000) by state from the airports table. In the query, alias the percentage column as percentage_high_elevation_airports.\n\n(sum of \n1\n) / count(\n) and not (count of \n1\n) / count(\n):\n\n\nSELECT state, 100.0 * sum(CASE WHEN elevation \n= 2000 THEN 1 ELSE 0 END) / count(*)  as percentage_high_elevation_airports FROM airports GROUP BY state;\n\n\n\n\nWhat can we generalize so far?\n\n\n\n\nConditional Aggregates are aggregate functions the compute a result set based on a given set of conditions.\n\n\nNULL can be used to denote an empty field value\n\n\nCASE statements allow for custom classification of data\n\n\nCASE statements can be used inside aggregates (like SUM() and COUNT()) to provide filtered measures\n\n\n\n\n4, Date, Number, and String Functions\n\n\nOftentimes, data in columns of tables is not in the exact format we need to complete our desired analysis. We may need to extract a date from a full timestamp, manipulate a number, or combine first and last name columns to create a full name.\n\n\nIn this lesson, we\nll be learning about some of SQL\ns built-in functions for transforming dates, numbers and strings. We\nll be using database of bakeries in this lesson.\n\n\nIt is important to note that date, number, and string functions are highly database dependent. Here, we focus on built-in functions in the SQLite database management system.\n\n\nSelect ten rows from the bakeries table:\n\n\nSELECT * FROM bakeries LIMIT 10;\n\n\n\n\nWe\nll begin with dates. Dates are often written in the following format\n\n\n\n\nDate: YYYY-MM-DD\n\n\nDatetime or Timestamp: YYYY-MM-DD hh:mm:ss\n\n\n\n\nWe can use SQL\ns date functions to transform data into a desired format. Since date functions can be database specific, verify the functions that exist on your relational database management system. For example, this statement:\n\n\nSELECT DATETIME(manufacture_time) FROM baked_goods;\n\n\n\n\nUsing the datetime function, select the date and time of all deliveries in the baked_goods table using the column delivery_time.\n\n\nSELECT DATETIME(delivery_time) FROM baked_goods;\n\n\n\n\nNow let\ns assume that we have a column in our baked_goods table named manufacture_time in the format YYYY-MM-DD hh:mm:ss. We\nd like to know the number of baked_goods manufactured by day, and not by second. We can use the DATE() function to easily convert timestamps to dates and complete the following query:\n\n\nSELECT DATE(manufacture_time), count(*) as count_baked_goods\nFROM baked_goods\nGROUP BY DATE(manufacture_time);\n\n\n\n\nSimilarly, we can query the time with:\n\n\nSELECT TIME(manufacture_time), count(*) as count_baked_goods\nFROM baked_goods\nGROUP BY TIME(manufacture_time);\n\n\n\n\nFind the number of baked goods by date of delivery. Be sure to alias the total count of baked goods as count_baked_goods.\n\n\nSELECT DATE(delivery_time), count(*) as count_baked_goods FROM baked_goods GROUP BY DATE(delivery_time);\n\n\n\n\nGiven a datepart and a column of date or timestamp data type, we can increment date or timestamp values by a specified interval. For example, in SQLite, the statement:\n\n\nDATETIME(time1, '+3 hours', '40 minutes', '2 days');\n\n\n\n\nWould return a time 3 hours, 20 minutes, and 2 days after time1.\n\n\nImagine that each dessert in our baked_goods table is inspected 2 hours, 30 minutes, and 1 day after the manufacture time. To derive the inspection date for each baked good, we can use the following query:\n\n\nSELECT DATETIME(manufacture_time, '+2 hours', '30 minutes', '1 day') as inspection_time\nFROM baked_goods;\n\n\n\n\nEach of the baked goods is packaged by Baker\ns Market exactly five hours, twenty minutes, and two days after the delivery (designated by delivery_time). Create a query returning all the packaging times for the goods in the baked_goods table. Be sure to alias the package time column as package_time.\n\n\nSELECT DATETIME(delivery_time, '+5 hours', '20 minutes', '2 days') as package_time\nFROM baked_goods;\n\n\n\n\nNumeric functions can be used to transform numbers. Some common SQLite mathematical functions are included below that take numeric data types as inputs:\n\n\n\n\nSELECT (number1 + number2);: Returns the sum of two numbers. Similar, SQL can be used for subtraction, multiplication, and division.\n\n\nSELECT CAST(number1 AS REAL) / number3;: Returns the result as a real number by casting one of the values as a real number, rather than an integer.\n\n\nSELECT ROUND(number, precision);: Returns the numeric value rounded off to the next value specified.\n\n\n\n\nIn our baked_goods table, we have information about cost designated by ingredients_cost. For accounting purposes, we\nd like to make sure that each ingredient cost is rounded to four decimal places rather than two, to account for currency fluctuations.\n\n\nSELECT ROUND(ingredients_cost, 4) as rounded_cost\nFROM baked_goods;\n\n\n\n\nFind the bakery\ns distance from the market rounded to two decimal places. Be sure to alias the column as distance_from_market.\n\n\nSELECT ROUND(distance, 2) as distance_from_market\nFROM bakeries;\n\n\n\n\nA couple more useful numeric SQL functions are included below: MAX and MIN. MAX(n1,n2,n3,\n): returns the greatest value in the set of the input numeric expressions MIN(n1,n2,n3,\n): returns the least value in the set of the input numeric expressions\n\n\nIn our baked_goods table, in addition to the numeric ingredients_cost we have information about the packaging cost located in the packaging_cost column. We can use the MAX function to determine the overall greatest value of cost for each item using the following query:\n\n\nSELECT id, MAX(ingredients_cost, packaging_cost)\nFROM baked_goods;\n\n\n\n\nWe also have information about cook time designated as cook_time and cool down time designated as cool_down_time in the baked_goods table. Find the greatest time value for each item in the table.\n\n\nSELECT id, MAX(cook_time, cool_down_time)\nFROM baked_goods;\n\n\n\n\nFind the least time value for each item in the table.\n\n\nSELECT id, MIN(cook_time, cool_down_time)\nFROM baked_goods;\n\n\n\n\nString manipulation can be useful to derive information from columns. We\nll cover a couple of the common string functions here.\n\n\nA common use case for string manipulation in SQL is concatenation of strings. In SQLite, this is written as:\n\n\nSELECT string1 || ' ' || string2;\n\n\n\n\nFor example, the bakeries table contains both city and state columns. In order to create a route for these columns, we use the || function to concatenate them as in the following query:\n\n\nSELECT city || ' ' || state as location\nFROM bakeries;\n\n\n\n\nString functions are again, very database specific, and it is best practice to consult documentation before proceeding. \n\n\nCombine the first_name and last_name columns from the bakeries table as the full_name to identify the owners of the bakeries. Be sure to add a space between the names in the full_name as shown in the example.\n\n\nSELECT first_name || ' ' || last_name as full_name FROM bakeries;\n\n\n\n\nAnother useful string function in SQL is REPLACE():\n\n\nREPLACE(string,from_string,to_string)\n\n\n\n\nThe function returns the string string with all occurrences of the string from_string replaced by the string to_string. For example in baked_goods, there is a column named ingredients. The ingredients strings are formatted with underscores, such as baking_soda and vanilla_extract. To make these values more readable, we might like to replace the underscores with spaces. We can do so by using the following query:\n\n\nSELECT id, REPLACE(ingredients,'_',' ') as item_ingredients\nfrom baked_goods;\n\n\n\n\nAny time enriched_flour appears in the ingredients list, we\u2019d like to replace it with just flour. Apply this transformation and also be sure to remove the underscore in enriched_flour.\n\n\nSELECT REPLACE(ingredients,'enriched_',' ') as item_ingredients\nFROM baked_goods;\n\n\n\n\nWhat can we generalize so far?\n\n\n\n\nDate Functions:\n\n\nDATETIME; Returns the date and time of the column specified. This can be modified to return only the date or only the time.\n\n\nDATETIME(time1, +X hours, Y minutes, Z days): Increments the specificed column by a given number of hours, minutes, or days.\n\n\n\n\n\n\nNumeric Functions:\n\n\n(number1 + number2);: Returns the sum of two numbers, or other mathematical operations, accordingly.\n\n\nCAST(number1 AS REAL) / number2;: Returns the result as a real number by casting one of numeric inputs as a real number\n\n\nROUND(number, precision);: Returns the numeric value rounded off to the next value specified.\n\n\n\n\n\n\nString Functions:\n\n\nstring1\n || \n \n || \nstring2\n;: Concatenates string1 and string 2, with a space between.\n\n\nREPLACE(string,from_string,to_string): Returns the string with all occurrences of the string from_string replaced by the string to_string.", 
            "title": "Codecademy SQL, Table Transformation"
        }, 
        {
            "location": "/Codecademy SQL, Table Transformation/#1-subqueries", 
            "text": "Subqueries, sometimes referred to as inner queries or nested queries, are used to transform table data by nesting one query within another query.   Two tables:  airports  and  flights .  Select ten rows from the flights table.  SELECT * FROM flights LIMIT 10;\n```sql\n\nWe first create an inner query, or subquery, that finds the 'airports' with elevation greater than 2000 from the airports table. \n\n```sql\nSELECT code FROM airports WHERE elevation   2000;  Next, we take the result set of the inner query and use it to filter on the  flights  table, to find the flight detail that meets the elevation criteria.  SELECT * FROM flights WHERE origin in (\n    SELECT code FROM airports WHERE elevation   2000);  Find flight information about flights where the origin elevation is less than 2000 feet.  SELECT * FROM flights WHERE origin in (\n    SELECT code FROM airports WHERE elevation   2000);", 
            "title": "1, Subqueries"
        }, 
        {
            "location": "/Codecademy SQL, Table Transformation/#non-correlated-subqueries", 
            "text": "A non-correlated subquery is a subquery that can be run independently of the outer query and as we saw, can be used to complete a multi-step transformation.  Perhaps we d like to look at a selection of flights whose origin airport is a seaplane base, designated by SEAPLANE_BASE. The facility type of an airport is located in the fac_type field of the airports table.  SELECT * FROM flights WHERE origin in (\n    SELECT code FROM airports WHERE fac_type = 'SEAPLANE_BASE');  Using the same pattern, find flight information about flights where the Federal Aviation Administration region (faa_region) is the Southern region (ASO).  SELECT * FROM flights WHERE origin in (\n    SELECT code FROM airports WHERE faa_region = 'ASO');  Perform transformations on a single table. For instance, sometimes we need to aggregate in multiple steps - like taking an average of a count.  Imagine you\u2019d like to know how many flights there are on average, for all Fridays in a given month from the flights table. First, we\u2019d need to calculate the number of flights per day, and then we\u2019d need to calculate the average based on the daily flight count for each day of the week. We can do this all in one step using a subquery:  SELECT a.dep_month,\n       a.dep_day_of_week,\n       AVG(a.flight_count) AS average_flights\n    FROM (\n        SELECT dep_month,\n               dep_day_of_week,\n               dep_date,\n               COUNT(*) AS flight_count\n        FROM flights\n        GROUP BY 1,2,3\n        ) a\nGROUP BY 1,2\nORDER BY 1,2;  The inner query provides the count of flights by day, and the outer query uses the inner query\u2019s result set to compute the average by day of week of a given month.  Using a subquery, find the average total distance flown by day of week and month. Be sure to alias the outer query as average_distance and the inner query as flight_distance.  SELECT a.dep_month,\n       a.dep_day_of_week,\n       AVG(a.flight_distance) AS average_distance\n    FROM (\n        SELECT dep_month,\n              dep_day_of_week,\n               dep_date,\n               sum(distance) AS flight_distance\n        FROM flights\n        GROUP BY 1,2,3\n    ) a\nGROUP BY 1,2\nORDER BY 1,2;", 
            "title": "Non-Correlated Subqueries"
        }, 
        {
            "location": "/Codecademy SQL, Table Transformation/#correlated-subqueries", 
            "text": "In a correlated subquery, the subquery can not be run independently of the outer query. The order of operations is important in a correlated subquery:  A row is processed in the outer query.\nThen, for that particular row in the outer query, the subquery is executed.  This means that for each row processed by the outer query, the subquery will also be processed for that row. In this example, we will find the list of all flights whose distance is above average for their carrier (query on a query, same table).  SELECT id\nFROM flights AS f\nWHERE distance   (\n    SELECT AVG(distance)\n    FROM flights\n    WHERE carrier = f.carrier);  In the above query the inner query has to be re-executed for each flight. Correlated subqueries may appear elsewhere besides the WHERE clause, they can also appear in the SELECT.  Find the id of the flights whose distance is below average for their carrier.  SELECT id\nFROM flights AS f\nWHERE distance   (\n    SELECT AVG(distance)\n    FROM flights\n    WHERE carrier = f.carrier);  It would also be interesting to order flights by giving them a sequence number based on time, by carrier. For instance, assuming flight_id increments with each additional flight, we could use the following query to view flights by carrier, flight id, and sequence number:  SELECT carrier, id,\n    (SELECT COUNT(*)\n        FROM flights f\n        WHERE f.id   flights.id\n        AND f.carrier=flights.carrier) + 1 \n        AS flight_sequence_number\n        FROM flights;  Using the same pattern, write a query to view flights by origin, flight id, and sequence number. Alias the sequence number column as flight_sequence_number.  SELECT origin, id,\n    (SELECT COUNT(*)\n    FROM flights f\n    WHERE f.id   flights.id\n    AND f.origin=flights.origin) + 1 \n    AS flight_sequence_number\n    FROM flights;", 
            "title": "Correlated Subqueries"
        }, 
        {
            "location": "/Codecademy SQL, Table Transformation/#what-can-we-generalize-so-far", 
            "text": "Subqueries are used to complete an SQL transformation by nesting one query within another query.  A non-correlated subquery is a subquery that can be run independently of the outer query and can be used to complete a multi-step transformation.  A correlated subquery is a subquery that cannot be run independently of the outer query. The order of operations in a correlated subquery is as follows:  A row is processed in the outer query.  Then, for that particular row in the outer query, the subquery is executed.  Set Operations.     Unions allow us to utilize information from multiple tables in our queries. In this lesson, we\u2019ll utilize data from an e-commerce store. Let\u2019s explore the available data we\u2019ll be using.   Four tables:  new_products , legacy_products ,  order_items  and  order_items_historic .  In our database, we have products tables that contain metadata about each product in the store. Select ten rows from the new_products table.  SELECT * FROM new_products LIMIT 10;", 
            "title": "What can we generalize so far?"
        }, 
        {
            "location": "/Codecademy SQL, Table Transformation/#merging-tables-together", 
            "text": "Sometimes, in order to answer certain questions based on data, we need to merge two tables together and then query the merged result. Perhaps we have two tables that contain information about products in an ecommerce store that we would like to combine. There are two ways of doing this:   Merge the rows, called a JOIN.  Merge the columns, called a UNION.", 
            "title": "Merging Tables Together"
        }, 
        {
            "location": "/Codecademy SQL, Table Transformation/#union", 
            "text": "We ll focus on unions here. Union combines the result of two or more SELECT statements, using the following syntax:  SELECT column_name(s) FROM table1\nUNION\nSELECT column_name(s) FROM table2;  Each SELECT statement within the UNION must have the same number of columns with similar data types. The columns in each SELECT statement must be in the same order. By default, the UNION operator selects only distinct values.  Suppose we are a growing ecommerce store and recently acquired another store to diversify our offering. The product data still exists in two separate tables: a legacy_products table and a new_products table. To get the complete list of product names from both tables, we can perform the following union.  SELECT item_name FROM legacy_products\nUNION \nSELECT item_name FROM new_products;  Select a complete list of brand names from the legacy_products and new_products tables.  SELECT brand FROM legacy_products\nUNION\nSELECT brand FROM new_products;  What if we wanted to allow duplicate values? We can do this by using the ALL keyword with UNION, with the following syntax:  SELECT column_name(s) FROM table1\nUNION ALL\nSELECT column_name(s) FROM table2;  In our ecommerce store, if we learned that we had records from historic order items in an additional table, we could use the following query to combine the tables for a complete analysis of sale price:  SELECT id, sale_price FROM order_items\nUNION ALL\nSELECT id, sale_price FROM order_items_historic;  Then we can perform an analysis on top of the combined result set, like finding the total count of order items.  SELECT count(*) FROM (\n    SELECT id, sale_price FROM order_items\n    UNION ALL\n    SELECT id, sale_price FROM order_items_historic) as a;  Using the same pattern, utilize a subquery to find the average sale price over both order_items and order_items_historic tables.  SELECT id, avg(a.sale_price) FROM (\n    SELECT id, sale_price FROM order_items\n    UNION ALL\n    SELECT id, sale_price FROM order_items_historic) AS a \n    GROUP BY 1;  (before running the top analysis, create an alias a with the preliminary results, run the avg(a.sale_price), and group by 1 to view separate records and not a unique aggregate record!!!)", 
            "title": "UNION"
        }, 
        {
            "location": "/Codecademy SQL, Table Transformation/#intersect", 
            "text": "is used to combine two SELECT statements, but returns rows only from the first SELECT statement that are identical to a row in the second SELECT statement. This means that it returns only common rows returned by the two SELECT statements.  SELECT column_name(s) FROM table1\nINTERSECT\nSELECT column_name(s) FROM table2;  For instance, we might want to know what brands in our newly acquired store are also in our legacy store. We can do so using the following query:  SELECT brand FROM new_products\nINTERSECT\nSELECT brand FROM legacy_products;  Select the items in the category column that are both in the newly acquired new_products table and the legacy_products table.  SELECT category FROM new_products\nINTERSECT\nSELECT category FROM legacy_products;", 
            "title": "INTERSECT"
        }, 
        {
            "location": "/Codecademy SQL, Table Transformation/#except", 
            "text": "is constructed in the same way, but returns distinct rows from the first SELECT statement that aren\u2019t output by the second SELECT statement.  SELECT column_name(s) FROM table1\nEXCEPT\nSELECT column_name(s) FROM table2;  Suppose we want to see if there are any categories that are in the new_products table that aren t in the legacy_products table. We can use an EXCEPT query to perform this analysis:  SELECT category FROM new_products\nEXCEPT\nSELECT category FROM legacy_products;  Conversely, select the items in the category column that are in the legacy_products table and not in the new_products table.  SELECT category FROM legacy_products\nEXCEPT\nSELECT category FROM new_products;", 
            "title": "EXCEPT"
        }, 
        {
            "location": "/Codecademy SQL, Table Transformation/#what-can-we-generalize-so-far_1", 
            "text": "The UNION clause allows us to utilize information from multiple tables in our queries.  The UNION ALL clause allows us to utilize information from multiple tables in our queries, including duplicate values.  INTERSECT is used to combine two SELECT statements, but returns rows only from the first SELECT statement that are identical to a row in the second SELECT statement.  EXCEPT returns distinct rows from the first SELECT statement that aren\u2019t output by the second SELECT statement", 
            "title": "What can we generalize so far?"
        }, 
        {
            "location": "/Codecademy SQL, Table Transformation/#3-conditional-aggregates", 
            "text": "Aggregate functions compute a single result from a set of multiple input values. You can think of aggregate data as data collected from multiple rows at a time. In this lesson, we ll continue learning about aggregate functions by focusing on conditionals, sums, and combining aggregate functions.  Conditional Aggregates are aggregate functions that compute a result set based on a given set of conditions.  The count function is an aggregate function, since it aggregates data from multiple rows.  Count the number of rows in the flights table, representing the total number of flights contained in the table.  SELECT COUNT(*) FROM flights;  While working with databases, it s common to have empty or unknown  cells  in data tables.  What do we do when we need to test whether a value is or is not null? We use the special keywords IS NULL or IS NOT NULL in the WHERE clause (= NULL does not work).  Count the number of rows from the flights table, where arr_time is not null and the destination is ATL.  SELECT COUNT(*) FROM flights WHERE arr_time IS NOT NULL AND destination =  ATL ;  Almost every programming language has a way to represent  if, then, else , or conditional logic. In SQL, we represent this logic with the CASE statement, as follows:  SELECT\n    CASE\n        WHEN elevation   500 THEN 'Low'\n        WHEN elevation BETWEEN 500 AND 1999 THEN 'Medium'\n        WHEN elevation  = 2000 THEN 'High'\n        ELSE 'Unknown'\n    END AS elevation_tier, \n    COUNT(*)\nFROM airports\nGROUP BY 1;  In the above statement, END is required to terminate the statement, but ELSE is optional. If ELSE is not included, the result will be NULL. Also notice the shorthand method of referencing columns to use in GROUP BY, so we don t have to rewrite the entire Case Statement.  Modify the case statement s such that when the elevation is less than 250, the elevation_tier column returns  Low , when between 250 and 1749 it returns  Medium , and when greater than or equal to 1750 it returns  High .  Be sure to alias the conditional statement as elevation_tier, in your query.  SELECT\n    CASE\n        WHEN elevation   250 THEN 'Low'\n        WHEN elevation BETWEEN 250 AND 1749 THEN 'Medium'\n        WHEN elevation  = 1750 THEN 'High'\n        ELSE 'Unknown'\n    END AS elevation_tier, \n    COUNT(*)\nFROM airports\nGROUP BY 1;  Sometimes you want to look at an entire result set, but want to implement conditions on certain aggregates.  For instance, maybe you want to identify the total amount of airports as well as the total amount of airports with high elevation in the same result set. We can accomplish this by putting a CASE WHEN statement in the aggregate.  SELECT state, \n    COUNT(CASE WHEN elevation  = 2000 THEN 1 ELSE NULL END) as count_high_elevation_aiports \nFROM airports \nGROUP BY state;  Using the same pattern, write a query to count the number of low elevation airports by state where low elevation is defined as less than 1000 ft. Be sure to alias the counted airports as count_low_elevation_airports.  SELECT state, \n    COUNT(CASE WHEN elevation   1000 THEN 1 ELSE NULL END) as count_low_elevation_aiports \nFROM airports \nGROUP BY state;  We can do that same thing for other aggregates like SUM(). For instance, if we wanted to sum the total flight distance and compare that to the sum of flight distance from a particular airline (in this case, United Airlines) by origin airport, we could run the following query: \nsum(distance) for all carriers (total_flight_distance) \nsum(distance) for UA only (others are turned to 0) (total_united_flight_distance).  SELECT origin, sum(distance) as total_flight_distance, sum(CASE WHEN carrier = 'UA' THEN distance ELSE 0 END) as total_united_flight_distance \nFROM flights \nGROUP BY origin;  Using the same pattern, find both the total flight distance as and flight distance by origin for Delta (carrier =  DL ).  Alias the flight distance as total_flight_distance and the and flight distance by origin as total_delta_flight_distance.  SELECT origin, sum(distance) as total_flight_distance, sum(CASE WHEN carrier = 'DL' THEN distance ELSE 0 END) as total_delta_flight_distance \nFROM flights \nGROUP BY origin;  Using the same pattern, find the percentage of flights from Delta by origin (carrier =  DL ):  SELECT origin, 100.0*(sum(CASE WHEN carrier = 'DL' THEN distance ELSE 0 END)/sum(distance)) as percentage_flight_distance_from_delta FROM flights \nGROUP BY origin;  Find the percentage of high elevation airports (elevation  = 2000) by state from the airports table. In the query, alias the percentage column as percentage_high_elevation_airports. \n(sum of  1 ) / count( ) and not (count of  1 ) / count( ):  SELECT state, 100.0 * sum(CASE WHEN elevation  = 2000 THEN 1 ELSE 0 END) / count(*)  as percentage_high_elevation_airports FROM airports GROUP BY state;", 
            "title": "3, Conditional Aggregates"
        }, 
        {
            "location": "/Codecademy SQL, Table Transformation/#what-can-we-generalize-so-far_2", 
            "text": "Conditional Aggregates are aggregate functions the compute a result set based on a given set of conditions.  NULL can be used to denote an empty field value  CASE statements allow for custom classification of data  CASE statements can be used inside aggregates (like SUM() and COUNT()) to provide filtered measures", 
            "title": "What can we generalize so far?"
        }, 
        {
            "location": "/Codecademy SQL, Table Transformation/#4-date-number-and-string-functions", 
            "text": "Oftentimes, data in columns of tables is not in the exact format we need to complete our desired analysis. We may need to extract a date from a full timestamp, manipulate a number, or combine first and last name columns to create a full name.  In this lesson, we ll be learning about some of SQL s built-in functions for transforming dates, numbers and strings. We ll be using database of bakeries in this lesson.  It is important to note that date, number, and string functions are highly database dependent. Here, we focus on built-in functions in the SQLite database management system.  Select ten rows from the bakeries table:  SELECT * FROM bakeries LIMIT 10;  We ll begin with dates. Dates are often written in the following format   Date: YYYY-MM-DD  Datetime or Timestamp: YYYY-MM-DD hh:mm:ss   We can use SQL s date functions to transform data into a desired format. Since date functions can be database specific, verify the functions that exist on your relational database management system. For example, this statement:  SELECT DATETIME(manufacture_time) FROM baked_goods;  Using the datetime function, select the date and time of all deliveries in the baked_goods table using the column delivery_time.  SELECT DATETIME(delivery_time) FROM baked_goods;  Now let s assume that we have a column in our baked_goods table named manufacture_time in the format YYYY-MM-DD hh:mm:ss. We d like to know the number of baked_goods manufactured by day, and not by second. We can use the DATE() function to easily convert timestamps to dates and complete the following query:  SELECT DATE(manufacture_time), count(*) as count_baked_goods\nFROM baked_goods\nGROUP BY DATE(manufacture_time);  Similarly, we can query the time with:  SELECT TIME(manufacture_time), count(*) as count_baked_goods\nFROM baked_goods\nGROUP BY TIME(manufacture_time);  Find the number of baked goods by date of delivery. Be sure to alias the total count of baked goods as count_baked_goods.  SELECT DATE(delivery_time), count(*) as count_baked_goods FROM baked_goods GROUP BY DATE(delivery_time);  Given a datepart and a column of date or timestamp data type, we can increment date or timestamp values by a specified interval. For example, in SQLite, the statement:  DATETIME(time1, '+3 hours', '40 minutes', '2 days');  Would return a time 3 hours, 20 minutes, and 2 days after time1.  Imagine that each dessert in our baked_goods table is inspected 2 hours, 30 minutes, and 1 day after the manufacture time. To derive the inspection date for each baked good, we can use the following query:  SELECT DATETIME(manufacture_time, '+2 hours', '30 minutes', '1 day') as inspection_time\nFROM baked_goods;  Each of the baked goods is packaged by Baker s Market exactly five hours, twenty minutes, and two days after the delivery (designated by delivery_time). Create a query returning all the packaging times for the goods in the baked_goods table. Be sure to alias the package time column as package_time.  SELECT DATETIME(delivery_time, '+5 hours', '20 minutes', '2 days') as package_time\nFROM baked_goods;  Numeric functions can be used to transform numbers. Some common SQLite mathematical functions are included below that take numeric data types as inputs:   SELECT (number1 + number2);: Returns the sum of two numbers. Similar, SQL can be used for subtraction, multiplication, and division.  SELECT CAST(number1 AS REAL) / number3;: Returns the result as a real number by casting one of the values as a real number, rather than an integer.  SELECT ROUND(number, precision);: Returns the numeric value rounded off to the next value specified.   In our baked_goods table, we have information about cost designated by ingredients_cost. For accounting purposes, we d like to make sure that each ingredient cost is rounded to four decimal places rather than two, to account for currency fluctuations.  SELECT ROUND(ingredients_cost, 4) as rounded_cost\nFROM baked_goods;  Find the bakery s distance from the market rounded to two decimal places. Be sure to alias the column as distance_from_market.  SELECT ROUND(distance, 2) as distance_from_market\nFROM bakeries;  A couple more useful numeric SQL functions are included below: MAX and MIN. MAX(n1,n2,n3, ): returns the greatest value in the set of the input numeric expressions MIN(n1,n2,n3, ): returns the least value in the set of the input numeric expressions  In our baked_goods table, in addition to the numeric ingredients_cost we have information about the packaging cost located in the packaging_cost column. We can use the MAX function to determine the overall greatest value of cost for each item using the following query:  SELECT id, MAX(ingredients_cost, packaging_cost)\nFROM baked_goods;  We also have information about cook time designated as cook_time and cool down time designated as cool_down_time in the baked_goods table. Find the greatest time value for each item in the table.  SELECT id, MAX(cook_time, cool_down_time)\nFROM baked_goods;  Find the least time value for each item in the table.  SELECT id, MIN(cook_time, cool_down_time)\nFROM baked_goods;  String manipulation can be useful to derive information from columns. We ll cover a couple of the common string functions here.  A common use case for string manipulation in SQL is concatenation of strings. In SQLite, this is written as:  SELECT string1 || ' ' || string2;  For example, the bakeries table contains both city and state columns. In order to create a route for these columns, we use the || function to concatenate them as in the following query:  SELECT city || ' ' || state as location\nFROM bakeries;  String functions are again, very database specific, and it is best practice to consult documentation before proceeding.   Combine the first_name and last_name columns from the bakeries table as the full_name to identify the owners of the bakeries. Be sure to add a space between the names in the full_name as shown in the example.  SELECT first_name || ' ' || last_name as full_name FROM bakeries;  Another useful string function in SQL is REPLACE():  REPLACE(string,from_string,to_string)  The function returns the string string with all occurrences of the string from_string replaced by the string to_string. For example in baked_goods, there is a column named ingredients. The ingredients strings are formatted with underscores, such as baking_soda and vanilla_extract. To make these values more readable, we might like to replace the underscores with spaces. We can do so by using the following query:  SELECT id, REPLACE(ingredients,'_',' ') as item_ingredients\nfrom baked_goods;  Any time enriched_flour appears in the ingredients list, we\u2019d like to replace it with just flour. Apply this transformation and also be sure to remove the underscore in enriched_flour.  SELECT REPLACE(ingredients,'enriched_',' ') as item_ingredients\nFROM baked_goods;", 
            "title": "4, Date, Number, and String Functions"
        }, 
        {
            "location": "/Codecademy SQL, Table Transformation/#what-can-we-generalize-so-far_3", 
            "text": "Date Functions:  DATETIME; Returns the date and time of the column specified. This can be modified to return only the date or only the time.  DATETIME(time1, +X hours, Y minutes, Z days): Increments the specificed column by a given number of hours, minutes, or days.    Numeric Functions:  (number1 + number2);: Returns the sum of two numbers, or other mathematical operations, accordingly.  CAST(number1 AS REAL) / number2;: Returns the result as a real number by casting one of numeric inputs as a real number  ROUND(number, precision);: Returns the numeric value rounded off to the next value specified.    String Functions:  string1  ||     ||  string2 ;: Concatenates string1 and string 2, with a space between.  REPLACE(string,from_string,to_string): Returns the string with all occurrences of the string from_string replaced by the string to_string.", 
            "title": "What can we generalize so far?"
        }, 
        {
            "location": "/Introduction to SQL JOINs/", 
            "text": "Introduction to SQL JOINs\n\n\nForeword\n\n\nNotes.\n\n\n\n\nCONTENT\n\n\nIntroduction to SQL JOINs\n\n\nUsing the code\n\n\nInner JOIN\n\n\nLeft JOIN\n\n\nRight JOIN\n\n\nOuter JOIN\n\n\nLeft Excluding JOIN\n\n\nRight Excluding JOIN\n\n\nOuter Excluding JOIN\n\n\nExamples\n\n\nConclusion\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the code\n\n\nSeven different ways you can return data from two relational tables; excluding cross Joins and self referencing Joins:\n\n\n\n\nINNER JOIN\n\n\nLEFT JOIN\n\n\nRIGHT JOIN\n\n\nOUTER JOIN\n\n\nLEFT JOIN EXCLUDING INNER JOIN\n\n\nRIGHT JOIN EXCLUDING INNER JOIN\n\n\nOUTER JOIN EXCLUDING INNER JOIN\n\n\n\n\n\n\nFor the sake of this article, 5, 6, and 7 are LEFT EXCLUDING JOIN, RIGHT EXCLUDING JOIN, and OUTER EXCLUDING JOIN, respectively. Some may argue that 5, 6, and 7 are not really joining the two tables, but for simplicity, let\ns refer to these as Joins because you use a SQL Join in each of these queries (but exclude some records with a WHERE clause).\n\n\nInner JOIN\n\n\n\n\nThis is the simplest, most understood Join and is the most common. This query will return all of the records in the left table (table A) that have a matching record in the right table (table B). This Join is written as follows:\n\n\nSELECT \nselect_list\n \nFROM Table_A A\nINNER JOIN Table_B B\nON A.Key = B.Key\n\n\n\n\nLeft JOIN\n\n\n\n\nThis query will return all of the records in the left table (table A) regardless if any of those records have a match in the right table (table B). It will also return any matching records from the right table. This Join is written as follows:\n\n\nSELECT \nselect_list\n\nFROM Table_A A\nLEFT JOIN Table_B B\nON A.Key = B.Key\n\n\n\n\nRight JOIN\n\n\n\n\nThis query will return all of the records in the right table (table B) regardless if any of those records have a match in the left table (table A). It will also return any matching records from the left table. This Join is written as follows:\n\n\nSELECT \nselect_list\n\nFROM Table_A A\nRIGHT JOIN Table_B B\nON A.Key = B.Key\n\n\n\n\nOuter JOIN\n\n\n\n\nThis Join can also be referred to as a FULL OUTER JOIN or a FULL JOIN. This query will return all of the records from both tables, joining records from the left table (table A) that match records from the right table (table B). This Join is written as follows:\n\n\nSELECT \nselect_list\n\nFROM Table_A A\nFULL OUTER JOIN Table_B B\nON A.Key = B.Key\n\n\n\n\nLeft Excluding JOIN\n\n\n\n\nThis query will return all of the records in the left table (table A) that do not match any records in the right table (table B). This Join is written as follows:\n\n\nSELECT \nselect_list\n \nFROM Table_A A\nLEFT JOIN Table_B B\nON A.Key = B.Key\nWHERE B.Key IS NULL\n\n\n\n\nRight Excluding JOIN\n\n\n\n\nThis query will return all of the records in the right table (table B) that do not match any records in the left table (table A). This Join is written as follows:\n\n\nSELECT \nselect_list\n\nFROM Table_A A\nRIGHT JOIN Table_B B\nON A.Key = B.Key\nWHERE A.Key IS NULL\n\n\n\n\nOuter Excluding JOIN\n\n\n\n\nThis query will return all of the records in the left table (table A) and all of the records in the right table (table B) that do not match. I have yet to have a need for using this type of Join, but all of the others, I use quite frequently. This Join is written as follows:\n\n\nSELECT \nselect_list\n\nFROM Table_A A\nFULL OUTER JOIN Table_B B\nON A.Key = B.Key\nWHERE A.Key IS NULL OR B.Key IS NULL\n\n\n\n\nExamples\n\n\nSuppose we have two tables, Table_A and Table_B. The data in these tables are shown below:\n\n\nTABLE_A\n  PK Value\n---- ----------\n   1 FOX\n   2 COP\n   3 TAXI\n   6 WASHINGTON\n   7 DELL\n   5 ARIZONA\n   4 LINCOLN\n  10 LUCENT\n\nTABLE_B\n  PK Value\n---- ----------\n   1 TROT\n   2 CAR\n   3 CAB\n   6 MONUMENT\n   7 PC\n   8 MICROSOFT\n   9 APPLE\n  11 SCOTCH\n\n\n\n\nThe results of the seven Joins are shown below:\n\n\nINNER JOIN\n\n\n--INNER JOIN\nSELECT A.PK AS A_PK, A.Value AS A_Value,\n       B.Value AS B_Value, B.PK AS B_PK\nFROM Table_A A\nINNER JOIN Table_B B\nON A.PK = B.PK\n\n\n\n\nA_PK A_Value    B_Value    B_PK\n---- ---------- ---------- ----\n   1 FOX        TROT          1\n   2 COP        CAR           2\n   3 TAXI       CAB           3\n   6 WASHINGTON MONUMENT      6\n   7 DELL       PC            7\n\n(5 row(s) affected)\n\n\n\n\nLEFT JOIN\n\n\n--LEFT JOIN\nSELECT A.PK AS A_PK, A.Value AS A_Value,\nB.Value AS B_Value, B.PK AS B_PK\nFROM Table_A A\nLEFT JOIN Table_B B\nON A.PK = B.PK\n\n\n\n\nA_PK A_Value    B_Value    B_PK\n---- ---------- ---------- ----\n   1 FOX        TROT          1\n   2 COP        CAR           2\n   3 TAXI       CAB           3\n   4 LINCOLN    NULL       NULL\n   5 ARIZONA    NULL       NULL\n   6 WASHINGTON MONUMENT      6\n   7 DELL       PC            7\n  10 LUCENT     NULL       NULL\n\n(8 row(s) affected)\n\n\n\n\nRIGHT JOIN\n\n\n--RIGHT JOIN\nSELECT A.PK AS A_PK, A.Value AS A_Value,\nB.Value AS B_Value, B.PK AS B_PK\nFROM Table_A A\nRIGHT JOIN Table_B B\nON A.PK = B.PK\n\n\n\n\nA_PK A_Value    B_Value    B_PK\n---- ---------- ---------- ----\n   1 FOX        TROT          1\n   2 COP        CAR           2\n   3 TAXI       CAB           3\n   6 WASHINGTON MONUMENT      6\n   7 DELL       PC            7\nNULL NULL       MICROSOFT     8\nNULL NULL       APPLE         9\nNULL NULL       SCOTCH       11\n\n(8 row(s) affected)\n\n\n\n\nOUTER JOIN\n\n\n--OUTER JOIN\nSELECT A.PK AS A_PK, A.Value AS A_Value,\nB.Value AS B_Value, B.PK AS B_PK\nFROM Table_A A\nFULL OUTER JOIN Table_B B\nON A.PK = B.PK\n\n\n\n\nA_PK A_Value    B_Value    B_PK\n---- ---------- ---------- ----\n   1 FOX        TROT          1\n   2 COP        CAR           2\n   3 TAXI       CAB           3\n   6 WASHINGTON MONUMENT      6\n   7 DELL       PC            7\nNULL NULL       MICROSOFT     8\nNULL NULL       APPLE         9\nNULL NULL       SCOTCH       11\n   5 ARIZONA    NULL       NULL\n   4 LINCOLN    NULL       NULL\n  10 LUCENT     NULL       NULL\n\n(11 row(s) affected)\n\n\n\n\nLEFT EXCLUDING JOIN\n\n\n--LEFT EXCLUDING JOIN\nSELECT A.PK AS A_PK, A.Value AS A_Value,\nB.Value AS B_Value, B.PK AS B_PK\nFROM Table_A A\nLEFT JOIN Table_B B\nON A.PK = B.PK\nWHERE B.PK IS NULL\n\n\n\n\nA_PK A_Value    B_Value    B_PK\n---- ---------- ---------- ----\n   4 LINCOLN    NULL       NULL\n   5 ARIZONA    NULL       NULL\n  10 LUCENT     NULL       NULL\n(3 row(s) affected)\n\n\n\n\nRIGHT EXCLUDING JOIN\n\n\n-RIGHT EXCLUDING JOIN\nSELECT A.PK AS A_PK, A.Value AS A_Value,\nB.Value AS B_Value, B.PK AS B_PK\nFROM Table_A A\nRIGHT JOIN Table_B B\nON A.PK = B.PK\nWHERE A.PK IS NULL\n\n\n\n\nA_PK A_Value    B_Value    B_PK\n---- ---------- ---------- ----\nNULL NULL       MICROSOFT     8\nNULL NULL       APPLE         9\nNULL NULL       SCOTCH       11\n\n(3 row(s) affected)\n\n\n\n\nOUTER EXCLUDING JOIN\n\n\n--OUTER EXCLUDING JOIN\nSELECT A.PK AS A_PK, A.Value AS A_Value,\nB.Value AS B_Value, B.PK AS B_PK\nFROM Table_A A\nFULL OUTER JOIN Table_B B\nON A.PK = B.PK\nWHERE A.PK IS NULL\nOR B.PK IS NULL\n\n\n\n\nA_PK A_Value    B_Value    B_PK\n---- ---------- ---------- ----\nNULL NULL       MICROSOFT     8\nNULL NULL       APPLE         9\nNULL NULL       SCOTCH       11\n   5 ARIZONA    NULL       NULL\n   4 LINCOLN    NULL       NULL\n  10 LUCENT     NULL       NULL\n\n(6 row(s) affected)\n\n\n\n\nConclusion\n\n\nNote on the OUTER JOIN that the inner joined records are returned first, followed by the right joined records, and then finally the left joined records (at least, that\ns how my Microsoft SQL Server did it; this, of course, is without using any ORDER BY statement).", 
            "title": "Introduction to SQL JOINs"
        }, 
        {
            "location": "/Introduction to SQL JOINs/#using-the-code", 
            "text": "Seven different ways you can return data from two relational tables; excluding cross Joins and self referencing Joins:   INNER JOIN  LEFT JOIN  RIGHT JOIN  OUTER JOIN  LEFT JOIN EXCLUDING INNER JOIN  RIGHT JOIN EXCLUDING INNER JOIN  OUTER JOIN EXCLUDING INNER JOIN    For the sake of this article, 5, 6, and 7 are LEFT EXCLUDING JOIN, RIGHT EXCLUDING JOIN, and OUTER EXCLUDING JOIN, respectively. Some may argue that 5, 6, and 7 are not really joining the two tables, but for simplicity, let s refer to these as Joins because you use a SQL Join in each of these queries (but exclude some records with a WHERE clause).", 
            "title": "Using the code"
        }, 
        {
            "location": "/Introduction to SQL JOINs/#inner-join", 
            "text": "This is the simplest, most understood Join and is the most common. This query will return all of the records in the left table (table A) that have a matching record in the right table (table B). This Join is written as follows:  SELECT  select_list  \nFROM Table_A A\nINNER JOIN Table_B B\nON A.Key = B.Key", 
            "title": "Inner JOIN"
        }, 
        {
            "location": "/Introduction to SQL JOINs/#left-join", 
            "text": "This query will return all of the records in the left table (table A) regardless if any of those records have a match in the right table (table B). It will also return any matching records from the right table. This Join is written as follows:  SELECT  select_list \nFROM Table_A A\nLEFT JOIN Table_B B\nON A.Key = B.Key", 
            "title": "Left JOIN"
        }, 
        {
            "location": "/Introduction to SQL JOINs/#right-join", 
            "text": "This query will return all of the records in the right table (table B) regardless if any of those records have a match in the left table (table A). It will also return any matching records from the left table. This Join is written as follows:  SELECT  select_list \nFROM Table_A A\nRIGHT JOIN Table_B B\nON A.Key = B.Key", 
            "title": "Right JOIN"
        }, 
        {
            "location": "/Introduction to SQL JOINs/#outer-join", 
            "text": "This Join can also be referred to as a FULL OUTER JOIN or a FULL JOIN. This query will return all of the records from both tables, joining records from the left table (table A) that match records from the right table (table B). This Join is written as follows:  SELECT  select_list \nFROM Table_A A\nFULL OUTER JOIN Table_B B\nON A.Key = B.Key", 
            "title": "Outer JOIN"
        }, 
        {
            "location": "/Introduction to SQL JOINs/#left-excluding-join", 
            "text": "This query will return all of the records in the left table (table A) that do not match any records in the right table (table B). This Join is written as follows:  SELECT  select_list  \nFROM Table_A A\nLEFT JOIN Table_B B\nON A.Key = B.Key\nWHERE B.Key IS NULL", 
            "title": "Left Excluding JOIN"
        }, 
        {
            "location": "/Introduction to SQL JOINs/#right-excluding-join", 
            "text": "This query will return all of the records in the right table (table B) that do not match any records in the left table (table A). This Join is written as follows:  SELECT  select_list \nFROM Table_A A\nRIGHT JOIN Table_B B\nON A.Key = B.Key\nWHERE A.Key IS NULL", 
            "title": "Right Excluding JOIN"
        }, 
        {
            "location": "/Introduction to SQL JOINs/#outer-excluding-join", 
            "text": "This query will return all of the records in the left table (table A) and all of the records in the right table (table B) that do not match. I have yet to have a need for using this type of Join, but all of the others, I use quite frequently. This Join is written as follows:  SELECT  select_list \nFROM Table_A A\nFULL OUTER JOIN Table_B B\nON A.Key = B.Key\nWHERE A.Key IS NULL OR B.Key IS NULL", 
            "title": "Outer Excluding JOIN"
        }, 
        {
            "location": "/Introduction to SQL JOINs/#examples", 
            "text": "Suppose we have two tables, Table_A and Table_B. The data in these tables are shown below:  TABLE_A\n  PK Value\n---- ----------\n   1 FOX\n   2 COP\n   3 TAXI\n   6 WASHINGTON\n   7 DELL\n   5 ARIZONA\n   4 LINCOLN\n  10 LUCENT\n\nTABLE_B\n  PK Value\n---- ----------\n   1 TROT\n   2 CAR\n   3 CAB\n   6 MONUMENT\n   7 PC\n   8 MICROSOFT\n   9 APPLE\n  11 SCOTCH  The results of the seven Joins are shown below:  INNER JOIN  --INNER JOIN\nSELECT A.PK AS A_PK, A.Value AS A_Value,\n       B.Value AS B_Value, B.PK AS B_PK\nFROM Table_A A\nINNER JOIN Table_B B\nON A.PK = B.PK  A_PK A_Value    B_Value    B_PK\n---- ---------- ---------- ----\n   1 FOX        TROT          1\n   2 COP        CAR           2\n   3 TAXI       CAB           3\n   6 WASHINGTON MONUMENT      6\n   7 DELL       PC            7\n\n(5 row(s) affected)  LEFT JOIN  --LEFT JOIN\nSELECT A.PK AS A_PK, A.Value AS A_Value,\nB.Value AS B_Value, B.PK AS B_PK\nFROM Table_A A\nLEFT JOIN Table_B B\nON A.PK = B.PK  A_PK A_Value    B_Value    B_PK\n---- ---------- ---------- ----\n   1 FOX        TROT          1\n   2 COP        CAR           2\n   3 TAXI       CAB           3\n   4 LINCOLN    NULL       NULL\n   5 ARIZONA    NULL       NULL\n   6 WASHINGTON MONUMENT      6\n   7 DELL       PC            7\n  10 LUCENT     NULL       NULL\n\n(8 row(s) affected)  RIGHT JOIN  --RIGHT JOIN\nSELECT A.PK AS A_PK, A.Value AS A_Value,\nB.Value AS B_Value, B.PK AS B_PK\nFROM Table_A A\nRIGHT JOIN Table_B B\nON A.PK = B.PK  A_PK A_Value    B_Value    B_PK\n---- ---------- ---------- ----\n   1 FOX        TROT          1\n   2 COP        CAR           2\n   3 TAXI       CAB           3\n   6 WASHINGTON MONUMENT      6\n   7 DELL       PC            7\nNULL NULL       MICROSOFT     8\nNULL NULL       APPLE         9\nNULL NULL       SCOTCH       11\n\n(8 row(s) affected)  OUTER JOIN  --OUTER JOIN\nSELECT A.PK AS A_PK, A.Value AS A_Value,\nB.Value AS B_Value, B.PK AS B_PK\nFROM Table_A A\nFULL OUTER JOIN Table_B B\nON A.PK = B.PK  A_PK A_Value    B_Value    B_PK\n---- ---------- ---------- ----\n   1 FOX        TROT          1\n   2 COP        CAR           2\n   3 TAXI       CAB           3\n   6 WASHINGTON MONUMENT      6\n   7 DELL       PC            7\nNULL NULL       MICROSOFT     8\nNULL NULL       APPLE         9\nNULL NULL       SCOTCH       11\n   5 ARIZONA    NULL       NULL\n   4 LINCOLN    NULL       NULL\n  10 LUCENT     NULL       NULL\n\n(11 row(s) affected)  LEFT EXCLUDING JOIN  --LEFT EXCLUDING JOIN\nSELECT A.PK AS A_PK, A.Value AS A_Value,\nB.Value AS B_Value, B.PK AS B_PK\nFROM Table_A A\nLEFT JOIN Table_B B\nON A.PK = B.PK\nWHERE B.PK IS NULL  A_PK A_Value    B_Value    B_PK\n---- ---------- ---------- ----\n   4 LINCOLN    NULL       NULL\n   5 ARIZONA    NULL       NULL\n  10 LUCENT     NULL       NULL\n(3 row(s) affected)  RIGHT EXCLUDING JOIN  -RIGHT EXCLUDING JOIN\nSELECT A.PK AS A_PK, A.Value AS A_Value,\nB.Value AS B_Value, B.PK AS B_PK\nFROM Table_A A\nRIGHT JOIN Table_B B\nON A.PK = B.PK\nWHERE A.PK IS NULL  A_PK A_Value    B_Value    B_PK\n---- ---------- ---------- ----\nNULL NULL       MICROSOFT     8\nNULL NULL       APPLE         9\nNULL NULL       SCOTCH       11\n\n(3 row(s) affected)  OUTER EXCLUDING JOIN  --OUTER EXCLUDING JOIN\nSELECT A.PK AS A_PK, A.Value AS A_Value,\nB.Value AS B_Value, B.PK AS B_PK\nFROM Table_A A\nFULL OUTER JOIN Table_B B\nON A.PK = B.PK\nWHERE A.PK IS NULL\nOR B.PK IS NULL  A_PK A_Value    B_Value    B_PK\n---- ---------- ---------- ----\nNULL NULL       MICROSOFT     8\nNULL NULL       APPLE         9\nNULL NULL       SCOTCH       11\n   5 ARIZONA    NULL       NULL\n   4 LINCOLN    NULL       NULL\n  10 LUCENT     NULL       NULL\n\n(6 row(s) affected)", 
            "title": "Examples"
        }, 
        {
            "location": "/Introduction to SQL JOINs/#conclusion", 
            "text": "Note on the OUTER JOIN that the inner joined records are returned first, followed by the right joined records, and then finally the left joined records (at least, that s how my Microsoft SQL Server did it; this, of course, is without using any ORDER BY statement).", 
            "title": "Conclusion"
        }, 
        {
            "location": "/Build a Website/", 
            "text": "Build a Website\n\n\nForeword\n\n\nNotes.\n\n\n\n\nCONTENT\n\n\nBuild a Website\n\n\nThemes and Goals\n\n\nStatic Webpage\n\n\nNeeds\n\n\nStatic Site Generators\n\n\nCode Repository vs Pastebin\n\n\n\n\n\n\n\n\n\n\n\n\nThemes and Goals\n\n\n\n\n1-pager or personal card: a simple online presence, aocial media hub, a starting point around a name, a topic.\n\n\nAcademic.\n\n\nBlogging.\n\n\nGallery: showcase audio, video, images (or links to), samples; good for storytelling or explaining a project.\n\n\nKnowledge management, documentation.\n\n\nPortfolio: showcase work, experience, competences, etc.\n\n\nProfessional: showcase a business, teams, products, services, etc.\n\n\nProject management.\n\n\nFor \nmobile devices and tablets\n, make sure the theme is \nresponsive\n. Some themes analytics-ready or SEO-optimized.\n\n\n\n\nStatic Webpage\n\n\nA static web page (sometimes called a flat page/stationary page) is a web page that is delivered to the user exactly as stored, in contrast to dynamic web pages which are generated by a web application or a web framework.\n\n\nConsequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a web server to negotiate content-type or language of the document where such versions are available and the server is configured to do so.\n\n\nStatic web pages are often HTML documents stored as files in the file system and made available by the web server over HTTP (nevertheless URLs ending with \n.html\n are not always static). However, loose interpretations of the term could include web pages stored in a database, and could even include pages formatted using a template and served through an application server, as long as the page served is unchanging and presented essentially as stored.\n\n\nStatic web pages are suitable for the contents that never or rarely need to be updated. However, maintaining large numbers of static pages as files can be impractical without automated tools. Any personalization or interactivity has to run client-side, which is restricting.\n\n\nAdvantages of static website:\n\n\n\n\nCheap to develop.\n\n\nCheap to host.\n\n\nNo database or server-side logic.\n\n\nQuick to develop.\n\n\n\n\nDisadvantages of static website:\n\n\n\n\nContent can get stagnant.\n\n\nRequires web development expertise to update site.\n\n\nSite not as useful for the user.\n\n\n\n\nNeeds\n\n\n\n\nA static site generator (instead of building everything yourself).\n\n\nA theme (each website as a goal).\n\n\nA simple text/code editor (such as notepad++, gedit, etc.).\n\n\nGit or another version control system (push the local PC content to the online repository and pull from it).\n\n\nA code repository to host the website:\n\n\nBitbucket\n.\n\n\nGitHub\n.\n\n\nGitLab\n\n\nSourceforge\n\n\nAnd many more\n\n\nWarning! Some are closed, some changed names, etc.\n\n\n\n\n\n\n\n\nStatic Site Generators\n\n\n\n\nTop Open-Source Static Site Generators\n\n\nStatic Site Generators\n\n\n\n\nCode Repository vs Pastebin\n\n\nA code repository:\n\n\n\n\nArchive where portions of source code are stored.\n\n\nPublically available.\n\n\nUsed by open source software developers and other projects with co-developers.\n\n\n\n\nA pastebin:\n\n\n\n\nOnline repository that allows its users to upload snippets of text, usually samples of source code.\n\n\nFor public viewing, sharing and, in some cases, editing.", 
            "title": "Build a Website"
        }, 
        {
            "location": "/Build a Website/#themes-and-goals", 
            "text": "1-pager or personal card: a simple online presence, aocial media hub, a starting point around a name, a topic.  Academic.  Blogging.  Gallery: showcase audio, video, images (or links to), samples; good for storytelling or explaining a project.  Knowledge management, documentation.  Portfolio: showcase work, experience, competences, etc.  Professional: showcase a business, teams, products, services, etc.  Project management.  For  mobile devices and tablets , make sure the theme is  responsive . Some themes analytics-ready or SEO-optimized.", 
            "title": "Themes and Goals"
        }, 
        {
            "location": "/Build a Website/#static-webpage", 
            "text": "A static web page (sometimes called a flat page/stationary page) is a web page that is delivered to the user exactly as stored, in contrast to dynamic web pages which are generated by a web application or a web framework.  Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a web server to negotiate content-type or language of the document where such versions are available and the server is configured to do so.  Static web pages are often HTML documents stored as files in the file system and made available by the web server over HTTP (nevertheless URLs ending with  .html  are not always static). However, loose interpretations of the term could include web pages stored in a database, and could even include pages formatted using a template and served through an application server, as long as the page served is unchanging and presented essentially as stored.  Static web pages are suitable for the contents that never or rarely need to be updated. However, maintaining large numbers of static pages as files can be impractical without automated tools. Any personalization or interactivity has to run client-side, which is restricting.  Advantages of static website:   Cheap to develop.  Cheap to host.  No database or server-side logic.  Quick to develop.   Disadvantages of static website:   Content can get stagnant.  Requires web development expertise to update site.  Site not as useful for the user.", 
            "title": "Static Webpage"
        }, 
        {
            "location": "/Build a Website/#needs", 
            "text": "A static site generator (instead of building everything yourself).  A theme (each website as a goal).  A simple text/code editor (such as notepad++, gedit, etc.).  Git or another version control system (push the local PC content to the online repository and pull from it).  A code repository to host the website:  Bitbucket .  GitHub .  GitLab  Sourceforge  And many more  Warning! Some are closed, some changed names, etc.", 
            "title": "Needs"
        }, 
        {
            "location": "/Build a Website/#static-site-generators", 
            "text": "Top Open-Source Static Site Generators  Static Site Generators", 
            "title": "Static Site Generators"
        }, 
        {
            "location": "/Build a Website/#code-repository-vs-pastebin", 
            "text": "A code repository:   Archive where portions of source code are stored.  Publically available.  Used by open source software developers and other projects with co-developers.   A pastebin:   Online repository that allows its users to upload snippets of text, usually samples of source code.  For public viewing, sharing and, in some cases, editing.", 
            "title": "Code Repository vs Pastebin"
        }, 
        {
            "location": "/HTML and CSS/", 
            "text": "HTML and CSS\n\n\nForeword\n\n\nNotes.\n\n\n\n\nCONTENT\n\n\nHTML and CSS\n\n\nOption One: Convert\n\n\nOption Two: Generate\n\n\nOption Three: Learn and Code\n\n\nAdd CSS\n\n\nCSS Generators\n\n\nWeb Generator Lists\n\n\n\n\n\n\n\n\n\n\n\n\nOption One: Convert\n\n\n\n\nWrite a document in Word, Markdown and save it in HTML, including a style (no or minimal CSS).\n\n\nUse Pandoc to convert Word, Markdown, and other format to HTML.\n\n\n\n\nOption Two: Generate\n\n\n\n\nLook for a HTML generator online.\n\n\nUse a PC generator: \nStaticGen\n\n\n\n\nOption Three: Learn and Code\n\n\nCreate a webpage with notepad, notepad++ or gedit (among many text editors) using simple HTML and CSS.\n\n\n\n\nPDF: \nWeb Page Design\n\n\nTutorial: \nBuilding your First Web Page\n\n\nComprehensive Tutorial: \nHTML 4.01 Specification\n\n\n\n\nAdd CSS\n\n\n\n\nAdding CSS\n\n\n\n\nCSS Generators\n\n\n\n\nhttp://csscreator.com/tools/cssgenerate\n\n\nhttp://csslayoutgenerator.com/\n\n\nhttp://jdstiles.com/java/ssgen.html\n\n\nhttp://www.css3maker.com/\n\n\nhttp://www.generatecss.com/\n\n\nhttp://www.htmlbasix.com/stylesheet.shtml\n\n\nhttp://www.madsubmitter.com/css-generator/\n\n\n\n\nWeb Generator Lists\n\n\n\n\nhttp://www.html.am/\n\n\nhttp://www.webdesignerdepot.com/2012/04/15-great-html5-and-css3-generators/", 
            "title": "HTML and CSS"
        }, 
        {
            "location": "/HTML and CSS/#option-one-convert", 
            "text": "Write a document in Word, Markdown and save it in HTML, including a style (no or minimal CSS).  Use Pandoc to convert Word, Markdown, and other format to HTML.", 
            "title": "Option One: Convert"
        }, 
        {
            "location": "/HTML and CSS/#option-two-generate", 
            "text": "Look for a HTML generator online.  Use a PC generator:  StaticGen", 
            "title": "Option Two: Generate"
        }, 
        {
            "location": "/HTML and CSS/#option-three-learn-and-code", 
            "text": "Create a webpage with notepad, notepad++ or gedit (among many text editors) using simple HTML and CSS.   PDF:  Web Page Design  Tutorial:  Building your First Web Page  Comprehensive Tutorial:  HTML 4.01 Specification", 
            "title": "Option Three: Learn and Code"
        }, 
        {
            "location": "/HTML and CSS/#add-css", 
            "text": "Adding CSS", 
            "title": "Add CSS"
        }, 
        {
            "location": "/HTML and CSS/#css-generators", 
            "text": "http://csscreator.com/tools/cssgenerate  http://csslayoutgenerator.com/  http://jdstiles.com/java/ssgen.html  http://www.css3maker.com/  http://www.generatecss.com/  http://www.htmlbasix.com/stylesheet.shtml  http://www.madsubmitter.com/css-generator/", 
            "title": "CSS Generators"
        }, 
        {
            "location": "/HTML and CSS/#web-generator-lists", 
            "text": "http://www.html.am/  http://www.webdesignerdepot.com/2012/04/15-great-html5-and-css3-generators/", 
            "title": "Web Generator Lists"
        }, 
        {
            "location": "/Markdown & Pandoc/", 
            "text": "Markdown \n Pandoc\n\n\nForeword\n\n\nNotes.\n\n\n\n\nCONTENT\n\n\nMarkdown \n Pandoc\n\n\nMarkdown Editors\n\n\nWindows\n\n\nLinux\n\n\nOnline\n\n\n\n\n\n\nR Markdown Documentation\n\n\nPandoc, Document Converter\n\n\nInstall\n\n\nUse in a terminal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarkdown Editors\n\n\nWindows\n\n\n\n\nMarkdownPad\n.\n\n\nMarkPad\n.\n\n\n\n\nLinux\n\n\n\n\nRemarkable\n.\n\n\n\n\nOnline\n\n\n\n\ndilliger\n.\n\n\nstackedit\n.\n\n\ntablesgenerator\n.\n\n\n\n\nR Markdown Documentation\n\n\n\n\nR Markdown\n.\n\n\nQuicktour\n.\n\n\n\n\nPandoc, Document Converter\n\n\nInstall\n\n\n\n\nPandoc\n\n\nFor Windows and Linux, download the \npackages\n.\n\n\nInstall Pandoc on Ubuntu with \nhaskell\n\n\n\n\nUse in a terminal\n\n\n\n\npandoc --version\n.\n\n\npandoc --help\n, \nman pandoc\n on Linux.\n\n\nman pandoc_markdown\n.\n\n\nPandoc is located in a directory. It is where conversions are done. For example, on Windows: \ncd D:\\User\\Documents\\Pandoc\n\n\nSnippets", 
            "title": "Markdown & Pandoc"
        }, 
        {
            "location": "/Markdown & Pandoc/#markdown-editors", 
            "text": "", 
            "title": "Markdown Editors"
        }, 
        {
            "location": "/Markdown & Pandoc/#windows", 
            "text": "MarkdownPad .  MarkPad .", 
            "title": "Windows"
        }, 
        {
            "location": "/Markdown & Pandoc/#linux", 
            "text": "Remarkable .", 
            "title": "Linux"
        }, 
        {
            "location": "/Markdown & Pandoc/#online", 
            "text": "dilliger .  stackedit .  tablesgenerator .", 
            "title": "Online"
        }, 
        {
            "location": "/Markdown & Pandoc/#r-markdown-documentation", 
            "text": "R Markdown .  Quicktour .", 
            "title": "R Markdown Documentation"
        }, 
        {
            "location": "/Markdown & Pandoc/#pandoc-document-converter", 
            "text": "", 
            "title": "Pandoc, Document Converter"
        }, 
        {
            "location": "/Markdown & Pandoc/#install", 
            "text": "Pandoc  For Windows and Linux, download the  packages .  Install Pandoc on Ubuntu with  haskell", 
            "title": "Install"
        }, 
        {
            "location": "/Markdown & Pandoc/#use-in-a-terminal", 
            "text": "pandoc --version .  pandoc --help ,  man pandoc  on Linux.  man pandoc_markdown .  Pandoc is located in a directory. It is where conversions are done. For example, on Windows:  cd D:\\User\\Documents\\Pandoc  Snippets", 
            "title": "Use in a terminal"
        }, 
        {
            "location": "/tests/", 
            "text": "Tests\n\n\nForeword\n\n\nThis page is temporary. It tests markup possibilities.\n\n\n\n\nCONTENT\n\n\nTests\n\n\nList\n\n\nSymbols\n\n\nCode\n\n\nFont\n\n\nEmbed videos\n\n\nBuild a table\n\n\n\n\n\n\n\n\n\n\n\n\nList\n\n\n\n\nApprenez \u00e0 programmer en Python\n\n\nAutomate the Boring Stuff with Python\n\n\n\n\nSymbols\n\n\n\n\n \n\n\n\n\nshort dash: -\n\n\nmid dash: \n (for literature)\n\n\nlong dash: \n (for literature)\n\n\n\n\nCode\n\n\ndef aaa():\n    print(\na\n)\n\n\n\n\nFont\n\n\nEnter superscript: 2\n10\n is 1024.\n\n\nsuperscript\n\n\npip install MarkdownSuperscript\n\n\nEnter subscript: The molecular composition of water is H\n2\nO.\n\n\nsubscript\n\n\npip install MarkdownSubscript\n\n\nEmbed videos\n\n\n!embed\n\n\npip install pyembed-markdown\n\n\nBuild a table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na\n\n\nb\n\n\n\n\n\n\nc\n\n\nd\n\n\n\n\n\n\n\n\nDraw a line:\n\n\n\n\nWithout outside borders:\n\n\n\n\n\n\n\n\nFirst Header\n\n\nSecond Header\n\n\nThird Header\n\n\n\n\n\n\n\n\n\n\nContent Cell\n\n\nContent Cell\n\n\nContent Cell\n\n\n\n\n\n\nContent Cell\n\n\nContent Cell\n\n\nContent Cell\n\n\n\n\n\n\n\n\n\n\nWith outside borders:\n\n\n\n\n\n\n\n\nFirst Header\n\n\nSecond Header\n\n\nThird Header\n\n\n\n\n\n\n\n\n\n\nContent Cell\n\n\nContent Cell\n\n\nContent Cell\n\n\n\n\n\n\nContent Cell\n\n\nContent Cell\n\n\nContent Cell\n\n\n\n\n\n\n\n\n\n\nAligned:\n\n\n\n\n\n\n\n\nFirst Header\n\n\nSecond Header\n\n\nThird Header\n\n\n\n\n\n\n\n\n\n\nLeft\n\n\nCenter\n\n\nRight\n\n\n\n\n\n\nLeft\n\n\nCenter\n\n\nRight", 
            "title": "Tests"
        }, 
        {
            "location": "/tests/#list", 
            "text": "Apprenez \u00e0 programmer en Python  Automate the Boring Stuff with Python", 
            "title": "List"
        }, 
        {
            "location": "/tests/#symbols", 
            "text": "short dash: -  mid dash:   (for literature)  long dash:   (for literature)", 
            "title": "Symbols"
        }, 
        {
            "location": "/tests/#code", 
            "text": "def aaa():\n    print( a )", 
            "title": "Code"
        }, 
        {
            "location": "/tests/#font", 
            "text": "Enter superscript: 2 10  is 1024.  superscript  pip install MarkdownSuperscript  Enter subscript: The molecular composition of water is H 2 O.  subscript  pip install MarkdownSubscript", 
            "title": "Font"
        }, 
        {
            "location": "/tests/#embed-videos", 
            "text": "!embed  pip install pyembed-markdown", 
            "title": "Embed videos"
        }, 
        {
            "location": "/tests/#build-a-table", 
            "text": "a  b    c  d     Draw a line:   Without outside borders:     First Header  Second Header  Third Header      Content Cell  Content Cell  Content Cell    Content Cell  Content Cell  Content Cell      With outside borders:     First Header  Second Header  Third Header      Content Cell  Content Cell  Content Cell    Content Cell  Content Cell  Content Cell      Aligned:     First Header  Second Header  Third Header      Left  Center  Right    Left  Center  Right", 
            "title": "Build a table"
        }
    ]
}